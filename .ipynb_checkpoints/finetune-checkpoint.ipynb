{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106ef11a-943e-4135-b169-6f4c44529482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0672cdcf-aacb-4749-8e8c-256b94c3402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_path = '/mnt/qwen2.5-72B/llm-research/meta-llama-3___1-8b-instruct'\n",
    "lora_path = '/mnt/qwen2.5-72B/lora' # 这里改称你的 lora 输出对应 checkpoint 地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b8c36d-0185-4fee-a89c-0a7e79d1005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7c2a73-9f4c-4886-8f55-144aff7513f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 120000    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. At the same time, provide a step-by-step reasoning process according to the article to answer the question. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"unanswerable\\\". Do not provide any explanation in Answer but in Reasoning.Please provide your answer and reasoning according to the following format: Answer: [Your concise final answer here], Reasoning: [Your step-by-step thought process here]. Remember to keep the Answer and Reasoning separate and not mix them together. The Answer should be brief and to the point, while the Reasoning can be more detailed to explain your thought process.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nArticle{example['context']}\\n\\nHere is the following question. Question: {example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}<|eot_id|>\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    print(len(input_ids))\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f1530a-a509-43dc-aa53-f57abe99d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('dataset/new_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40f64cd-306a-4238-8d4e-a08e7d8cd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b2ffe49-4eff-494e-8225-229478d8134b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d020b69bf2b49078965931dce939d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4171\n",
      "3503\n",
      "3149\n",
      "4611\n",
      "5545\n",
      "6670\n",
      "3662\n",
      "5494\n",
      "4765\n",
      "3407\n",
      "3541\n",
      "15735\n",
      "5313\n",
      "5274\n",
      "7986\n",
      "5507\n",
      "3303\n",
      "9401\n",
      "2984\n",
      "5504\n",
      "10789\n",
      "3898\n",
      "2828\n",
      "6015\n",
      "2858\n",
      "4406\n",
      "2642\n",
      "4510\n",
      "5748\n",
      "3746\n",
      "6067\n",
      "4081\n",
      "19713\n",
      "5304\n",
      "4967\n",
      "21448\n",
      "6786\n",
      "3280\n",
      "3801\n",
      "4531\n",
      "4170\n",
      "2715\n",
      "5298\n",
      "10168\n",
      "3303\n",
      "5669\n",
      "6742\n",
      "9825\n",
      "4184\n",
      "3929\n",
      "5905\n",
      "4542\n",
      "3550\n",
      "5521\n",
      "2742\n",
      "6879\n",
      "5814\n",
      "2705\n",
      "3920\n",
      "4932\n",
      "2288\n",
      "2393\n",
      "6065\n",
      "6764\n",
      "3426\n",
      "7689\n",
      "6045\n",
      "3330\n",
      "2284\n",
      "2645\n",
      "6266\n",
      "4611\n",
      "5297\n",
      "3244\n",
      "3799\n",
      "3867\n",
      "3247\n",
      "4833\n",
      "4079\n",
      "3802\n",
      "5777\n",
      "3485\n",
      "4158\n",
      "8212\n",
      "2885\n",
      "5771\n",
      "2633\n",
      "3522\n",
      "3150\n",
      "2895\n",
      "5765\n",
      "3474\n",
      "4380\n",
      "6142\n",
      "3983\n",
      "5904\n",
      "6636\n",
      "2966\n",
      "5121\n",
      "5903\n",
      "2796\n",
      "3001\n",
      "3367\n",
      "4428\n",
      "5313\n",
      "3329\n",
      "6352\n",
      "8615\n",
      "2879\n",
      "3099\n",
      "5332\n",
      "5450\n",
      "3830\n",
      "4166\n",
      "4475\n",
      "4272\n",
      "5674\n",
      "6066\n",
      "9826\n",
      "6065\n",
      "2657\n",
      "6970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 122\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c94498a-29a5-4060-8215-b9b1c8ecc3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cc67141b9c49e8b99bc41013d3d336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"cuda:0\",torch_dtype=torch.bfloat16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255045e-46bb-489d-a4c5-22282d1996ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() 开启梯度检查点时，要执行该方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lb",
   "language": "python",
   "name": "lb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
