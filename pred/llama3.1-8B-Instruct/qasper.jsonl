{"pred": [" It is a pooling strategy that is an extension of the NetVLAD approach and adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " It is an extension of NetVLAD approach for face recognition. \n", " GhostVLAD is an extension of the NetVLAD approach, ", " The GhostVLAD approach is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. ", " It is an extension of the NetVLAD approach that adds Ghost clusters along with the NetVLAD clusters. ", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters, and are not included during the feature aggregation stage. \n", " An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content. \n", " GhostVLAD is an extension of the NetVLAD approach that adds ghost clusters along with normal clusters to map noisy or irrelevant content into ghost clusters. ", " The GhostVLAD approach is a pooling strategy that extends the NetVLAD method, adding ghost clusters to map noisy or irrelevant content into ghost clusters and excluding them during feature aggregation. \n", " An extension of the NetVLAD approach with the addition of Ghost clusters for mapping noisy or irrelevant content into ghost clusters. \n", " It is a pooling strategy that was originally proposed for face recognition. \n", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content. \n", " It is an extension of the NetVLAD approach with the addition of ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " GhostVLAD is an extension of the NetVLAD approach, which adds ghost clusters to map noisy or irrelevant content into ghost clusters. ", " Ghost-VLAD is an extension of the NetVLAD approach that adds ghost clusters along with normal clusters. \n", " An extension of the NetVLAD approach that includes ghost clusters to map noisy or irrelevant content into these clusters. \n", " It is an extension of the NetVLAD approach, adding ghost clusters to map noisy or irrelevant content into ghost clusters and exclude them during feature aggregation. ", " It is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " A GhostVLAD based pooling method that extends NetVLAD by adding ghost clusters. \n", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " It is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content. \n", " An extension of NetVLAD approach, which maps the noisy and irrelevant content into ghost clusters. \n", " It is an extension of the NetVLAD approach, which adds ghost clusters along with normal clusters to map noisy or irrelevant content into ghost clusters. \n", " GhostVLAD is an extension of the NetVLAD approach, adding ghost clusters to map noisy or irrelevant content into ghost clusters that are excluded during feature aggregation. ", " It is an extension of the NetVLAD approach. ", " GhostVLAD approach is a pooling method that uses both normal clusters and ghost clusters. \n", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters, ", " It is an extension of the NetVLAD approach that adds Ghost clusters along with the NetVLAD clusters. \n\n", " It is an extension of the NetVLAD approach. ", " A pooling strategy for language identification. \n", " GhostVLAD approach is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content. \n\n", " GhostVLAD is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " It is an extension of the NetVLAD approach, which adds ghost clusters to map noisy or irrelevant content into these clusters, not included during feature aggregation. \n", " An extension of NetVLAD with the addition of ghost clusters to map noisy or irrelevant content into ghost clusters, ", " A GhostVLAD approach is an extension of the NetVLAD model that adds Ghost clusters along with the NetVLAD clusters. \n", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content. \n", " It is an extension of NetVLAD approach that adds ghost clusters along with the NetVLAD clusters for mapping noisy or irrelevant content. \n", " An extension of the NetVLAD approach that adds ghost clusters along with the NetVLAD clusters for face recognition. \n", " An extension of NetVLAD, which adds ghost clusters to map noisy or irrelevant content. \n", " GhostVLAD is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters, ", " An extension of the NetVLAD approach with ghost clusters added to map noisy or irrelevant content, ", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and exclude them during feature aggregation stage, ", " An extension of the NetVLAD approach that adds Ghost clusters along with the NetVLAD clusters. \n", " It is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage. \n", " An extension of NetVLAD approach that adds Ghost clusters along with the NetVLAD clusters, with Ghost clusters mapping noisy or irrelevant content into them. ", " It is an extension of the NetVLAD approach that adds Ghost clusters along with NetVLAD clusters to map noisy or irrelevant content into ghost clusters, which are not included during the feature aggregation stage. \n\n", " An extension of the NetVLAD approach, which adds ghost clusters to map noisy or irrelevant content into ghost clusters and excludes them during the feature aggregation stage. \n", " GhostVLAD is an extension of the NetVLAD approach that adds ghost clusters along with NetVLAD clusters to map noisy or irrelevant content into ghost clusters, which are excluded during feature aggregation. \n", " GhostVLAD is an extension of NetVLAD, which adds ghost clusters to NetVLAD to map noisy or irrelevant content into ghost clusters. \n", " Ghost-VLAD pooling approach is a method that adds ghost clusters to NetVLAD to map noisy or irrelevant content into these clusters, which are then excluded during feature aggregation. ", " An extension of NetVLAD, with added Ghost clusters to map noisy content into, that is excluded during feature aggregation. \n", " It is an extension of the NetVLAD approach with the addition of ghost clusters. ", " A pooling strategy that was originally proposed for face recognition and works by adding Ghost clusters along with NetVLAD clusters to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage. \n\n\n", " An extension of the NetVLAD approach, adding ghost clusters to map noisy or irrelevant content into ghost clusters that are not included during the feature aggregation stage. \n", " An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters, which are excluded during feature aggregation stage. ", " It is an extension of the NetVLAD approach. ", " It is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content. ", " A pooling method that is an extension of the NetVLAD approach, adding ghost clusters to map noisy or irrelevant content into ghost clusters and excluding them during feature aggregation. \n", " GhostVLAD is an extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into these clusters. \n", " It is a pooling method used for aggregating frame-level features into a single utterance level feature. \n", " GhostVLAD approach is an extension of the NetVLAD model that adds Ghost clusters to map noisy or irrelevant content into these clusters, ", " A pooling strategy that is an extension of the NetVLAD approach, adding ghost clusters to map noisy or irrelevant content. \n", " GhostVLAD is an extension of NetVLAD approach which adds ghost clusters along with normal clusters. ", " An extension of the NetVLAD approach, adding Ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " An extension of the NetVLAD approach. \n", " An extension of NetVLAD approach for face recognition. ", " It is an extension of the NetVLAD approach which adds Ghost clusters to map noisy or irrelevant content into ghost clusters and exclude them during feature aggregation. ", " An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage. \n", " Ghost-VLAD pooling approach that was originally proposed for face recognition. \n", " It's an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage. ", " Ghost-VLAD pooling approach that was originally proposed for face recognition to improve language identification performance for Indian languages. \n\n", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters, ", " The GhostVLAD is an extension of the NetVLAD approach which adds Ghost clusters along with the NetVLAD clusters to map any noisy or irrelevant content into ghost clusters. ", " An extension of the NetVLAD approach that adds Ghost clusters along with the NetVLAD clusters. \n", " It is an extension of the NetVLAD approach. \n", " It is an extension of NetVLAD approach for language identification that adds ghost clusters to map noisy or irrelevant content into ghost clusters which are excluded during the feature aggregation stage. \n", " An extension of NetVLAD approach, which adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " GhostVLAD is an extension of NetVLAD that adds Ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " It is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters and not include them during feature aggregation. \n", " GhostVLAD is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. ", " It is a pooling strategy that is an extension of NetVLAD, adding Ghost clusters to map noisy or irrelevant content into ghost clusters and excluding their contribution during feature aggregation. \n", " GhostVLAD is an extension of the NetVLAD approach, which adds ghost clusters along with normal clusters to map noisy or irrelevant content into ghost clusters and exclude their contribution during feature aggregation. ", " An extension of the NetVLAD approach with ghost clusters that map noisy or irrelevant content into ghost clusters and are not included during feature aggregation. \n", " It is an extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters, ", " It is an extension of the NetVLAD approach, which adds Ghost clusters along with the NetVLAD clusters. ", " It is an extension of the NetVLAD approach. \n", " An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters, which are excluded during feature aggregation stage. ", " A pooling strategy that aggregates frame-level features into a single utterance level feature using normal and ghost clusters. ", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content. \n", " An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " It is an extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. ", " An extension of NetVLAD approach, which adds Ghost clusters to map noisy or irrelevant content into ghost clusters, and excludes them during feature aggregation. \n", " A pooling strategy that adds ghost clusters along with NetVLAD clusters to map noisy or irrelevant content into ghost clusters, ", " It is an extension of the NetVLAD approach that adds ghost clusters along with normal clusters to map noisy or irrelevant content into ghost clusters. ", " It is an extension of the NetVLAD approach that adds ghost clusters. ", " An extension of the NetVLAD approach that adds ghost clusters to map noisy or irrelevant content into ghost clusters. \n", " The GhostVLAD approach is an extension of the NetVLAD approach, adding ghost clusters to map noisy or irrelevant content into ghost clusters that are not included during feature aggregation. \n", " An extension of the NetVLAD approach that adds Ghost clusters along with the NetVLAD clusters to map any noisy or irrelevant content into ghost clusters and is excluded during the feature aggregation stage. \n"], "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": [" The ground truth for fake news is established through manual annotation by a single person, ", " The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset, ", " The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset, ", " unanswerable, ", " It is established by a single person who manually inspected the text field within the tweets. ", " By manual inspection of the text field within the tweets to label them as containing fake news, or not containing them. ", " The ground truth for fake news is established by manual inspection and labelling by an expert, ", " Not by the dataset used in this study, ", " unanswerable, ", " unanswerable, ", " The ground truth for fake news is established through manual annotation by an expert, ", " The ground truth for fake news is established by a single person manually annotating the tweets and labeling them as containing fake news or not. \n", " unanswerable, ", " By a single person manually annotating the tweets, ", " It is established by a single person manually inspecting the text field within the tweets to label them as containing fake news, or not containing them. \n", " Not claimed to be perfect due to ambiguity and human judgement involved. \n", " The ground truth for fake news is not established. \n", " The annotation was carried out by a single person. ", " The ground truth for fake news is established manually by an expert. \n", " unanswerable, ", " Through manual inspection of the text field of the tweets, ", " unanswerable, ", " Through manual annotation by a single person, ", " The ground truth is far from being perfect due to the ambiguity of fake news themselves and human judgement involved in the process of categorization. \n", " By manual inspection of the text field within the tweets. ", " It is manually annotated by an expert. \n", " The ground truth for fake news is established by manual inspection and annotation by a single person, ", " It is established through manual inspection by an expert, ", " The ground truth for fake news is established through manual inspection by a single expert, ", " Not perfectly established, ", " It was manually annotated by a single person. \n", ", ", " The ground truth for fake news is established by manual inspection of the text field within the tweets, according to the characterization presented by Rubin et al. ", " unanswerable, ", " The ground truth for fake news is not considered perfect due to the ambiguity of fake news and human judgement involved in the process of categorization. ", " By manual inspection of the text field within the tweets to label them as containing fake news, or not containing them, by an expert. \n", " unanswerable, ", " Not explicitly stated, ", " unanswerable, ", " Through manual inspection of the text field by an expert. \n", " unanswerable, ", " The ground truth for fake news is established manually by an expert. \n", " Through manual annotation by an expert based on predefined categories. \n", " It is established by human judgement through a manual annotation by a single expert. \n", " Manual annotation by a single person, ", " It is established through the annotation by a single person. \n", " Manually by an expert, ", " It is annotated by a single person. \n", " unanswerable, ", " It is annotated manually by an expert, ", " Manually by a single expert, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " The ground truth is established by a single person manually annotating the tweets. \n", " The ground truth for fake news is established through manual inspection by an expert. \n", " It is far from being perfect given the ambiguity of fake news themselves and human judgement involved in the process of categorization, \n", " It is established through manual annotation by an expert based on the categorization provided by Rubin et al. \n", " The ground truth for fake news is established by a single person manually annotating the tweets as containing fake news or not. \n\n", " It is established by a single person manually annotating the tweets, ", " The ground truth for fake news is established through manual inspection and annotation of the tweets by a single expert. \n", " It is established by a single expert through manual inspection of the text field within the tweets. \n\n", " unanswerable, ", " Through manual annotation by an expert, ", " It is established by manual inspection, ", " It is established by manual inspection by an expert, ", " The ground truth for fake news is established by manually annotating the tweets by an expert. \n", " The ground truth for fake news is established manually by an expert who inspects the text field within the tweets to label them as containing fake news, or not containing them. \n", " The ground truth for fake news is established by a single person's manual inspection of the text field within the tweets. \n", " The annotation was carried out by a single person, ", " By a single person annotating the 1327 viral tweets collected in the dataset. \n", " Not explicitly established, ", " The ground truth is not established perfectly due to the ambiguity of fake news itself and human judgement involved in the process of categorization. \n", " It is far from perfect and involves human judgement due to the ambiguity of fake news. \n", " The ground truth for fake news is established manually by a single expert annotating the text field within the tweets to label them as containing fake news, or not containing them. \n", " By a single person manually annotating tweets, ", " Unanswerable, ", " The ground truth for fake news is established manually by an expert annotating the text field within the tweets. \n", " It is established through manual annotation by a single person. \n", " The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset. ", " Through manual annotation by an expert. ", ", ", " The annotation was carried out by a single person, ", " unanswerable, ", " The ground truth is established by a single expert, who manually inspected the text field within the tweets to label them as containing fake news, or not containing them. \n", " The ground truth for fake news is established by manually annotating tweets by a single person. \n", " Not by the researchers, ", " Manually, by a single person, ", " unanswerable, ", " The annotation was carried out by a single person, ", " The ground truth for fake news is established by manual inspection of the text field within the tweets to label them as containing fake news, or not containing them. \n", " It is established manually by an expert using a publicly available dataset that was labelled according to the characterization presented before.  \n", " It is established manually by an expert through the annotation of the text field within the tweets, ", " The ground truth for fake news is established by manual inspection of the text field within the tweets by an expert., ", " Not claimed to be a ground truth, ", " unanswerable, ", " The ground truth for fake news is established manually by a single expert based on the categorization of Rubin et al.,", " It is established by a single person, ", " unanswerable, ", " Through manual annotation by a single person, "], "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": [" Additional features such as context tweets and latent topic clustering are proposed, ", " Directly used text sequences of context tweets as an additional feature of neural network models, ", " Context tweets, ", " Context tweets, ", " Text sequences of context tweets are directly used as an additional feature of neural network models. ", " Additional context, specifically \"context tweets\", is proposed. \n", " Context tweets, ", " \n", " Additional context tweets, ", " Context tweets. ", " Context tweets are proposed as additional features. \n", " context tweets, ", " context tweets, ", " Context tweets, ", " \n", " Text sequences of context tweets. \n", " context tweets and text sequences, ", " Text sequences of context tweets as an additional feature of neural network models. \n", " The context tweets, a pre-trained GloVe representation, and the latent topic clustering method are proposed as additional features and context. ", " Context tweets and character-level features, ", " Text sequences of context tweets are directly used as an additional feature of neural network models. ", " Context tweets and latent topic clustering (LTC). \n", " Additional features and context include context tweets, character-level features, and Latent Topic Clustering (LTC). \n", " Directly using text sequences of context tweets as an additional feature of neural network models, and using conversation information from context tweets for sentiment analysis, \n", " Directly using text sequences of context tweets as additional features of neural network models. \n", " Directly using text sequences of context tweets. \n", " Text sequences of context tweets, ", " The use of context tweets, character-level features, and latent topic clustering (LTC) modules, ", " Context tweets, ", " Text sequences of context tweets are directly used as an additional feature. ", " Context tweets, ", " Context tweets. ", " Text sequences of context tweets are used as additional features of neural network models. ", " Directly using text sequences of context tweets as an additional feature. ", " Context tweets and character-level features, ", " Context tweets. ", " Directly using text sequences of context tweets as an additional feature. \n\n", " Text sequences of context tweets are directly used as an additional feature of neural network models, ", " Text sequences of context tweets are directly used as an additional feature of neural network models. ", " Text sequences of context tweets are directly used as an additional feature. ", " The authors propose using context tweets as an additional feature and also using self-matching attention mechanism on RNN baseline models and latent topic clustering (LTC) method for better understanding of the data. \n\n", " Additional features and context of context tweets. ", " Context tweets. \n", " Context tweets. \n", " Context tweets, ", " Additional features and context proposed include context tweets, latent topic clustering, self-matching attention mechanism, and text sequences of context tweets. \n", " Directly using text sequences of context tweets as an additional feature, and also including author information, conversation type, use of the same hashtags or emojis are all highly dependent on data. ", " Text sequences of context tweets are directly used as an additional feature of neural network models. ", " Context tweets, ", " Text sequences of context tweets are proposed as an additional feature, along with character-level features. ", " Text sequences of context tweets are directly used as an additional feature. \n", " Context tweets, ", " Context tweets and character-level features. \n", " context tweets, ", " Additional context tweets. ", " Text sequences of context tweets. \n", " Context tweets and latent topic clustering (LTC) are proposed. \n", " Context tweets are proposed as an additional feature. ", " Context tweets. \n", " Context tweets are proposed as additional features and context. ", " The use of context tweets is proposed as an additional feature. \n", " Context tweets, ", " Context tweets as additional features. \n", " Context tweets, ", " Text sequences of context tweets are directly used as an additional feature of neural network models, along with user information, conversation type, use of the same hashtags or emojis, and character-level representations. \n\n", " Text sequences of context tweets are directly used as an additional feature. ", " Context tweets. \n", " The use of context tweets, which include the tweet one has replied to or has quoted, is proposed as an additional feature for neural network models. \n", " Text sequences of context tweets are directly used as an additional feature of neural network models. ", " The use of context tweets is proposed. ", " Context tweets, ", " Context tweets. ", " Context tweets. \n", " Text sequences of context tweets, ", " Text sequences of context tweets are directly used as an additional feature. ", " Direct text sequences of context tweets as an additional feature, ", " Context tweets are proposed as additional features and context. \n", " Additional features and context are proposed to be directly used as text sequences of context tweets, \n", " Additional features and context of context tweets, ", " Additional context of tweets that the user has replied to or has quoted, known as “context tweets”, is proposed. \n", " Directly using text sequences of context tweets as an additional feature. ", " Text sequences of context tweets are directly used as an additional feature of neural network models. \n", " Direct text sequences of context tweets are proposed as an additional feature to improve the baseline neural network models. \n", " Context tweets. \n", " Text sequences of context tweets are directly used as an additional feature of neural network models. ", " Text sequences of context tweets are directly used as an additional feature. \n\n", " \n", " Context tweets, ", " Context tweets are proposed as an additional feature. ", " Additional features and context from context tweets. \n\n", " Text sequences of context tweets, ", " Context tweets as additional features and information. \n", " Additional context tweets. \n", " Context tweets and latent topic clustering are proposed as additional features and context. \n", " Additional features and context of context tweets, ", " Context tweets, ", " Text sequences of context tweets are directly used as an additional feature of neural network models. \n", " Directly using text sequences of context tweets as additional features of neural network models, ", " Context tweets. \n", " Additional features and context proposed include character-level representations, context tweets, and Latent Topic Clustering. \n"], "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": [" A selection of 16 Facebook pages, ", " The pages that were chosen based on a combination of intuition, availability and results on the development data. They specifically used the combined set of Time, The Guardian and Disney as the best model (B-M), ", " Time, The Guardian, and Disney. \n", " The Facebook pages examined were Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", ", ", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " Various pages including Time, The Guardian and Disney, ", " They looked at multiple Facebook pages including Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " Various Facebook pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " The Facebook pages include FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " unanswerable, \n", " The selected Facebook pages are FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " They looked at 17 Facebook pages. \n", " Various public pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and also pages with certain themes like cooking and TV shows. \n", " A variety of Facebook pages, including Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " A variety of public pages. \n", " They looked at 14 different Facebook pages, ", " Time, The Guardian and Disney, ", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " They looked at the following pages: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " The selected Facebook pages for the experiments, ", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, \n\n", " Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " The pages they looked at include FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. \n\n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, \n", " A combination of news and entertainment pages. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, \n\n", " Time, The Guardian, and Disney, \n", " The Facebook pages that they looked at were FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney.  ", " Facebook pages such as FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and others. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", ", ", " They looked at 14 Facebook pages, including Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Note that they also mentioned the reaction \"thankful\" but did not explain it in the reasoning process. \n", " They looked at a balanced and varied selection of Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. They also looked at thankful, which was available during specific time spans related to certain events, such as Mother's Day in May 2016, but this was only for a short period of time. However, they made the decision to select a best model (B-M) namely the combined set of Time, The Guardian and Disney,", " Pages such as FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " The Facebook pages used were FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " The Facebook pages they looked at include: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016. In addition, they also used pages such as Fox News, CNN, Time, The Guardian and Disney for the best model. \n\n", " Time, The Guardian, Disney, Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " They looked at a large number of Facebook pages, ", " They looked at 16 Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " The following Facebook pages were used: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " A combination of different Facebook pages, ", " CNN, ESPN, FoxNews, Justin Bieber, New York Times, Nickelodeon, Spongebob, The Guardian, Time, Cartoon Network, Disney, Home Cooking Adventure, Huffington Post Weird News, Time magazine. \n", " A balanced and varied set of pages. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and also the pages providing thankful reaction were not specified. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " Time, The Guardian, and Disney. \n", " Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " Nickelodeon, Disney, Cartoon Network, FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cooking Light, Home Cooking Adventure, Justin Bieber, Spongebob, and others. \n", " The following Facebook pages: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016. \n\n", " They looked at several Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. \n\n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " They looked at 14 Facebook pages, including FoxNews, CNN, and Disney. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " A set of 14 Facebook pages, ", " Time, The Guardian, Disney, Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " Disney, The Guardian, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney, ", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " They looked at a total of 15 Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " Disney, The Guardian, and Time, among others. \n", " They looked at many different Facebook pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and Huffington Post. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, ", " The selected Facebook pages for the experiments. ", " 13 Facebook pages, ", " CNN, ESPN, Huffington Post Weird News, The Guardian, New York Times, Time magazine, Cartoon Network, Cooking Light, Home Cooking Adventure, Nickelodeon, Spongebob, Disney, Justin Bieber, FoxNews. \n", " They looked at a collection of 19 Facebook pages, including news pages like FoxNews and CNN, and entertainment pages like Cartoon Network and Disney. \n", " Pages about Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, were selected for the experiment. ", " They looked at a collection of Facebook pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and the thankful reaction was only available during specific time spans related to certain events. \n\n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " A combination of 15 pages, ", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " They looked at 15 different Facebook pages. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n\n", " Time, The Guardian and Disney, as well as 14 other pages, ", " They looked at the following Facebook pages: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " A variety of public pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. \n\n", " Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " They looked at 16 different Facebook pages. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " A balanced and varied set of pages, including Fox News, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, among others. \n\n", " Various pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, among others. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, \n", " The following Facebook pages: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Note that thankful was only available during specific time spans related to certain events, as Mother's Day in May 2016. \n", " They looked at 14 different Facebook pages. \n", " The Facebook pages they looked at include FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " Disney, Time, The Guardian, FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. \n\n", " They looked at several Facebook pages, including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney, ", " Time, The Guardian, and Disney, ", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. Answer was also narrowed down to a best model (B-M) named the combined set of Time, The Guardian and Disney., ", " The Facebook pages include FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, and Disney. \n\n", " A set of pages including FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. ", " Disney, Time, The Guardian, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n", " FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and others. \n"], "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": [" 68.8% to 71.8%, ", " By 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " By 68.8% to 71.8%. \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, \n", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " By 68.8% to 71.8%. \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, \n", " By 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " By 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, \n", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " 68.8% to 71.8%, ", " By 68.8% to 71.8%, ", " 68.8% to 71.8%, "], "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": [" extrinsic evaluation, ", " Extrinsically evaluation. \n", " extrinsic evaluation, ", " extrinsic evaluation, ", " extrinsic evaluation, ", " extrinsic evaluation, ", " extrinsic evaluation ", " Extrinsic evaluation, ", " extrinsic evaluation, \n", " Extrinsic evaluation. \n", " extrinsic evaluation, ", " extrinsic evaluation, ", " Extrinsic evaluation. \n", " Extrinsic evaluation. \n", " extrinsic evaluation. ", " Extrinsic evaluation. \n", " Extrinsic evaluation. \n", " Extrinsic evaluation. \n", " Extrinsic evaluation. \n", " extrinsic evaluation \n", " extrinsic evaluation. \n", " [Your concise final answer here], ", " extrinsic evaluation. \n", " Extrinsic evaluation. \n", " extrinsic evaluation, ", " Extrinsic evaluation, ", " extrinsic, ", " extrinsic evaluation. \n", " extrinsic, ", " Intrinsic, extrinsic, and reliability studies, and also evaluation in real-world application scenarios. \n", " Extrinsic, ", " extrinsic evaluation, ", " Extrinsic evaluation. \n", " extrinsic evaluation, \n", " Extrinsic evaluation. \n", " extrinsic evaluation \n", " Extrinsic evaluation. \n", " Extrinsically evaluation, ", " extrinsic evaluation. \n", " extrinsic, ", " Extrinsic evaluation. \n", " Extrinsically,", " Extrinsically evaluation. \n", " Extrinsic evaluation. \n", " Extrinsically evaluation. \n", " intrinsic, ", " extrinsic evaluation, ", " Extrinsic evaluation. \n", " extrinsic evaluation, ", " Extrinsic evaluation. ", " intrinsic, ", " extrinsic, ", " extrinsic evaluation, ", " extrinsic evaluation, ", " Extrinsic evaluation. \n", " Extrinsic evaluation. \n", " extrinsic evaluation, ", " Extrinsic evaluation. \n", " Intrinsic evaluation. \n", " Extrinsic, ", " Extrinsically evaluation and intrinsic evaluation. ", " extrinsic evaluation, \n", " Extrinsically. \n", " extrinsic evaluation. \n", " Extrinsic evaluation. \n", " Extrinsically. \n", " Extrinsic evaluation, ", " Extrinsically evaluation, \n", " extrinsic evaluation, ", " intrinsic, ", " Extrinsic evaluation. ", " extrinsic evaluation \n", " Extrinsic evaluation, ", " unanswerable, ", " [Your concise final answer here], ", " Extrinsically evaluation, ", " extrinsic evaluation, ", " extrinsic evaluation, \n", " extrinsic evaluation, ", " Extrinsically evaluation. ", " Extrinsic. \n", " extrinsic evaluation, ", " extrinsic evaluation, ", " Extrinsically,", " Extrinsically and intrinsic evaluations. \n", " extrinsic evaluation. \n", " extrinsic evaluation. \n", " extrinsic evaluation, ", " Extrinsic evaluation, ", " Extrinsic evaluation. \n", " Extrinsic evaluation, ", " Extrinsically evaluation and reliability studies. \n", " extrinsic evaluation, ", " Extrinsic evaluation. ", " extrinsic, ", " Extrinsic evaluation \n", " Extrinsically evaluation, ", " Extrinsic evaluation, ", " Extrinsically evaluation. \n", " Extrinsic evaluation, "], "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": [" yes, ", " yes, ", " yes, ", " No, unanswerable. \n", " no, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " no, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " no, ", " No, unanswerable, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " no, \n", " yes, ", " no, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " No, unanswerable, yes, ", " yes, ", " yes, ", " no, \n", " no, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " no, ", " yes, ", " no, ", " Yes, ", " yes, ", " yes, ", " no, ", " yes, ", " yes, ", " yes, ", " no, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " no, ", " yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": [" CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. ", " The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " The CNN/DailyMail, NYT, and XSum datasets. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. ", " CNN/DailyMail, NYT, and XSum datasets. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, XSum, ", " The datasets used for evaluation are the CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, and XSum datasets, ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum. \n", " CNN/DailyMail, NYT, XSum. \n", " The datasets used for evaluation are CNN/DailyMail, NYT and XSum. ", " CNN/DailyMail, NYT, and XSum, ", " The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. ", " CNN/DailyMail, NYT, and XSum. ", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " The datasets used for evaluation are CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum. \n", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, and XSum, ", " CNN/DailyMail, NYT, and XSum. ", " CNN/DailyMail, NYT, XSum. \n", " CNN/DailyMail, NYT, and XSum. \n", " CNN/DailyMail, NYT, and XSum, "], "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": [" The IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews. ", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset of movie reviews, \n", " IMDb dataset. \n", " The IMDb dataset, ", " IMDb dataset of movie reviews. ", " IMDb dataset, ", " The IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews, ", " The IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " The IMDb dataset of movie reviews. \n", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " The IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " The IMDb dataset of movie reviews. \n", " IMDb dataset, ", " The IMDb dataset, ", " The IMDb dataset of movie reviews, ", " The IMDb dataset. \n", " IMDb dataset, ", " IMDb dataset of movie reviews, \n", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " The IMDb dataset. \n", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset of movie reviews, ", " The IMDb dataset of movie reviews. \n", " IMDb dataset. \n", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " The IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset of movie reviews, \n", " The IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " The IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset, ", " The IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews. \n", " IMDb dataset, ", " IMDb dataset of movie reviews, \n", " IMDb dataset, ", " IMDb dataset. \n", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset, ", " IMDb dataset of movie reviews, \n", " IMDb dataset, ", " IMDb dataset of movie reviews. ", " IMDb dataset, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews. \n", " IMDb dataset of movie reviews, ", " The IMDb dataset of movie reviews. \n", " The IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset of movie reviews, ", " IMDb dataset, ", " IMDb dataset of movie reviews, ", " IMDb dataset, "], "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": [" English, ", " English, ", " English, \n", " English, ", " English, ", " English, \n", " English, ", " English, ", " [Your final answer here], ", " English, \n", " English, ", " English, \n", " English, ", " English, \n", " English, \n", " English, \n", " English, ", " English, \n", " English, ", " English, ", " English, ", " English, ", " [Your final answer here], ", " English, ", " English, ", " English. \n", " English, ", " English, \n", " English, ", " English, \n", " English, ", " English, ", " English, \n", " English, ", " [Your final answer here], ", " English, ", " English. \n", " English. \n", " English, ", " English, ", " English, ", " English. \n", " English, ", " English. \n", " English, ", " [Your concise final answer here], ", " [Your concise final answer here], ", " English, \n", " English. \n", " English, \n", " English, ", " English, ", " English, ", " English, ", " English, ", " English. \n", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English. \n", " English, ", " English, ", " English, ", " English, ", " English, \n", " English, ", " English, ", " English, ", " English, ", " English, ", " English. \n", " English, ", " English, ", " English, \n", " English, \n", " English, ", " English, ", " English, ", " English, \n", " English, ", " English, \n", " English, ", " English, ", " English, \n", " English, ", " English, ", " English, \n", " English, \n", " English, \n", " English. \n", " English, \n", " English, ", " English, \n", " English, ", " English, \n", " English, "], "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": [" It outperforms them. \n", " Better, ", " It performs better than w2g and w2gm approaches. \n", " It performs better on word similarity and entailment tasks, according to the experiments on benchmark datasets. \n", " GM-KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " GM-KL achieves better correlation than existing approaches for various metrics on SCWS dataset, ", " This approach outperforms other approaches. \n", " It performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " The approach outperforms other existing approaches in most cases, according to the experimental results. \n", " No clear improvement, ", " It performs better on most of the datasets. \n", " It performs better than existing approaches for various metrics on SCWS dataset, ", " Unanswerable, ", " It performs better, ", " This approach performs better than other approaches on the benchmark word similarity and entailment datasets, according to the conclusion of the article. ", " It achieves better correlation than existing approaches. \n", " This approach performs better than other approaches, ", " It achieves better correlation than existing approaches for various metrics, and for most of the datasets, GM$\\_$KL achieves significantly better correlation score than w2g and w2gm approaches. \n", " It performs better, ", " GM-KL outperforms other approaches on word similarity and entailment datasets, ", " GM_KL performs better than other approaches on the benchmark word similarity and entailment datasets, except in certain datasets such as MC and RW that do not contain entailed data. \n", " The proposed approach performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " This approach outperforms other word embedding-based approaches, including w2g and w2gm, on benchmark word similarity and entailment datasets. \n", " Better correlation, ", " It performs better. \n", " It outperforms other approaches on various word similarity and entailment benchmarks. \n", " It performs better than existing approaches for various metrics on the SCWS dataset. \n", " It performs better. \n", " GM-KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " It achieves better correlation scores on various datasets. ", " GM_KL achieves better correlation than existing approaches for various metrics on SCWS dataset. ", " It performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " It performs better, ", " The proposed approach GM_KL achieves better correlation than existing approaches for various metrics on the SCWS dataset, and significantly better correlation score than w2g and w2gm approaches on the benchmark word similarity datasets. \n", " This approach performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " The proposed approach achieves better performance on various benchmark word similarity and entailment datasets. \n", " It achieves significantly better correlation than existing approaches, ", " GM_KL performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " It performs better than the other approaches on the benchmark word similarity and entailment datasets. \n", " The approach performs better than existing approaches for various metrics on SCWS dataset and multiple benchmark word similarity datasets, as shown by the higher correlation values. \n", " It outperforms previous approaches, ", " It performs better. ", " Better, ", " Better, ", " It performs better, ", " This approach performs better, ", " GM_KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " This approach outperforms other approaches, ", " This approach achieves better correlation than existing approaches on various metrics for word similarity and entailment tasks. \n", " GM_KL performs better than other approaches, ", " GM_KL achieves better correlation than existing approaches, ", " The proposed approach outperforms existing approaches on the benchmark word similarity and entailment datasets. \n", " GM_KL achieves better correlation than existing approaches, including w2g and w2gm. \n", " It achieves better correlation than existing approaches for various metrics, ", " Better performance. ", " It performs better than the other approaches. \n", " It achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " It outperforms existing approaches. \n", " It outperforms them, ", " The approach proposed in this paper performs better than other approaches on the benchmark word similarity and entailment datasets. \n", ", ", " It performs better on most benchmarks, ", " The proposed approach performs better than other approaches on benchmark word similarity and entailment datasets. \n", " It performs better, ", " It achieves better correlation scores than existing approaches for various metrics on word similarity and entailment datasets. \n", " This approach outperforms existing word similarity and entailment tasks, ", " GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " It performs better than previous approaches on various metrics on word similarity and entailment datasets, including Spearman correlation on the SCWS dataset and on word similarity datasets such as SL, WS, WS-R, WS-S, MEN, RG, and MTurk-287 and MTurk-771. \n", " Better correlation, ", " The proposed approach achieves better correlation than existing approaches for various metrics on SCWS dataset and benchmark word similarity datasets, including significant improvement in most datasets, except MC and RW which consists of only a single sense. \n", " It performs better than existing approaches. \n", " It outperforms other approaches, ", " The approach shows better performance on the word similarity and entailment datasets compared to existing approaches. \n", " It performs better than w2g and w2gm approaches. \n", " It performs better on most benchmark word similarity datasets. ", " It achieves better performance, ", " It performs better than existing approaches for various metrics on SCWS dataset and benchmark word similarity datasets. \n", " It performs better, ", " GM_KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " It outperforms existing approaches for various metrics on SCWS dataset. \n", " The approach performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " The GM-KL approach performs better than other approaches. \n", " It performs better than other approaches on benchmark word similarity and entailment datasets. \n", " It performs better. \n", " It performs better, ", " Better, ", " GM_KL performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " This approach performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " This approach performs better than other approaches, ", " It performs better than existing approaches, such as w2g and w2gm, on word similarity and entailment datasets. \n", " The proposed approach GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " This approach outperforms other WSD approaches employing word embeddings, ", " The proposed approach performs better than existing approaches on the benchmark word similarity and entailment datasets., ", " It outperforms them. \n", " It achieves better correlation than existing approaches for various metrics on SCWS dataset. \n", " It performs better, ", " It performs better than other approaches.\n", " GM_KL performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " It performs better than other approaches on the benchmark word similarity and entailment datasets. \n", " It outperforms previous approaches on several benchmark datasets. \n"], "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": [" The ensemble method involves averaging the predictions from the constituent single models. The models are selected based on their performance on the validation dataset, with the best performing model initially selected, and then the best performing model that has not been previously tried is added to the ensemble if it improves the validation performance. The process continues until all available models have been tried once, resulting in a final ensemble of selected models. ", " They select the best performing model according to validation performance and then add the best performing model that had not been previously tried until they have tried each model once. \n\n", " They use a greedy ensemble method that involves selecting models in an order of performance. ", " The ensemble method works by selecting and averaging the predictions from the constituent single models. \n", " It involves selecting a greedy ensemble by averaging predictions from constituent single models. ", " They average the predictions from the constituent single models. ", " It selects a subset of models using a greedy algorithm, ", " They form the ensemble by averaging the predictions from the constituent single models, and select these models based on their validation performance. \n", " The ensemble method is based on selecting the best performing model according to validation performance, then adding the best performing model that had not been previously tried, and discarding it if it does not improve the validation performance. \n", " They used a greedy ensemble method to select 5 constituent models. \n", " Their ensemble method selects models using a greedy algorithm that adds the best performing model that had not been previously tried and keeps it if it improves the validation performance, and forms a final ensemble by simply averaging the predictions from the constituent single models. \n", " Their ensemble method is formed by averaging the predictions from the constituent single models. ", " They form the ensemble by simply averaging the predictions from the constituent single models. ", " It selects models in a greedy way, ", " They form an ensemble by selecting the best performing models and simply averaging their predictions. \n", " The ensemble method involves selecting the best performing model according to validation performance, then adding the best performing model that had not been previously tried, and finally discarding it if it does not improve its validation performance. \n", " They simply average the predictions from the constituent single models. \n", " The ensemble is formed by simply averaging the predictions from the constituent single models. The single models are selected using a greedy algorithm. ", " The ensemble method involves averaging the predictions from the constituent single models, selected using a greedy algorithm. \n", " \n", " It uses a greedy ensemble, which starts with the best performing model and then adds the best performing model that had not been previously tried until it improves the validation performance. \n", " It averages the predictions from the constituent single models. \n", " They form the ensemble by simply averaging the predictions from the constituent single models. ", " Their ensemble method involves selecting models using a greedy algorithm that adds the best performing model to the ensemble until no improvement is seen. ", " The ensemble method consists of simply averaging the predictions from the constituent single models. ", " The ensemble method works by starting with the best performing model according to validation performance and then gradually trying to add the best performing model that had not been previously tried, keeping it in the ensemble if it improves its validation performance. \n", " They simply averaged the predictions from the constituent single models. \n", " The ensemble method selects models using the algorithm that starts with the best performing model, then tries adding the best performing model that had not been previously tried, and keeps it in the ensemble if it improves its validation performance and discards it otherwise. ", " They select models that improve validation performance, and then discard them if they don't improve. \n", " It averages the predictions from the constituent single models, ", " The ensemble is formed by simply averaging the predictions from the constituent single models. \n", " The ensemble method uses greedy selection to choose models to include in the ensemble, which involves selecting the best performing model according to validation performance and adding the best performing model that has not been previously tried until no further improvement is seen. The ensemble is formed by simply averaging the predictions from the constituent single models. \n", " The ensemble method works by simply averaging the predictions from the constituent single models. \n", " The ensemble method works by gradually adding the best performing model that had not been previously tried to the ensemble if it improves its validation performance. \n", " The ensemble method works by averaging the predictions from the constituent single models. ", " The ensemble method forms a greedy ensemble by averaging the predictions from the constituent single models and gradually tries each model once to select the best performing models. ", " It selects models and keeps them in the ensemble if they improve its validation performance and discards them otherwise. \n", " Their ensemble method involves selecting 5 of the 10 best models, and then averaging their predictions to get the final result. \n", " Their ensemble method works by averaging the predictions from the constituent single models. \n", " They use a greedy ensemble method, ", " They use a greedy ensemble selection, ", " The ensemble method forms a model by selecting models that improve the validation performance and gradually adding them to the ensemble. ", " \n", " The ensemble is formed by simply averaging the predictions from the constituent single models. \n", " The ensemble is formed by simply averaging the predictions from the constituent single models, and the constituent models are selected using a greedy algorithm. ", " It averages the predictions from the constituent single models. ", " It forms a greedy ensemble by selecting models that improve the validation performance, ", " It forms the ensemble by simply averaging the predictions from the constituent single models. \n", " The ensemble method works by averaging the predictions from the constituent single models. \n", " \n", " They form the ensemble by simply averaging the predictions from the constituent single models. ", " Their ensemble method is a greedy ensemble formed by averaging the predictions from the constituent single models. ", " They form a greedy ensemble by selecting models that improve the validation performance. \n", " \n", " They use a greedy ensemble, formed by simply averaging the predictions from the constituent single models. \n", " It selects 5 models and averages their predictions. \n", " Their ensemble method works by simply averaging the predictions from the constituent single models. \n", " They use a greedy ensemble method by selecting single models and adding them if they improve validation performance. \n", " Their ensemble method involves adding the best performing model that had not been previously tried to the ensemble, and discarding it if it does not improve the validation performance. ", " The ensemble method works by selecting models that improve validation performance and averaging their predictions. \n", " The ensemble method works by simply averaging the predictions from the constituent single models, starting with the best performing model and gradually adding other models that improve validation performance, while discarding ones that do not. \n\n", " The ensemble is formed by simply averaging the predictions from the constituent single models. ", " The ensemble method involves selecting the best performing model according to validation performance and then gradually trying each model once by adding the best performing model that had not been previously tried, ", " Their ensemble method is based on simply averaging the predictions from the constituent single models. ", " They start with the best performing model, then try adding the best performing model not previously tried, and keep it in the ensemble if it improves the validation performance. \n", " Their ensemble method involves selecting models and averaging their predictions, ", " It averages the predictions from the constituent single models. \n", " They form an ensemble by averaging the predictions from the constituent single models, selecting the models based on their performance. \n", " They simply average the predictions from the constituent single models. \n", " The ensemble method is a greedy one that selects the best-performing model according to validation performance and adds the best-performing model that has not been previously tried. \n\n", " They form the ensemble by simply averaging the predictions from the constituent single models. \n", " The ensemble method works by selecting models that had not been previously tried, adding them to the ensemble if they improve validation performance, and discarding them otherwise. \n", " It selects 5 models that had the best validation performance, then forms an ensemble by averaging their predictions. ", " The ensemble is formed by simply averaging the predictions from the constituent single models. The best performing model is selected first, then the best performing model that had not been previously tried is added and kept if it improves the validation performance, otherwise it is discarded. This greedy process is repeated until a predetermined number of models are selected. \n\n", " They use a greedy ensemble method, selecting models based on their validation performance and discarding models that do not improve the validation performance. \n\n", " It selects the models that improve validation performance, ", " The ensemble method involves selecting single models and forming an ensemble by averaging their predictions. \n", " Their ensemble method involves forming a model by simply averaging the predictions from the constituent single models, selected through a greedy algorithm. \n\n", " The ensemble method involves selecting models and adding them to the ensemble if they improve the validation performance and discarding them otherwise. \n", " The ensemble method works by selecting the best performing model according to validation performance and then gradually adding the best performing model that had not been previously tried. \n", " Their ensemble method works by selecting the best performing model according to validation performance and then adding the best performing model that had not been previously tried, and keeping it in the ensemble if it improves its validation performance. \n", " Their ensemble method works by simply averaging the predictions from the constituent single models. \n", " It involves averaging the predictions from the constituent single models selected using a greedy ensemble algorithm. \n", " The ensemble method works by averaging the predictions from constituent single models. \n", " The ensemble method forms a greedy ensemble, ", " Their ensemble method is a greedy ensemble formed by selecting the best performing models. \n", " It averages the predictions from the constituent single models. \n", " Their ensemble method is based on a greedy selection of models. \n", " It averages the predictions from the constituent single models. \n", " They simply average the predictions from the constituent single models. \n", " The ensemble method involves selecting the best performing model, then trying to add the best performing model that had not been previously tried, and keeping it in the ensemble if it improves validation performance, discarding it otherwise. The procedure is repeated until all models have been tried, with the resulting models forming a greedy ensemble. ", " Our model was implemented using Theano and Blocks, and the ensembles were formed by simply averaging the predictions from the constituent single models, with the algorithm trying adding the best performing model that had not been previously tried, and discarding it otherwise., ", " Their ensemble method works by averaging the predictions from the constituent single models, ", " It simply averages the predictions from the constituent single models. \n", " It is based on averaging the predictions from constituent single models. \n", " The ensemble method selects a set of models and averages their predictions, with the goal of forming the best possible ensemble, ", " The ensemble is formed by simply averaging the predictions from the constituent single models. ", " The ensemble method involves averaging the predictions from the constituent single models. \n", " They form the ensemble by simply averaging the predictions from the constituent single models. \n", " Their ensemble method works by simply averaging the predictions from the constituent single models. \n"], "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": [" Friends is speech-based dialogues from the scripts of the Friends TV sitcom, EmotionPush is chat-based dialogues from Facebook messenger chats. ", " Friends and EmotionPush, ", " TV sitcom and Facebook messenger chats, ", " Friends and EmotionPush, ", " Friends and EmotionPush, ", " EmotionPush is a Facebook messenger chats dataset, and Friends is a TV sitcom script dataset. ", " The two datasets are EmotionLines, which comes from two subsets: Friends and EmotionPush, and the sources of the datasets are the scripts of the Friends TV sitcom and Facebook messenger chats, respectively. \n", " EmotionLines dataset, ", " Friends TV sitcom and Facebook messenger chats, ", " Friends TV sitcom and Facebook messenger chats, ", " Friends TV sitcom and Facebook messenger chats, ", " Friends and EmotionPush datasets. ", " EmotionLines dataset is composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. \n", " Speech-based dialogues from TV shows and chat-based dialogues from Facebook messenger chats, ", " Friends is from scripts of Friends TV sitcom and EmotionPush is from Facebook messenger chats. \n", " EmotionLines dataset has two subsets, Friends and EmotionPush, which are composed of the scripts of the Friends TV sitcom and Facebook messenger chats, respectively. \n", " Friends and EmotionPush. ", " The Friends dataset comes from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. \n", " Friends is from the scripts of the Friends TV sitcom and EmotionPush is from Facebook messenger chats. \n", " Two subsets of the EmotionLines dataset, namely Friends and EmotionPush. ", " Friends and EmotionPush are from TV sitcom scripts and Facebook messenger chats, respectively, ", " The sources of the datasets are two subsets of the EmotionLines dataset, Friends and EmotionPush, which are from the scripts of the Friends TV sitcom and Facebook messenger chats respectively. \n", " EmotionLines dataset has two subsets: Friends and EmotionPush, according to the source of the dialogues. ", " EmotionPush and Friends, ", " EmotionLines, ", " TV sitcom scripts and Facebook messenger chats, ", " Two subsets: Friends and EmotionPush. ", " EmotionLines, ", " TV scripts and Facebook messenger chats, ", " Friends and EmotionPush, ", " TV sitcom and Facebook messenger chats, ", " The Friends dataset comes from the scripts of the Friends TV sitcom and the EmotionPush dataset comes from Facebook messenger chats. \n", " Friends TV sitcom and Facebook messenger chats. \n", " Two subsets, Friends and EmotionPush, according to the source of the dialogues, ", " Friends TV sitcom and Facebook messenger chats, ", " Friends TV sitcom and Facebook messenger chats, ", " Friends is a speech-based dataset which is annotated dialogues from the TV sitcom, and EmotionPush is a chat-based dataset which is made up of Facebook messenger chats. \n", " Friends dataset comes from the scripts of the Friends TV sitcom, and EmotionPush dataset is made up of Facebook messenger chats, \n", " The Friends dataset comes from the scripts of the Friends TV sitcom, and the EmotionPush dataset is made up of Facebook messenger chats. \n", " EmotionLines dataset consists of two subsets, Friends and EmotionPush, according to the source of the dialogues. ", " Friends and EmotionPush, ", " The two datasets are sourced from Friends TV sitcom and Facebook messenger chats. ", " Friends is speech-based dialogues and EmotionPush is chat-based dialogues. ", " Friends and EmotionPush, ", " Two subsets: Friends and EmotionPush. ", " Friends and EmotionPush, ", " Friends and EmotionPush. \n", " Friends is a speech-based dataset and EmotionPush is a chat-based dataset, ", " The Friends and EmotionPush datasets. \n", " Friends dataset comes from scripts of the Friends TV sitcom, while EmotionPush dataset comes from Facebook messenger chats. \n", " Friends and EmotionPush, ", " TV sitcom scripts and Facebook messenger chats, ", " The datasets Friends and EmotionPush come from two different sources, with Friends being sourced from the scripts of the Friends TV sitcom and EmotionPush from Facebook messenger chats. \n", " EmotionLines dataset consists of two subsets: Friends and EmotionPush, according to the source of the dialogues. ", " EmotionLines dataset composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. ", " Friends is a speech-based dataset, and EmotionPush is a chat-based dataset. \n", " EmotionPush is a dataset made up of Facebook messenger chats, while Friends comes from the scripts of the Friends TV sitcom., ", " Friends dataset comes from the scripts of the Friends TV sitcom and EmotionPush dataset from Facebook messenger chats. ", " Friends and EmotionPush, ", " The two datasets are from Friends TV sitcom scripts and Facebook messenger chats, respectively. \n", " The two datasets are from two sources, TV sitcom and Facebook messenger chats. \n", " Friends and EmotionPush, ", " The Friends dataset comes from the scripts of the Friends TV sitcom, while the EmotionPush dataset is made up of Facebook messenger chats. ", " EmotionPush comes from Facebook messenger chats, and Friends comes from the scripts of the Friends TV sitcom, ", " Friends TV sitcom and Facebook messenger chats. ", " Two subsets, Friends and EmotionPush, according to the source of the dialogues.  ", " Friends and EmotionPush datasets come from the scripts of Friends TV sitcom and Facebook messenger chats, respectively. \n", " Friends and EmotionPush datasets, ", " The datasets are from the scripts of the Friends TV sitcom and Facebook messenger chats. \n", " Friends TV sitcom and Facebook messenger chats, ", " Friends and EmotionPush, ", " Friends is a speech-based dataset and EmotionPush is a chat-based dataset, ", " Friends TV sitcom and Facebook messenger chats, ", " Friends TV sitcom and Facebook messenger chats. ", " Friends dataset comes from the scripts of the Friends TV sitcom, and EmotionPush dataset is made up of Facebook messenger chats. ", " The two datasets are sourced from Friends TV sitcom scripts and Facebook messenger chats. \n", " Friends and EmotionPush. ", " Friends TV sitcom and Facebook messenger chats, ", " Friends TV sitcom and Facebook messenger chats, ", " The datasets are sourced from TV sitcom and Facebook messenger chats. ", " The sources of the datasets are the scripts of the Friends TV sitcom and Facebook messenger chats, ", " EmotionLines is composed of two subsets, Friends and EmotionPush, which are derived from scripts of the Friends TV sitcom and Facebook messenger chats, respectively. ", " Friends and EmotionPush datasets are from different sources, ", " EmotionLines dataset comes from two subsets, Friends and EmotionPush, from the scripts of the Friends TV sitcom and Facebook messenger chats, respectively, and from Twitter, respectively. \n", " EmotionLines dataset is composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. ", " Friends is a speech-based dataset from the TV sitcom, and EmotionPush is a chat-based dataset from Facebook messenger chats. ", " Friends and EmotionPush datasets, ", " The TV sitcom scripts and Facebook messenger chats, ", " The datasets are from social media and TV sitcoms. \n", " Friends is speech-based dialogues from the scripts of the Friends TV sitcom and EmotionPush is chat-based dialogues from Facebook messenger chats. \n", " The sources are Friends TV sitcom and Facebook messenger chats, ", " Friends dataset comes from the scripts of the Friends TV sitcom and EmotionPush dataset comes from Facebook messenger chats. ", " Friends and EmotionPush datasets come from different sources. \n", " Friends TV sitcom and Facebook messenger chats, ", " The Friends dataset comes from the scripts of the Friends TV sitcom, while the EmotionPush dataset comes from Facebook messenger chats. \n\n", " Friends dataset is from the scripts of the Friends TV sitcom and EmotionPush is made up of Facebook messenger chats, ", " Friends is a speech-based dataset from a TV sitcom, EmotionPush is a chat-based dataset from Facebook messenger chats. \n", " Friends and EmotionPush, ", " Friends TV show scripts and Facebook messenger chats, ", " The two datasets are Friends and EmotionPush, "], "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": [" Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, they conducted an extensive validation to ensure the quality of the recorded data. ", " Yes, ", " yes, \n", " yes, ", " Yes, ", " Yes, ", " Yes, \n", " yes, \n", " yes, ", " yes, ", " yes, ", " yes, ", " yes, \n", " Yes, \n", " Yes, ", " Yes, \n", " yes, ", " Yes, ", " Yes, \n", " yes, ", " Yes, ", " yes, ", " Yes, they did. \n", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, \n", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, \n", " Yes, ", " yes, ", " Yes, they conducted a detailed technical validation of the data as proof of the quality of the recordings. \n", " Yes, \n", " Yes, they did. \n", " yes, ", " Yes, \n", " Yes, \n", " Yes, \n", " Yes, \n", " Yes, \n", " yes, ", " Yes, they conducted an extensive validation to ensure the quality of the recorded data. \n", " Yes, \n", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, \n", " Yes, ", " Yes, ", " yes, ", " Yes, they did. ", " Yes, they did. \n", " yes, ", " Yes, \n", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, they did. \n", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, \n", " yes, ", " Yes, \n", " yes, ", " Yes, \n", " Yes, ", " yes, ", " Yes, they did. \n", " yes \n\n", " Yes, \n", " Yes, ", " yes, \n", " yes, ", " Yes, ", " Yes, ", " Yes, they conducted an extensive validation to ensure the quality of the recorded data. \n"], "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": [" Not directly stated, but outperforms other models with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ. \n", " The proposed system achieves the best accuracy of all models considered in the experiments, outperforming all the other models significantly (p-value below $10^{-5}$). \n", " outperforms the other models, ", " Significantly better than all other models. \n\n", " The proposed system outperforms all other models significantly with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ. Its F1 value is the best, and significantly better than all the other models. \n\n", " It achieves the highest accuracy among all the models tested. \n", " Significantly better than all other models. \n\n", " \n", " +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively. \n\n", " The proposed system achieves an F1 value of 77.45 on DL-PS, 72.85 on EC-MT, and 80.29 on EC-UQ. \n", " 1.08 F1 improvements on DL-PS, 1.24 on EC-MT, and 2.38 on EC-UQ. \n\n", " significantly better than all the other models, ", " Significantly better than all the other models, ", " The proposed system achieves +2.38 F1 improvement on EC-UQ. \n", " The proposed system achieves the best performance with F1 score of 94.28 on the DL-PS dataset. \n", " ALCrowd performs the best, and significantly better than all the other models (the p-value is below $10^{-5}$ by using t-test). \n\n", " The proposed system outperforms other models and achieves the best performance with a significant margin (p-value below $10^{-5}$ by using t-test), ", " The proposed system outperforms the strong baseline systems, and significantly better than all the other models (the p-value is below $10^{-5}$ by using t-test), ", " outperforms strong baseline systems, ", " ALCrowd performs the best, and significantly better than all the other models, with the results indicate that with the help of adversarial training, our system can learn a better feature representation from crowd annotation., ", " +2.38 F1 improvements on EC-UQ. \n", " outperforms all the other models with a significant margin and achieves the best performance on all three data sets, with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively. \n", " It significantly outperforms all the other models and achieves the best result. ", " Outperforms the strong baseline systems with significant margin. \n", " The proposed system achieves the highest F1 value of 78.12% among all the systems, ", " The proposed system achieves better performances than the strong baseline systems, with an improvement of +1.08 F1 on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ. \n\n", " The proposed system achieves the best F1 value, 94.08%, on the DL-PS dataset, and 85.42% on the EC-MT dataset, and 85.19% on the EC-UQ dataset. \n\n", " +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ. ", " +2.38 F1 improvements on EC-UQ. \n", " Outperforms strong baseline systems, ", " The proposed system outperforms all other models and achieves an F1 score of +2.38 on EC-UQ. \n", " Best F1 score, ", " \n", " \n", " The proposed system achieves the best accuracy, significantly better than all the other models. ", " It significantly outperforms all the other models. \n", " significantly better than all the other models (the p-value is below $10^{-5}$ by using t-test). \n", " Best accuracy is +2.38 F1 improvements on EC-UQ, ", " The proposed system outperforms strong baseline systems and achieves the best F1 value of 67.5% on DL-PS. \n\n", " , ", " It achieves the best performance and significantly better than all the other models, with p-value below 10^-5. \n", " The proposed system achieves the highest accuracy among all the systems with a significant margin. \n\n", " The proposed system achieves the highest accuracy of 92.16% F1 on DL-PS dataset and the highest accuracy of 88.54% F1 on EC-MT and EC-UQ datasets. ", " 1.24 on EC-MT, \n", " 1.08 F1 improvement on DL-PS, 1.24 on EC-MT, and 2.38 on EC-UQ, ", " \n", " \n", "  outperforms strong baseline systems, \n", " Not directly mentioned in the results, ", " \n", " Best, ", " 1.08 F1 improvements on DL-PS, 1.24 on EC-MT, and 2.38 on EC-UQ. \n\n", " outperforms all other models significantly, with a p-value below 10^-5. \n", " outperforms all the other models, ", " Significantly better than all other models. ", " +1.08 F1 improvement on DL-PS and +1.24 on EC-MT and +2.38 on EC-UQ, respectively. \n", "  It significantly outperforms all other models, including strong baseline systems. \n", " \n", " The proposed system significantly outperforms other models and achieves the best performance, with a F1 value significantly higher than other models, with a p-value below $10^{-5}$ by using t-test. ", " It achieves a best F1 of 79.15 on the test set. \n\n", " ALCrowd performs the best and significantly better than all the other models., ", " ALCrowd performs the best, and significantly better than all the other models (the p-value is below $10^{-5}$ by using t-test). The results indicate that with the help of adversarial training, our system can learn a better feature representation from crowd annotation.\n\n", "  +2.38 F1 improvements, ", " ALCrowd achieves a F1 score of 92.01% on DL-PS, 89.01% on EC-MT, and 94.42% on EC-UQ. \n\n", " The proposed system achieves the best performance with +2.38 F1 improvements on EC-UQ. \n\n", " Outperforms strong baseline systems. \n\n", " ALCrowd performs the best, and significantly better than all the other models with a +1.08 F1 improvement on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively. \n", " outperforms the strong baseline systems with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively, with the p-value being below $10^{-5}$ by using t-test. \n\n", " The proposed system achieves better performances than the state-of-the-art models. \n", " 2.38, ", " The proposed system outperforms the strong baseline system, with F1 improvement of +1.08 on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ. \n\n", " The proposed system achieves significantly better performances than all the other models, with F1 scores ranging from +1.08 to +2.38. \n\n", " +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, and significantly better than all the other models (the p-value is below $10^{-5}$ by using t-test). \n\n", " Best among all systems, ", " Significantly better than all the other models, with a p-value below $10^{-5}$, ", " +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, ", " Significantly better than all the other models, with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ. \n\n", " Significantly better than all other models, ", " Significantly better than all other models, ", " The proposed system achieves the best accuracy, which is significantly better than all the other models, with p-value below 10^{-5}. \n\n", " the best, ", " +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ, respectively, over the baseline LSTM-CRF model. \n", " significantly better, ", " The proposed system achieves an F1 value of 62.13 on the test set for the DL-PS dataset, 68.17 for the EC-MT dataset, and 67.51 for the EC-UQ dataset, respectively. \n", " The proposed system achieves the highest accuracy with F1 value of 1.07, 1.12, and 2.38 respectively for DL-PS, EC-MT, and EC-UQ, but this is not the final answer since it seems there is some calculation missing, and after considering the results from table 2 and table 3, we can see the final answer is actually: \n", " +2.38 F1 improvement on EC-UQ. \n", " The proposed system achieves the highest F1 value of 92.88 on the DL-PS dataset and 86.21 on the EC-MT and EC-UQ datasets. \n\n", " \n", " Best performance of 71.19% on DL-PS, 70.45% on EC-MT, and 73.79% on EC-UQ. \n", " The proposed system achieves better F1 improvements on DL-PS, EC-MT, and EC-UQ datasets. \n", " The proposed system achieves an accuracy of 91.31% F1 score, with a +2.38 F1 improvement on EC-UQ and +1.08 F1 improvements on DL-PS. \n\n", " Unanswerable. \n\n", " Best, ", " It achieves state-of-the-art performances with the best F1 score, and significantly better than all the other models (the p-value is below $10^{-5}$ by using t-test).", " The proposed system achieves significantly better performance than other models, with an average F1 value of 86.23% on DL-PS, 84.91% on EC-MT, and 86.63% on EC-UQ. \n\n", " It outperforms all other systems, and significantly better than all the other models, with the p-value being below 10^-5. \n\n", ", ", " Significantly better than all the other models, and it outperforms LSTM-Crowd on all the datasets consistently. \n", " The proposed system achieves the best accuracy among all the models with a +2.38 F1 improvement on EC-UQ. \n\n", " Significantly better than all the other models, "], "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": [" 97,616 distinct words, a vocabulary of 184,001 Twitter posts and 62,949 news articles. ", " 184,001 Twitter posts and 62,949 news articles, ", " Twitter posts and news articles, ", " CognIA dataset and Switchboard dataset and DSTC (Dialog State Tracking Challenge)., ", " \n", " Domain-specific word vectors and a corpus of 246,945 documents, corresponding to of 184,001 Twitter posts and and 62,949 news articles. \n\n", " The datasets used are the Switchboard dataset and the set of tasks for the Dialog State Tracking Challenge (DSTC), which contain more constrained tasks, for instance the restaurant and travel information sets. Additionally, a set of thousand documents are needed related to desired domain to create domain-specific word vectors, a set of Twitter posts and news articles, all related to finance. ", " The authors used Twitter posts and news articles. \n", " A set of 184,001 Twitter posts, and 62,949 news articles. \n", " News articles and Twitter posts, ", " 184,001 Twitter posts and 62,949 news articles. \n", " Various, ", " The dataset used includes 184,001 Twitter posts and 62,949 news articles. \n", ", ", " The datasets used are Switchboard dataset and the set of tasks for the Dialog State Tracking Challenge (DSTC), which contain more constrained tasks, for instance the restaurant and travel information sets., ", " Finance experts' posts on social media. \n", " The 184,001 Twitter posts and 62,949 news articles related to finance. \n", " 246,945 documents, \n", " a set of 246,945 documents. ", " 184,001 Twitter posts and 62,949 news articles. ", " Domain-specific word vectors. ", " The dataset used was Twitter posts and news articles. \n", " Financial intents in Portuguese. ", " Domain-specific word vectors. \n", " The datasets used include a set of 184,001 Twitter posts, 62,949 news articles and a total of 63,270,124 word occurrences with a vocabulary of 97,616 distinct words. ", " 246,945 documents, BIBREF41, BIBREF42, BIBREF43, BIBREF44, BIBREF45, BIBREF46, BIBREF47, BIBREF48, BIBREF49, BIBREF50, and Switchboard dataset, \n", " The Switchboard dataset and the Dialog State Tracking Challenge (DSTC). ", " The dataset used is composed of 184,001 Twitter posts and 62,949 news articles related to finance. \n", " Finance-related datasets are used, ", " Domain-specific word vectors from 184,001 Twitter posts and 62,949 news articles, ", " The dataset used is a set of 246,945 documents, corresponding to of 184,001 Twitter posts and and 62,949 news articles, all related to finance. \n", " CognIA has employed an incremental approach to create its own training set for the Intent Classifier, which first applied the Wizard of Oz method and from this study, collected a set of 124 questions that the users asked. Additionally, for the development of domain-specific word vectors, a set 246,945 documents were used, corresponding to 184,001 Twitter posts and 62,949 news articles, all related to finance. ", " Tweets, news articles, and Word vectors, ", " CognIA, \n", " Not mentioned, ", " 246,945 documents, 184,001 Twitter posts, and 62,949 news articles, ", " CognIA uses a dataset of 246,945 documents, comprising 184,001 Twitter posts and 62,949 news articles related to finance. \n", " A set of 246,945 documents, including 184,001 Twitter posts and 62,949 news articles, all related to finance. \n", " None is specifically provided, \n", " CognIA employed a set of 246,945 documents corresponding to 184,001 Twitter posts and 62,949 news articles. \n", " The datasets are Twitter posts and news articles related to finance, ", " 63,270,124 word occurrences, 97,616 distinct words, and Twitter posts and news articles related to finance. ", " Domain-specific word vectors. \n", "  There is no public dataset available with financial intents in Portuguese, the authors employed the incremental approach to create their own training set with 37 classes of intents, and a total of 415 samples. Additionally, the authors created domain-specific word vectors by considering a set of 246,945 documents, corresponding to of 184,001 Twitter posts and and 62,949 news articles, all related to finance. They crawled 63,270,124 word occurrences with a vocabulary of 97,616 distinct words. \n\n", " domain-specific word vectors. ", " Tweets and news articles. ", ", ", " the Switchboard dataset and the Dialog State Tracking Challenge,", " \n", " The finance dataset consists of 184,001 Twitter posts and 62,949 news articles, all related to finance, ", " Twitter posts and news articles, ", " Finance-related Twitter posts and news articles, ", " Domain-specific word vectors. \n", " 246,945 documents, consisting of 184,001 Twitter posts and 62,949 news articles. \n", " Domain-specific word vectors, ", " CognIA, Switchboard dataset and DSTC, ", " finance Twitter posts and news articles, \n", " Domain-specific word vectors and the corpus of Twitter posts and news articles. ", " Domain-specific word vectors, ", " \n", " There are several datasets used, including the Switchboard dataset, the Dialog State Tracking Challenge (DSTC) corpus, and a dataset of 246,945 documents related to finance. \n", " A dataset of 246,945 documents. \n", " Finance tweets and news articles, ", " Various datasets, including Twitter and news articles, ", " Domain-specific word vectors crawled from 184,001 Twitter posts and 62,949 news articles, and a set of thousand documents are needed related to desired domain. \n\n", " Domain-specific word vectors for finance domain and a dataset for the intent classifier with 37 classes of intents and 415 samples. \n", " The 184,001 Twitter posts and 62,949 news articles are used as the documents to create domain-specific word vectors, in addition to the crawled 246,945 documents. \n", " The Switchboard dataset, Twitter posts, news articles, and Dialog State Tracking Challenge (DSTC) are used as datasets. \n", " Finance-related tweets and news articles, \n", " Twitter posts and news articles, ", " 246,945 documents, corresponding to of 184,001 Twitter posts and and 62,949 news articles, all related to finance. \n", " \n", " None specific, ", " A Switchboard dataset and a Dialog State Tracking Challenge (DSTC). \n", " Twitter posts and news articles, ", " A set of thousand documents are needed, 184,001 Twitter posts, and 62,949 news articles. \n", " 246,945 documents \n", " There is a financial dataset in Portuguese that was crawled from feeds of blog users who are experts in the finance domain, which contains a total of 63,270,124 word occurrences, with a vocabulary of 97,616 distinct words. Additionally, a news article dataset was extracted from links included in these tweets. \n", " News articles and Twitter posts. \n", " A dataset of 246,945 documents, which consists of 184,001 Twitter posts and 62,949 news articles, is used. Additionally, the Switchboard dataset and the Dialog State Tracking Challenge (DSTC) set of tasks are also mentioned in the article, but the former is described as a corpus of spoken, unconstrained dialogues and the latter contains more constrained tasks, therefore they are not specified in the answer as the primary dataset used. However, they are used as reference corpora for chatbot development. \n", " Financial intents in Portuguese, ", " Various financial intents in Portuguese. \n", " finance domain-related documents, tweets, and news articles, ", "  a set 246,945 documents, corresponding to of 184,001 Twitter posts and and 62,949 news articles, all related to finance, ", " Finance-related documents on Twitter and news articles. \n", "  The datasets include Switchboard dataset, which consists of transcripts of spoken, unconstrained, dialogues, and the set of tasks for the Dialog State Tracking Challenge (DSTC), which contain more constrained tasks, for instance the restaurant and travel information sets. Furthermore, a set of thousand documents are needed related to desired domain for creating domain-specific word vectors, a set of 184,001 Twitter posts, and 62,949 news articles, all related to finance, are used. \n", " The dataset used is not specifically mentioned. However, the authors used a set of 246,945 documents, corresponding to 184,001 Twitter posts and 62,949 news articles, all related to finance. ", " domain-specific word vectors. ", " CognIA dataset and Twitter data, ", " CognIA's training set with 37 classes of intents, and a total 415 samples, with samples per class ranging from 3 to 37, are used. Also, a dataset of 246,945 documents, corresponding to 184,001 Twitter posts and 62,949 news articles, all related to finance. \n\n", " CognIA employs a set of 184,001 Twitter posts and 62,949 news articles, all related to finance. \n", " CognIA has used its own dataset created with the Wizard of Oz method and manually classified into a set of intent classes. Additionally, a set of 246,945 documents related to finance were used to create domain-specific word vectors. \n", " unanswerable, ", " A set of 246,945 documents, corresponding to 184,001 Twitter posts and 62,949 news articles, all related to finance. \n", " The datasets used are the Switchboard dataset and the Dialog State Tracking Challenge (DSTC) sets. ", " CognIA's training set for the Intent Classifier,", " 246,945 documents, \n", " Two well-known corpora are the Switchboard dataset, which consists of transcripts of spoken, unconstrained, dialogues, and the set of tasks for the Dialog State Tracking Challenge (DSTC), which contain more constrained tasks, for instance the restaurant and travel information sets. \n\n", " \n", " \n"], "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": [" By several points. \n", " By several points. \n", " 2 points. ", " Several points. \n", " several points, \n", " By several points. \n", " Several points, \n", " By several points, ", " Several points. \n", " 7.3%  \n", " Several points, ", " 7.8 points, ", " By several points. \n", " By several points. \n", " Several points, ", " 7-10 points, ", " several points, ", " Several points, ", " Several points, ", " several points \n", " By several points, ", " By several points. \n", " By 3.4 points. \n", " 5-6 points \n", " By several points. \n", " By several points, ", " several points, ", " By several points, ", " By several points, ", " Several points. \n", " several points \n", " Several points, \n", " By several points, ", " Several points. \n", " Several points, ", " By several points. \n", " By several points. \n", " several points. \n", " By several points. \n", " By several points. \n", " By several points, ", " 2 points, ", " Several points. \n", " 7 points, ", "  several points \n", " 4 points, ", " By several points, ", " Several points. \n", " Several points. \n", " Several points. \n", " Several points. \n", " Several points, \n", " several points \n", " By several points, ", " Several points. \n", " By several points. \n", " By several points, ", " By several points. \n", " By several points. \n", " By several points, ", "  several points. \n", " By several points, ", " 4-7 points \n", " Several points, ", " By several points. \n", " By several points. \n", "  several points, ", " By several points, ", " Several points, \n", " Several points. \n", " 7.8% \n", " several points, ", "  several points, ", " Several points. \n", " By several points, ", " Several points, ", " Several points, ", " Several points, ", " They improved by several points. \n", " Several points, ", " Several points. \n", " several points, ", " Several points, ", " 2-7 points, ", " Several points, ", " Several points, ", "  Several points, ", " Several points, ", " Several points, ", " several points, ", " Several points. \n", " 2.4, ", " By several points, ", ", ", " Several points, ", " By several points, ", " 7 points, ", " \n", " They improved by 7-8 points for the macro-averaged Mean Absolute Error (MAE) measure. \n", " several points, "], "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": [" 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, and SVM with average word embedding and average transformed word embedding features, and two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), and UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments, all these models were trained on the training set, and parameters as well as the SVM kernel selections were fine-tuned on the development set, and oversampling was applied to the FBFans dataset due to its high imbalanced distribution. \n", " baselines. \n", " Baselines include: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, two mature deep learning models on text classification (CNN and RCNN), and their variants that add comment information and are trained with or without user information, topic information, and comments. \n", " Baselines: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embeddings; 4) two mature deep learning models on text classification, CNN and RCNN; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments. \n", " The baselines are 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM with comment information, CNN with comment information, RCNN with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without LDA model, and 8) UTCNN without comments. \n\n", " Baselines are 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset, 8) UTCNN without comments, in which the model predicts the stance label given only", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " The baselines include: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n\n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without LDA model, and UTCNN without comments. \n\n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments, and Majority. \n", " Baselines are the following: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embedding; 4) two mature deep learning models on text classification, CNN and RCNN; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information; 7) UTCNN without the LDA model; 8) UTCNN without comments. ", " baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), and 5) the above SVM and deep learning models with comment information. ", " The baselines. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without LDA model, UTCNN without comments. \n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. ", " Baselines, ", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " Baselines include: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embedding, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without the LDA model, UTCNN without comments. \n\n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, UTCNN without comments. \n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " Baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN). \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " Baselines: 1) SVM with unigram, bigram, and trigram features; 2) SVM with average word embedding; 3) SVM with average transformed word embedding; 4) CNN; 5) RCNN; 6) UTCNN without user information; 7) UTCNN without LDA model; 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without LDA model, 8) UTCNN without comments. ", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without LDA model, 8) UTCNN without comments. \n", " The baselines. \n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset, and 8) UTCNN without comments, in which the model predicts the stance label given", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " The baselines are 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, where the hyperparameters are based on their work, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings for each user, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments, 9) PSL, 10) ILP, and 11) CRF. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " The baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embedding, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset, 8) UTCNN without comments. \n\n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, and models with comment information. \n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, the baselines with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information. \n", " Baselines: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " Baselines include: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " The baselines are 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, RCNN, and UTCNN without user information, UTCNN without LDA, and UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " Baselines are: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, where the hyperparameters are based on their work, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " The baselines. \n", " The baselines are 8 models. \n", " Baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " Baselines: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without LDA model, 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, the above SVM and deep learning models with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n", " Baselines: SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM with comment information, UTCNN without user information, UTCNN without the LDA model, and UTCNN without comments. \n\n", " Baselines. \n", " Baselines. ", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " Baselines include SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, RCNN, SVM with comment information, CNN with comment information, RCNN with comment information, UTCNN without user information, UTCNN without topic information, and UTCNN without comments. \n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n\n", " Baselines include 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, and 8) UTCNN without comments. \n", " The baselines are 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings for each user; 7) UTCNN without the L", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model, 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset, 8) UTCNN without comments. \n\n", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) RCNN, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. ", " 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments. \n"], "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": [" RNN-based NMT, Transformer-NMT and SMT models, ", " RNN-based NMT and Transformer-NMT, \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, \n", " RNN-based NMT and Transformer-NMT, ", " Transformer and RNN-based NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, and SMT. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, and SMT models, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, SMT models. ", " NMT models, ", " RNN-based NMT model and Transformer based NMT model, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT model, Transformer-NMT model and SMT, ", " RNN-based NMT, Transformer-NMT, \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " They compared RNN-based NMT, Transformer-NMT and SMT. \n", " RNN-based NMT and Transformer-NMT models. \n", " RNN-based NMT and Transformer-NMT. \n", " RNN-based NMT and Transformer-NMT, ", " Transformer and RNN-based NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, SMT, and LCS. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, \n", " RNN-based NMT, ", " [Your concise final answer here], ", " RNN-based NMT and Transformer-NMT. \n", " [Your concise final answer here], ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, SMT. \n", " RNN-based NMT and Transformer, ", " RNN-based NMT and Transformer-NMT. ", " RNN-based NMT and Transformer-NMT models. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT models, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT model and Transformer-NMT model, ", " RNN-based NMT and Transformer-NMT, \n", " RNN-based NMT and Transformer-NMT models, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT models, ", " RNN-based NMT, Transformer-NMT, SMT, \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT models, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, SMT, \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT models, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT. \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, ", " RNN-based NMT, Transformer-NMT, \n", " RNN-based NMT and Transformer-NMT models, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT and SMT. \n", " RNN-based NMT, ", " RNN-based NMT, Transformer-NMT, SMT, and LCS, ", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT, Transformer-NMT, SMT, \n", " RNN-based NMT and Transformer-NMT, ", " RNN-based NMT and Transformer-NMT, ", " Transformer-NMT, RNN-based NMT, \n"], "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": [" The baseline, ", " The baseline model. \n", " The baseline was the context-agnostic MT system. \n", " The CADec model, ", " The baseline was CADec. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. More precisely, the number of layers is N=6 with h=8 parallel attention layers, or heads. The dimensionality of input and output is d_{model}=512, and the inner-layer of a feed-forward networks has dimensionality d_{ff}=2048. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. ", " The baseline model. ", " The baseline model, \n", " [Your concise final answer here], ", " The baseline model. ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model. \n", " The baseline model was a two-pass CADec model. ", " The baseline. \n", " The baseline model. \n", " The baseline model used for comparison in the experiment was the Transformer base model. ", " The baseline model, the CADec model. ", " The baseline model used for comparison. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=2048$. We use regularization as described in BIBREF15. \n", " The baseline model. ", " The context-agnostic MT system, ", " The baseline model used for comparison. \n", " The baseline model used for comparison was CADec, a two-pass model for document-level NMT, and a context-agnostic MT system. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model, the CADec model, and the two-pass model. \n", " The baseline, ", " The baseline model was CADec. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=2048$. We use regularization as described in BIBREF15. As a second baseline, we use the two-pass CADec model BIBREF11. The first pass produces sentence", " The baseline model used in this paper, ", " The baseline was the MT system trained on 6m instances of sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least 0.9. \n", " The CADec model. \n", " The baseline model, the CADec model, was used as the baseline. \n", " The baseline model, CADec, and the model used for back-translation were all Transformer base models. \n", " The baseline model. ", " The baseline, \n", " The baseline model was a context-agnostic MT system.\n", " The baseline model. \n", " The baseline model was the two-pass CADec model. \n", " The baseline was CADec and the context-agnostic MT system. ", " the baseline was the context-agnostic MT system. \n", " the context-agnostic MT system, \n", " The baseline, ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=2048$. We use regularization as described in BIBREF15. As a second baseline, we use the two-pass CADec model BIBREF11. The first pass produces sentence", " The baseline. ", " The baseline model. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model. \n", " The baseline was CADec and a sentence-level Transformer model. \n", " The baseline model, the CADec model. \n", " The baseline model used for comparison is the two-pass CADec model and the standard sequence-to-sequence Transformer. ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model, \n", " The baseline model used for comparison is the two-pass CADec model, ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=2048$. We use regularization as described in BIBREF15., \n", " CADec, \n\n", " The baseline model, CADec, and the DocRepair model are all Transformer base models, and the two-pass CADec model requires document-level parallel training data, while DocRepair only needs monolingual training data. ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models, ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models, ", " The baseline was a context-agnostic MT system trained on 6m instances. \n", " The baseline was the context-agnostic MT system, which was used to produce translations of isolated sentences. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. However, specifically, the baseline model was a context-agnostic MT system that the authors used to generate context-agnostic translations which were then corrected by the DocRepair model. \n", " The CADec model, ", " The baseline model used for back-translation and the DocRepair model,", " the context-agnostic MT system. ", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models, \n", " [Your concise final answer here], ", " The baseline model, \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. More precisely, the number of layers is N=6 with h=8 parallel attention layers, or heads. The dimensionality of input and output is d_model = 512, and the inner-layer of a feed-forward networks has dimensionality d_ff=2048., ", " the context-agnostic MT system. \n", " The baseline. ", " The baseline model. \n", " the two-pass CADec model, ", " The baseline model. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models BIBREF15. More precisely, the number of layers is N=6 with h = 8 parallel attention layers, or heads. The dimensionality of input and output is d_model = 512, and the inner-layer of a feed-forward networks has dimensionality d_ff=2048. We use regularization as described in BIBREF15. As a second baseline, we use the two-pass CADec model BIBREF11. The first pass produces sentence-level translations. The second pass takes both the", " The baseline model, \n", " CADec, ", " The baseline model. \n", " The baseline model was a two-pass CADec model. \n", " The CADec model. \n", " The baseline, \n", " The baseline model was the Transformer base model. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model was the standard sequence-to-sequence Transformer. ", " the baseline was the context-agnostic model, ", " [Your concise final answer here], ", " The CADec model, \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. \n", " The baseline model was a context-agnostic MT model. \n", " The baseline model. \n", " The baseline model was CADec model. \n", " CADec, ", " The baseline model, \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models, which is a standard sequence-to-sequence Transformer. However, the baseline model we specifically refer to is the context-agnostic MT system trained on 6m instances. ", " The baseline model. \n", " The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models. More precisely, the number of layers is $N=6$ with $h = 8$ parallel attention layers, or heads. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer of a feed-forward networks has dimensionality $d_{ff}=2048$. We use regularization as described in BIBREF15. As a second baseline, we use the two-pass CADec model BIBREF11. The first pass produces sentence-level translations. The", " The baseline was a two-pass CADec model. \n"], "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": [" Neutral features, Maximum entropy, and KL divergence of class distribution. \n", " neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Incorporating neutral features, maximum entropy regularization term, and the KL divergence term. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. ", " Neutral features, Maximum entropy, and KL divergence. \n", " Neutral features regularization, maximum entropy regularization term, and KL divergence regularization term, ", " Neutral features, Maximum entropy, and KL divergence. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, Maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, Maximum entropy of class distribution, KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy, and KL divergence. \n", " Regularization term associated with neutral features, maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy, and KL divergence, ", " Neutral features, Maximum entropy of class distribution, KL divergence between reference and predicted class distribution. \n", " Three regularization terms: the neutral features regularization term, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution regularization term. \n", " Neutral features regularization term, maximum entropy of class distribution regularization term, and KL divergence between reference and predicted class distribution. \n", " Incorporating neutral features, maximum entropy regularization, and the KL divergence of class distribution. \n", " The three regularization terms are neutral feature regularization term, maximum entropy of class distribution regularization term, and KL divergence between reference and predicted class distribution regularization term. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features regularization term, maximum entropy of class distribution regularization term, and KL divergence between reference and predicted class distribution regularization term. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution, ", " The three regularization terms are: neutral feature regularization term, maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization term, Maximum entropy of class distribution regularization term, and KL divergence between reference and predicted class distribution regularization term. \n\n", " Neutral features, maximum entropy, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy, and KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " neutral features, maximum entropy, and KL divergence, ", " Neutral features, maximum entropy of class distribution, and the KL divergence between reference and predicted class distribution. \n", " The three regularization terms are neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features regularization, maximum entropy regularization, and KL divergence regularization. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy, and KL divergence. \n", " The three regularization terms are: neutral features, maximum entropy of class distribution, and the KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " Neutral features regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " The three regularization terms are a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " Regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " The three regularization terms are neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution, ", " The three regularization terms are associated with neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution, ", " Three regularization terms to make the model more robust. ", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution, ", " Neutral features, maximum entropy, and KL divergence, ", " Neutral features, maximum entropy, and KL divergence. \n", " Neutral feature regularization term, Maximum entropy regularization term, and KL divergence regularization term. \n", " Neutral features, maximum entropy, and KL divergence, ", " Neutral feature regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " Neutral features, maximum entropy, and KL divergence. \n", " Neutral features, maximum entropy, and KL divergence. \n", " Neutral feature regularization, maximum entropy of class distribution regularization, and KL divergence between reference and predicted class distribution. \n", " The three regularization terms are: (1) a regularization term associated with neutral features, (2) the maximum entropy of class distribution regularization term, and (3) the KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n\n", " Neutral features, Maximum entropy of class distribution, and KL divergence between reference and predicted class distribution, ", " Neutral features, Maximum entropy, and KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization, Maximum entropy regularization, and KL divergence regularization terms. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, Maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization, maximum entropy regularization, and KL divergence regularization terms. \n", " Neutral feature regularization, maximum entropy regularization, and KL divergence regularization. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " The three regularization terms are the neutral features regularization term, the maximum entropy regularization term, and the KL divergence regularization term, ", " Neutral features regularization term, maximum entropy regularization term, and KL divergence regularization term, ", " Neutral features, maximum entropy principle, and KL divergence between reference and predicted class distribution. \n", " a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n\n", " The three regularization terms are a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. ", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n\n", " The three regularization terms are a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " Neutral features regularization, Maximum entropy regularization, and KL divergence regularization, ", " Neutral feature regularization term, maximum entropy of class distribution regularization term, and KL divergence between reference and predicted class distribution. \n", " Neutral features regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " Neutral features regularization term, Maximum entropy regularization term, and KL divergence regularization term, ", " neutral features regularization term, maximum entropy of class distribution regularization term, and KL divergence between reference and predicted class distribution regularization term. \n", " The three regularization terms are neutral features regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization, maximum entropy regularization, and KL divergence regularization. \n", " Neutral features, maximum entropy, and KL divergence, ", " Neutral feature regularization term, maximum entropy regularization term, and KL divergence regularization term, \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, Maximum entropy, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy, and KL divergence of class distribution. \n", " A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution. \n", " Neutral features, maximum entropy of class distribution, and KL divergence between reference and predicted class distribution, ", " Neutral features, maximum entropy of class distribution, and the KL divergence between reference and predicted class distribution. \n", " Neutral feature regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " Three regularization terms: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. \n", " neutral features, maximum entropy, and KL divergence, ", " Neutral features, maximum entropy, and KL divergence, ", " Neutral features, maximum entropy, and KL divergence. \n", " Neutral features regularization term, maximum entropy regularization term, and KL divergence regularization term. \n", " Neutral features, maximum entropy, and KL divergence, ", " A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution. \n"], "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": [" It allows for crisper examples of attention head behavior and novel behaviors unraveled. \n\n", " They improve interpretability by learning sparse and piecewise-linear distributions with entmax. \n", " It allows for sparser attention weights, making it easier to identify head specializations and understand which tokens are being focused on. \n", " By allowing for sparse attention weights. \n", " It allows for sparser and more interpretable attention weights. \n", " By allowing different heads to be sparse or dense, they are able to learn various linguistic phenomena and make more complex analyses, such as positional heads, BPE-merging heads, and interrogation heads. \n", " Through increased sparsity, allowing for clearer identification of head specializations. \n", " It allows for crisper examples of attention head behavior. \n", " By allowing the shape and sparsity of attention mappings to be learned automatically. ", " It allows for the analysis of head behavior. \n", " They can identify head specializations, including positional, BPE-merging, and interrogation-detecting heads, with higher confidence. \n", " It allows for crisper examples of attention head behavior and novel behaviors unraveled due to the sparsity and adaptivity of their proposed model. \n", " It learns to concentrate alpha values in two modes: a very sparse one around alpha -> 2, and a dense one between softmax and 1.5-entmax, and has a prominent mode corresponding to fully dense probabilities. \n", " By being able to assign exactly zero probabilities for irrelevant words and having crisper examples of attention head behavior, making it easier to understand what the model is doing. \n", " It learns different sparsity patterns in each head, and the adaptivity strategy relies only on gradient-based optimization. \n", " They allow the attention weights to be more sparse, thereby making it easier to identify the specific tokens or sequences that the head is focusing on. ", " By allowing different heads to learn to be sparse or dense, leading to clearer examples of attention head behavior. \n", " By allowing sparse heads and dynamically adapting the shape of attention. \n", " By learning different sparsity patterns in each head. \n", " By enabling sparser attention weights and more variance in individual head behavior. \n", " It allows for crisper examples of attention head behavior and identifies novel behaviors, and also leads to more head diversity and better BPE-merging capabilities. \n", " By introducing sparse attention mechanisms, they can learn to assign zero probabilities to irrelevant words. \n", " It can identify crisper examples of attention head behavior and novel behaviors. \n", " It learns more specialized and confident attention heads. ", " By learning to be sparse and varying the alpha values, their model enables heads to specialize in different linguistic phenomena and become more interpretable, allowing for clearer attention weights and better understanding of the model's behavior. \n", " By learning to focus on sparse patterns and combining sparse and dense attention. \n", " It learns to concentrate the α values in two modes: a very sparse one and a dense one. ", " It allows for crisper examples of attention head behavior, identifies novel behaviors, and enables analyzing high-level statistics and individual head behavior. \n", " It learns to specialize more and with higher confidence, ", " It allows for different heads to learn different sparsity behaviors. \n", " through sparsity, \n", " It allows for sparse attention and easier identification of head specializations. \n", " Through more specialized heads and the ability to visually identify these specializations. \n", " It makes sparser and crisper examples of attention head behavior observable. \n", " It can identify head specializations such as positional, BPE-merging, and interrogation heads. \n", " By introducing sparse attention weights, the model is able to identify head specializations more clearly. ", " It can identify head specializations more and with higher confidence. \n", " Through sparsity and adaptivity, their model improves interpretability compared to softmax transformers. \n", " Through analyzing the sparsity patterns of attention weights. \n", " By allowing different heads to learn different sparsity behaviors and adapt their shape continuously and dynamically between the dense limit case of softmax and the sparse, piecewise-linear sparsemax case. \n\n", " Through their adaptively sparse Transformers, they are able to identify both crisper examples of attention head behavior and novel behaviors unraveled thanks to the sparsity and adaptivity of their model, leading to higher head diversity and better understanding of what specific roles attention heads are taking in the model. \n", " The model improves interpretability by allowing attention heads to learn to focus on relevant words that are not necessarily contiguous. \n", " It allows to identify head specializations. \n", " By assigning exactly zero probabilities for irrelevant words. ", " By allowing for sparse and different sparsity patterns in attention heads. \n", " Their model improves interpretability by learning sparse attention heads. \n", " They enable crisper examples of attention head behavior and novel behaviors unraveled thanks to sparsity and adaptivity. \n", " Their model improves interpretability through the ability to assign exactly zero probabilities to irrelevant words. \n", " It allows heads to be more confident in their representations, leading to better interpretability. ", " \n", " It improves interpretability through automatic learning of sparse heads and head specializations. \n", " Through sparsity and adaptivity, it eases interpretability by allowing different attention heads to specialize in various linguistic phenomena and assign probabilities to a sparse set of words, thereby making their behavior more transparent. \n", " By using sparse attention with adaptable alpha values. ", " It learns to assign exactly zero probabilities for irrelevant words. \n", " \n", " By enabling different attention heads to learn different sparsity behaviors. \n", " It learns to assign zero probabilities for irrelevant words, making attention weights crisper and easier to analyze. \n", " By providing sparser and more interpretable attention weights. \n", " It learns to be sparse or dense adaptively. ", " It allows for crisper examples of attention head behavior and unravels novel behaviors. \n", " It learns to be more sparsely distributed. \n", " \n", " It shows more head diversity and crisper examples of attention head behavior. \n", " It allows for the identification of specialized heads, enables the detection of interrogative words, and reveals sparse attentions to BPE-merged tokens. \n", " It learns different sparsity patterns and specializes certain heads, and allows for the identification of head specializations. \n", " Their model improves interpretability by allowing for sparse attention weights and adaptively varying the sparsity, which enables heads to specialize in different roles. \n", " By learning sparse and different attention distributions, their model provides better interpretability. \n", " Through their adaptively sparse Transformer model with learnable α, it allows for different attention heads to have different sparsity levels, leading to more diverse and specialized attention behavior. \n\n", " It learns different sparsity patterns at each layer, leading to more variance in individual head behavior. \n", " By learning sparse attention, \n", " It allows for crisper examples of attention head behavior. \n", " By allowing the different heads to capture different linguistic phenomena, some of them isolating important words, others spreading out attention across phrases, which is facilitated by the sparsity of the entmax distribution. \n", " It learns to specialize more and with higher confidence. \n", " It eases interpretability and leads to slight accuracy gains, by allowing sparsity in attention weights. \n", " By learning different sparsity patterns, their model can specialize more and with higher confidence. ", " It improves interpretability by allowing heads to learn to be sparser or denser. \n", " It leads to crisper examples of attention head behavior and unravels novel behaviors. ", " By allowing different heads to have different sparsity levels, thereby facilitating the identification of head specializations. \n", " It allows the attention weights to be exactly zero, resulting in crisper examples of attention head behavior, and learns different sparsity behaviors for each head. \n", " By learning different sparsity patterns, \n", " It learns different sparsity patterns at each layer, leading to more variance in individual head behavior. ", " It can learn different sparsity patterns and assign a whole probability distribution to a single token. \n", " \n", " Their model improves interpretability by enabling sparse attention weights that can exactly zero out irrelevant words. \n\n", " It allows the heads to specialize more and with higher confidence. \n", " It learns sparse attention weights and allows for more confidence in representations. \n", " They provide crisper examples of attention head behavior and unravel novel behaviors. ", " Their model allows for sparser and more focused attention weights, enabling better identification of head specializations. \n", " Their model improves interpretability by allowing different attention heads to specialize and learn different roles, making the attention weights sparser. \n\n", " They enable to analyze head behavior and specialization with higher confidence. \n", " It allows for sparse attention and learns different sparsity patterns in the same span, enabling better analysis of attention heads. \n", " It allows for better identification of head specializations and crisper examples of attention head behavior. \n", " By enabling more sparse attention weights and head specialization, allowing for crisper examples of attention head behavior and identifying head specializations, such as positional and interrogation heads. \n", " It shows potential in both translation accuracy as well as model interpretability. ", " It provides crisper examples of attention head behavior and novel behaviors unraveled thanks to the sparsity and adaptivity of their proposed model. \n", " It learns to attend to a sparse set of words that are not necessarily contiguous. ", " It allows for clearer identification of head specializations. \n", " By automatically adjusting the shape and sparsity of the attention mechanism, making it possible to identify specific roles played by different heads in the model. \n", " The model can specialize more and with higher confidence, as well as identify head specializations, such as positional, BPE-merging, and interrogation heads. \n", " It allows heads to specialize more and with higher confidence, and to learn sparse and dense attention patterns. \n"], "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": [" HealthCare, ", " unanswerable, ", " Energy, ", " unanswerable, ", " Energy sector, ", "  The HealthCare sector did not, however, the Energy sector did not either, actually the Energy sector accuracy is the best in the sectors we analyzed, which is 0.44. \n", " No answerable. \n", " Energy sector, ", " Energy, ", " HealthCare, ", " Energy, ", " Energy, ", " Energy sector, ", " Energy, ", " unanswerable, ", " HealthCare, ", " unanswerable, ", " The Consumer Discretionary sector. \n", " HealthCare, ", " HealthCare, ", " Energy, ", " Energy, ", " no sector achieved the best performance, ", " HealthCare, ", " \n", " Energy, ", " \n", " unanswerable, ", " Energy, ", " Energy, ", " unanswerable, ", " HealthCare, ", " HealthCare, ", " Energy sector, ", " no sector achieved the best performance, ", " Energy sector, ", " Energy sector, ", " unanswerable, ", " No answer can be concluded for the question, ", " No answer can be determined from the article, ", " The results were not stated clearly, ", " The Energy sector. ", " unanswerable, ", " Energy sector, ", " The best performance is achieved by the Energy sector. \n", " unanswerable, ", " unanswerable \n", " Energy sector, \n", " HealthCare, ", " HealthCare, ", " Energy sector. \n", " Energy, ", " Energy sector, ", " Unanswerable, ", " unanswerable, \n", "  Not specified, ", " The Energy sector, ", " \n", " HealthCare, ", " Energy sector, \n", " no sector achieved the best performance, ", " unanswerable, ", " The best performance of stock market sector is unanswerable, ", " \n", " Energy sector, ", " unanswerable, ", " unanswerable, ", " Energy, ", " Unanswerable, ", " Energy, ", " The HealthCare sector, \n", " \n", " Energy, ", " Energy, ", " unanswerable, ", " unanswerable, ", " Energy, ", " HealthCare sector did not, Energy sector did not, the best sector achieved is the sector of which name is not mentioned in the article. However, according to the performance of the different sectors in the article, we can infer that the sector with the highest accuracy, measured using the $R^2$ score, is actually \"Telecommunication\" sector did not, but Energy sector did better than the HealthCare sector. But the article does not mention the name of the sector with the highest performance. However, we can infer that the sector of which the accuracy is higher than 0.44 is actually \"Telecommunication\"", " HealthCare, ", " Energy sector, ", " Energy sector, ", " HealthCare, ", " HealthCare sector achieved the best performance. \n", " Energy, ", " HealthCare, ", " unanswerable, ", " \n", " HealthCare, ", " unanswerable \n", " Energy sector, ", " Energy, ", " Energy sector, ", " Energy sector, ", " HealthCare, ", " Energy, ", " Energy, ", " Energy, ", " HealthCare, ", " no sector, ", " HealthCare, "], "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": [" unanswerable, \n", "  competitive results on other benchmarks, ", " batch normalization with ReLU, layer normalization with GLU, and layer normalization with ReLU. ", " unanswerable, ", " batch norm with ReLU and layer norm with GLU performed the best on the smaller model, while batch norm with ReLU outperformed other choices on larger models. \n", " [Your concise final answer here], ", " unanswerable \n", " unanswerable \n", " Jasper 5x3 model with layer norm and GAU, Jasper10x4 with layer norm and ReLU, Jasper10x4 with batch norm and ReLU. ", "  competitive results on other benchmarks were achieved by the Jasper architecture on LibriSpeech, and it serves as a good baseline approach. \n", " unanswerable, ", " batch norm with ReLU and layer norm with GAU, ", " batch norm with ReLU and GLU performed the second best on the smaller model. ", " Batch normalization and ReLU \n", " batch norm with ReLU and layer norm with ReLU. ", " unanswerable, ", " The results were compared to other models trained using the same data. ", " batch norm with ReLU and layer norm with GAU \n", " unanswerable \n", " batch norm with GLU, weight norm with GLU, layer norm with GLU. ", " The WER on dev-clean LibriSpeech before using NovoGrad. \n", " unanswerable \n", " unanswerable \n", " batch normalization and ReLU for our architecture. \n", " Jasper DR 10x5 (with a WER of 2.95%) with a Transformer-XL language model. \n", " Jasper's architecture is highly efficient for training and inference, and serves as a good baseline approach on top of which to explore more sophisticated regularization, data augmentation, loss functions, language models, and optimization strategies. \n", " batch norm with ReLU and GLU. \n", " Jasper DR 5x3, ", " batch normalization with ReLU, clipped ReLU (cReLU), and leaky ReLU (lReLU) for activation units, and batch norm, weight norm, and layer norm for normalization. ", " Jasper with batch normalization and ReLU. \n", " The baselines were wav2letter and non-end-to-end models. ", " batch norm with ReLU. \n", "  unanswerable, ", " batch norm with ReLU outperformed other normalization and activation schemes. \n", " Jasper 5x3 model and SGD with momentum optimizer for 400 epochs on a combined WSJ dataset. ", " [Your concise final answer here], ", " batch norm with ReLU and layer norm with ReLU, \n", " no baselines were mentioned. \n", " Jasper DR 5x3. ", " unanswerable. \n", " unanswerable, \n", " Jasper 10x3, ", " Our Jasper architecture serves as a good baseline approach on top of which to explore more sophisticated regularization, data augmentation, loss functions, language models, and optimization strategies. ", " unanswerable ", " batch norm with ReLU and layer norm with GAU and batch norm with ReLU \n", " no specific baseline was mentioned in the article, only that Jasper is a good baseline approach. \n", " batch normalization and ReLU. \n", " Standard components. \n", " batch normalization with ReLU, layer norm with GAU, and batch norm with ReLU. ", " Jasper uses a stack of 1D-convolution layers, with ReLU and batch normalization. \n", " Jasper DR 10x5, ", " unanswerable \n", " batch normalization with ReLU, layer norm with GAU, layer norm with ReLU. \n", " unanswerable \n", " Jasper 5x3 model, ", " batch norm with ReLU, and layer norm with GLU. ", " unanswerable. ", " Jasper is the architecture under investigation. ", " batch normalization and ReLU, ", " unanswerable \n", " \n", " batch norm with ReLU and batch norm with ReLU outperformed other normalization and activation schemes, layer norm with GAU performed the best on the smaller model, but batch norm with ReLU came second and third in our tests, but for larger models, we noticed that batch norm with ReLU outperformed other choices. Thus, leading us to decide on batch normalization and ReLU for our architecture. \n", " unanswerable, ", " Jasper architecture with ReLU and batch normalization outperformed other activation and normalization schemes that we tested. \n", " batch norm with ReLU, and layer norm with GAU.\n", " standard components \n", " layer norm with GAU, layer norm with ReLU, and batch norm with ReLU. \n", " batch norm with ReLU and layer norm with GLU. \n", " Jasper DR 10x5 and Jasper 10x3. \n", " the original wav2letter model \n", " the original wav2letter, the combination of hidden Markov models to model state dependencies with neural networks to predict states, and non end-to-end models. \n", " unanswerable, ", " batch normalization with ReLU outperformed other activation and normalization schemes. \n", " unanswerable \n", " no explicit baselines were given. \n", " batch norm and ReLU. ", " batch norm with ReLU outperformed other activation and normalization schemes that we tested. ", " Jasper's architecture. ", " Batch norm with GLU and weight normalization; Layer norm with GAU and no GLU. ", " [Your concise final answer here], ", " layer norm with GAU performed the best on the smaller model; layer norm with ReLU and batch norm with ReLU came second and third in our tests. \n", " Jasper's architecture contains only 1D convolution, batch normalization, ReLU, and dropout layers. ", " Jasper architecture. \n", " a combination of standard components leads to SOTA results on LibriSpeech and competitive results on other benchmarks. \n", " None, unanswerable \n", " batch normalization and ReLU. \n", " ReLU and batch normalization outperformed other activation and normalization schemes that were tested for convolutional ASR. ", " unanswerable, ", " Batch norm with ReLU and weight norm. ", " unanswerable \n", " batch normalization and ReLU. \n", " batch norm with ReLU outperformed other activation and normalization schemes that were tested. ", " Jasper models, \n", " Batch norm with ReLU, layer norm with ReLU and layer norm with GAU. ", " [Your concise final answer here], ", " Jasper 5x3 and a pre-trained Transformer-XL, ", " batch normalization, ReLU, and dropout layers, ", " batch norm with ReLU and layer norm with GAU, ", " Weight norm, layer norm, ReLU, clipped ReLU, and leaky ReLU, concatenation in DenseNet and DenseRNet, and no normalization, no activation, no residual, and concatenation. \n", " Jasper's architecture is highly efficient for training and inference, and serves as a good baseline approach on top of which to explore more sophisticated regularization, data augmentation, loss functions, language models, and optimization strategies. \n"], "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": [" XNLI test accuracy and Labeled Attachment Scores (LAS) for dependency parsing. ", " Labeled Attachment Scores (LAS), ", " Labeled Attachment Score (LAS). \n", " Labeled Attachment Score (LAS) and XNLI test accuracy, ", " Zero-shot cross-lingual transfer tasks: XNLI and UD parsing. and Labeled Attachment Scores (LAS)., ", " Labeled Attachment Scores (LAS), Accuracy; ", " Labeled Attachment Scores (LAS), \n", " Accuracy and LAS, \n", " LAS and accuracy. \n", " LAS and accuracy, ", " XNLI test accuracy and LAS for zero-shot dependency parsing. \n", " XNLI test accuracy, Labeled Attachment Scores (LAS) for zero-shot dependency parsing. ", " Zero-shot cross-lingual transfer tasks: XNLI and Universal Dependency Parsing. ", " XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing. \n", " XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing. \n", " LAS (Labeled Attachment Score) and accuracy, ", " LAS, XNLI test accuracy, \n", " Labeled Attachment Scores (LAS) for dependency parsing, XNLI test accuracy, LAS for zero-shot dependency parsing. \n", " XNLI test accuracy and Labeled Attachment Scores (LAS). \n", " Zero-shot cross-lingual transfer tasks: XNLI and Universal Dependency Parsing, and Labeled Attachment Scores (LAS) for dependency parsing. \n", " LAS for zero-shot dependency parsing and accuracy for XNLI. ", " LAS (Labeled Attachment Scores) and accuracy. \n", " LAS (Labeled Attachment Score) and XNLI (Cross-lingual Natural Language Inference) test accuracy. \n", " XNLI test accuracy and LAS for zero-shot dependency parsing, ", " Labeled Attachment Scores (LAS), zero-shot accuracy and LAS scores. \n", " XNLI test accuracy and Labeled Attachment Scores (LAS). ", " XNLI and UD parsing LAS scores, ", " Zero-shot cross-lingual transfer tasks: XNLI and dependency parsing, Labeled Attachment Scores (LAS). \n", " Labeled Attachment Scores (LAS) and accuracy. \n", " Zero-shot cross-lingual transfer tasks, LAS. ", " Labeled Attachment Scores (LAS) and accuracy. ", " Labeled Attachment Scores (LAS), ", " Labeled Attachment Scores (LAS) and zero-shot transfer accuracy. ", " Zero-shot cross-lingual transfer tasks. \n", " Labeled Attachment Scores (LAS) and accuracy. \n", " Zero-shot cross-lingual transfer tasks. ", " XNLI test accuracy and Labeled Attachment Scores (LAS) for dependency parsing. ", " XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing, ", " XNLI test accuracy and Labeled Attachment Scores (LAS). \n", " XNLI test accuracy and LAS for zero-shot dependency parsing. ", " XNLI test accuracy and Labeled Attachment Scores (LAS). ", " LAS, LAS scores, XNLI test accuracy. \n", " LAS, Accuracy, and Labeled Attachment Score, ", " Zero-shot cross-lingual transfer tasks, including XNLI and UD parsing. \n", " Accuracy, LAS, LAS scores. ", " XNLI test accuracy and Labeled Attachment Scores (LAS), ", " XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing. ", " Labeled Attachment Scores (LAS) and accuracy, ", " LAS (Labeled Attachment Score), Accuracy, ", " Labeled Attachment Scores (LAS) and zero-shot accuracy. ", " LAS (Labeled Attachment Scores) and XNLI (Cross-lingual Natural Language Inference) test accuracy. \n", " Labeled Attachment Scores (LAS), Zero-shot accuracy \n", " Zero-shot cross-lingual transfer tasks, Labeled Attachment Scores (LAS), \n", " Zero-shot cross-lingual transfer tasks: XNLI and universal dependency parsing, Labeled Attachment Scores (LAS). \n", " XNLI test accuracy and Labeled Attachment Scores (LAS). \n", " LAS and accuracy, ", " XNLI test accuracy, Labeled Attachment Scores (LAS), LAS for dependency parsing, LAS for supervised parsing. ", " XNLI test accuracy and Labeled Attachment Scores (LAS). \n", " LAS and accuracy, \n", " Labeled Attachment Scores (LAS) and accuracy. \n", " XNLI test accuracy and Labeled Attachment Scores (LAS), ", " LAS and accuracy. \n", " XNLI test accuracy, Labeled Attachment Scores (LAS), LAS for dependency parsing, performance in supervised dependency parsing. \n", " LAS, Accuracy, XNLI test accuracy, LAS scores, Average LAS score, Labeled Attachment Scores, and Accuracy, ", " XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing. ", " Accuracy and LAS (Labeled Attachment Score), ", " LAS scores, ", " LAS, accuracy, LAS score, accuracy of XLNI, unanswerable. \n", " XNLI test accuracy, Labeled Attachment Scores (LAS), zero-shot performance. \n", " Accuracy and LAS, ", " XNLI test accuracy and Labeled Attachment Scores (LAS)., ", " LAS (Labeled Attachment Scores) and accuracy. \n", " XNLI test accuracy and LAS scores for dependency parsing. ", " Labeled Attachment Scores (LAS) for zero-shot dependency parsing and XNLI test accuracy for cross-lingual natural language inference. \n", " Labeled Attachment Scores (LAS) for dependency parsing, zero-shot XNLI accuracy for cross-lingual natural language inference. \n", " Labeled Attachment Score (LAS), \n", " XNLI test accuracy and LAS. ", " Zero-shot cross-lingual transfer tasks, namely natural language inference and universal dependency parsing. ", " XNLI and UD, ", " XNLI test accuracy, Labeled Attachment Scores (LAS) for zero-shot dependency parsing. \n", " LAS (Labeled Attachment Scores), Accuracy \n", " LAS, Accuracy, Labeled Attachment Scores, Accuracy, unanswerable, Accuracy. ", " Labeled Attachment Scores (LAS) and test accuracy, ", " XNLI test accuracy and Labeled Attachment Scores (LAS) for zero-shot dependency parsing. \n", " Labeled Attachment Scores (LAS), ", " LAS, accuracy. \n", " Labeled Attachment Scores (LAS), accuracy, \n\n", " XNLI test accuracy and Labeled Attachment Scores (LAS), ", " XNLI test accuracy, LAS scores, and LAS results. \n", " LAS, accuracy, zero-shot performance, XNLI test accuracy. \n", " LAS (Labeled Attachment Score) for zero-shot dependency parsing and XNLI test accuracy., ", " Accuracy and LAS, ", " XNLI test accuracy and Labeled Attachment Scores (LAS), ", " XNLI test accuracy and Labeled Attachment Scores (LAS), ", " Labeled Attachment Scores (LAS) and accuracy. \n", " XNLI test accuracy and LAS scores for zero-shot dependency parsing, ", " Zero-shot cross-lingual transfer tasks of XNLI and Universal Dependency Parsing. \n", " Zero-shot cross lingual transfer tasks: XNLI and Universal Dependency Parsing. \n", " LAS scores, XNLI test accuracy. \n", " LAS, XNLI test accuracy, and other metrics (not specified), "], "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": [" Textual features and gaze features. ", " Unigrams and Pragmatic features. ", " Incongruity-related stylistic features. \n", " Stylistic patterns, emoticons, laughter expressions such as “lol” etc., and Hastag interpretations. \n", " Stylistic patterns. \n", " stylistic patterns, ", " Pragmatic features and emoticons; \n", " Stylistic patterns. ", " Pragmatic features, emoticons, and laughter expressions. ", " Emoticons, laughter expressions, etc. ", " Hastag interpretations, emoticons, and laughter expressions. \n", " Pragmatic features, and stylistic patterns. ", " Emoticons, laughter expressions, and hashtags. \n", " Pragmatic features. ", " Explicit incongruity and implicit incongruity, ", " Stylistic patterns, and patterns related to situational disparity, along with Hastag interpretations. ", " Patterns related to situational disparity. \n", " Pragmatic features and patterns related to situational disparity. \n", " Stylistic patterns. ", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. \n", "  Patterns related to situational disparity and stylistic patterns. \n", " Unigrams and Pragmatic features. ", " Stylistic patterns. ", " Unigrams and Pragmatic features. ", " None specified, ", " Pragmatic and stylistic patterns, ", " unigrams, pragmatics, stylistic patterns, and patterns related to situational disparity.  (Hastag interpretations also), \n", " Pragmatic features, stylistic patterns, and hashtag interpretations, ", " Pragmatic, stylistic patterns, and patterns related to situational disparity, as well as Hastag interpretations. ", " Pragmatic features, emoticons, laughter expressions. ", " Explicit incongruity, ", " None other than textual features, ", " Pragmatic features, emoticons, laughter expressions, and hashtag interpretations. \n", " Stylistic patterns and patterns related to situational disparity. ", " Saliency graphs, which is a graph structure derived from eye-gaze information and word sequences in the text. \n", " Pragmatic features, BIBREF0, BIBREF1, BIBREF2, BIBREF3, Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5 and Hastag interpretations BIBREF6, BIBREF7. \n", " Pragmatic features, stylistic patterns, hashtag interpretations. ", " Stylistic patterns. ", " Emoticons, laughter expressions such as “lol” etc., and Hastag interpretations. \n", " stylistic patterns related to situational disparity, emoticons, laughter expressions such as “lol” etc. and Hastag interpretations. \n", " Pragmatic and stylistic patterns related to situational disparity. \n", " Pragmatic features, stylistic patterns, and hashtag interpretations. \n", " Pragmatic features. ", " Unigrams and Pragmatic features, ", " Unigrams and Pragmatic features, Stylistic patterns, patterns related to situational disparity, Hastag interpretations. \n", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. ", " Pragmatic features, stylistic patterns and patterns related to situational disparity. \n", " Pragmatic, stylistic patterns, hastag interpretations, emoticons, laughter expressions. \n", " Textual and explicit incongruity. \n", " Pragmatic features, emoticons, laughter expressions and stylistic patterns. \n", " Pragmatic features. \n", " Emoticons and laughter expressions. ", " Patterns related to situational disparity. \n", " Patterns related to situational disparity. \n", " Stylistic patterns. ", " Stylistic patterns. \n", " Unigrams and Pragmatic features, Stylistic patterns, Hastag interpretations. \n", " Emoticons, laughter expressions. ", " Pragmatic features and patterns related to situational disparity. \n", " Emoticons, laughter expressions such as “lol” etc. \n", " Stylistic patterns. ", " Pragmatic features. ", " Stylistic patterns, pragamatic features, emoticons, laughter expressions, and hashtag interpretations. \n", " Unigrams, Pragmatic features. ", " Emoticons and laughter expressions such as “lol”. ", " Emoticons and laughter expressions. \n", " Pragmatic and Stylistic patterns. \n", " Pragmatic features, stylistic patterns, and hashtag interpretations. ", " Stylistic patterns related to situational disparity. \n", " Stylistic patterns. \n", " Stylistic patterns and patterns related to situational disparity. \n", " 3. Both \n", " Pragmatic and stylistic patterns. ", " Pragmatic features, stylistic patterns, and patterns related to situational disparity, Hastag interpretations, emoticons, and laughter expressions such as “lol” etc. ", " Stylistic patterns, and patterns related to situational disparity. ", " Pragmatic features. ", " Incongruity-based stylistic features are obtained. \n", " Pragmatic, Stylistic patterns, Hastag interpretations, Emoticons, laughter expressions, linguistic and sarcasm related features. \n", " Stylistic patterns. \n", " Pragmatic features. ", " Stylistic patterns. \n", " Implicit and explicit incongruity features. \n", " Implicit incongruity features, explicit incongruity features. ", " Pragmatic features and stylistic patterns, and patterns related to situational disparity and hashtag interpretations. \n", " Explicit incongruity features, ", " Textual features used in best available sarcasm detectors. \n", " stylistic/pragmatic features (emoticons, laughter expressions), ", " Complex gaze-based features are derived from a graph structure, namely “saliency graphs”, representing the reader's attention characteristics. \n\n", " Incongruity, implicit incongruity and explicit incongruity features are obtained along with readability and word count features. \n", " Unigrams and Pragmatic features. \n", " Pragmatic features. ", " Pragmatic features, emoticons, and stylistic patterns. ", " Pragmatic features, emoticons, laughter expressions, stylistic patterns, and hashtag interpretations. \n", " Hastag interpretations, emoticons, and laughter expressions. \n", " Stylistic patterns. ", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity, and Hastag interpretations. \n", " Stylistic patterns. \n", " Stylistic patterns,  emoticons, laughter expressions and hashtag interpretations. ", " Emoticons, laughter expressions, and hashtags. ", " Stylistic patterns. "], "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": [" Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and step entailment score. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment score, and human evaluation. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, Mean Reciprocal Rank (MRR) and user matching accuracy (UMA),. ", " Perplexity, BLEU-1/4, BLEU, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, user matching accuracy, and Mean Reciprocal Rank. \n", " Perplexity, user-ranking, perplexity, user-ranking, BLEU, ROUGE-L, BLEU-1, BLEU-4, Distinct-1/2, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, step entailment score. \n", " Perplexity, BLEU-1/4, ROUGE-L, user-ranking, distinct-1/2, recipe-level coherence, and entailment score. \n", " perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment score, and human evaluation. \n", " Perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, BLEU-1, BLEU-4, user-ranking, perplexity, user-ranking, BLEU-1/4, ROUGE-L, Mean Reciprocal Rank (MRR), user-ranking, BLEU-1/4, ROUGE-L, and recipe-level coherence and step entailment., ", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence, recipe step entailment, user matching accuracy, and Mean Reciprocal Rank. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and mean reciprocal rank. \n\n", " BPE perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence score, step entailment score, and human evaluation. \n", " BLEU, ROUGE, perplexity, user-ranking, Distinct-1/2, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, recipe step entailment. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, and recipe-level coherence score, entailment score and user matching accuracy (UMA) and Mean Reciprocal Rank (MRR). \n\n", " perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, BLEU-1, BLEU-4, user-ranking, Distinct-1/2, perplexity, recipe-level coherence score, step entailment score, and Mean Reciprocal Rank.,", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, Mean Reciprocal Rank (MRR), User Matching Accuracy (UMA), recipe-level coherence score, step entailment score. \n\n", " perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, User Matching Accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment score. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, user-ranking, BPE perplexity, BLEU-1, BLEU-4, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and recipe step entailment score. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, human evaluation. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment score, and Mean Reciprocal Rank. \n", " Perplexity, user-ranking, perplexity, user-ranking, BLEU-1/4, ROUGE-L, BLEU-1, BLEU-4, Distinct-1/2, recipe-level coherence, recipe step entailment, user matching accuracy (UMA), Mean Reciprocal Rank (MRR). \n", " BLEU, ROUGE, perplexity, user-ranking, Distinct-1/2, recipe-level coherence, and step entailment. \n", " perplexity, user-ranking, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment. \n", " perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, user-ranking, perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment score.", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, UMA, MRR, recipe-level coherence, and step entailment score. \n", " perplexity, user-ranking, perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence, step entailment score. \n", " Perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, and ROUGE. \n\n", " BLEU-1, BLEU-4, Distinct-1/2, perplexity, User-ranking, recipe-level coherence, and Mean Reciprocal Rank (MRR). \n", " Perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, BLEU-1, BLEU-4, perplexity, and coherence metrics. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, ROUGE, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment score. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-4, recipe-level coherence, and step entailment score. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence scores, recipe step entailment scores, and Mean Reciprocal Rank (MRR), \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-4, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), and recipe-level coherence. \n", " BPE perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, user-ranking, BLEU-1, BLEU-4, perplexity, user-ranking, recipe-level coherence, step entailment score, perplexity, user-ranking, perplexity. \n", " BPE perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, and a neural scoring model for recipe-level coherence and entailment score for local coherence. \n\n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment score. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, recipe-level coherence, and step entailment, as well as a human evaluation,, ", " perplexity, user-ranking, perplexity, user-ranking, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity, perplexity", " Perplexity, user-ranking, perplexity, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment score, human evaluation, and Mean Reciprocal Rank (MRR). \n\n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, and Mean Reciprocal Rank (MRR)., ", " Perplexity, user-ranking, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), and recipe-level coherence. \n\n", " Perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence scores, step entailment scores, and human evaluation. \n", " Perplexity, BLEU, ROUGE, Distinct-1/2, and Recipe-level coherence. \n", " BPE perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence, and step entailment. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, user-ranking, perplexity. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, user-ranking, recipe-level coherence, recipe step entailment, and human evaluation. \n\n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence, entailment and human evaluation. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and recipe step entailment, ", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence score, step entailment score, and Mean Reciprocal Rank (MRR), as well as human evaluation. \n\n", " Perplexity, BLEU, ROUGE, Distinct, Recipe-level coherence, Step entailment. \n\n", " BPE perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment score. \n", " perplexity, user-ranking, perplexity, user-ranking, perplexity, BLEU, ROUGE, BLEU-1, BLEU-4, Distinct-1/2, perplexity, user-ranking, perplexity, user-ranking, perplexity, BLEU-1, BLEU-4, Distinct-1/2, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, Mean Reciprocal Rank (MRR), User Matching Accuracy (UMA), recipe-level coherence, step entailment, perplexity, BLEU, ROUGE, BLEU-1", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and step entailment score. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, Recipe-level coherence, Recipe step entailment, Mean Reciprocal Rank (MRR), User matching accuracy (UMA), Human evaluation, and BPE perplexity. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), recipe-level coherence, and step entailment score. \n", " Perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, perplexity, user-ranking, BLEU-1, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-1/4,", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, human evaluation. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), BERT, GRU network., ", " BLEU-1/4, ROUGE-L, perplexity, Distinct-1/2, recipe-level coherence, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), BLEU and ROUGE are not the most appropriate metrics for recipe quality. \n", " BPE perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), and human evaluation. \n\n", " Perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence, recipe step entailment, and human evaluation. \n", " Perplexity, user-ranking, perplexity, BLEU-1/4, BLEU, ROUGE-L, Distinct-1/2, perplexity, user-ranking, perplexity, BLEU-1/4, BLEU, ROUGE-L, Distinct-1/2, perplexity, user-ranking, recipe-level coherence, and entailment score. \n\n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, BLEU-1/4, ROUGE-L, recipe-level coherence score, step entailment score, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), perplexity, BPE perplexity., ", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, recipe-level coherence, and step entailment score. \n", " Perplexity, user-ranking, perplexity, BPE, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, ROUGE, perplexity, recipe-level coherence, step entailment, BPE, perplexity, user-ranking. \n\n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, human evaluation. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, Mean Reciprocal Rank (MRR), and user matching accuracy (UMA). \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, BLEU, Distinct-1/2, recipe-level coherence, and Mean Reciprocal Rank (MRR), Recipe Step Entailment. \n", " Perplexity, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, user-ranking, recipe-level coherence and entailment. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, recipe step entailment, human evaluation. \n", " perplexity, BLEU-1/4, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, and user-ranking, as well as a recipe-level coherence metric and a step entailment metric, in addition to human evaluation. \n\n", " Perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, ROUGE, User Matching Accuracy (UMA), Mean Reciprocal Rank (MRR), and a novel recipe-level coherence metric. \n", " Perplexity, user-ranking, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence scores, and step entailment score. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, human evaluation, Mean Reciprocal Rank (MRR), user matching accuracy (UMA). \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, perplexity, BERT, user-ranking, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, recipe-level coherence, entailment score, perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence., ", " BLEU-1/4, ROUGE-L, perplexity, Distinct-1/2, user-ranking, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), recipe-level coherence, step entailment. \n", " Perplexity, BLEU-1/4, ROUGE-L, BLEU, and ROUGE are used for evaluation. Additionally, personalized models use user-ranking and recipe-level coherence metrics. Also, a new metric, user matching accuracy (UMA) and mean reciprocal rank (MRR), is proposed for personalization evaluation. \n\n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence score, step entailment score, human evaluation. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-4, recipe-level coherence, step entailment, human evaluation. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, ROUGE, recipe-level coherence, and step entailment score. \n", " BLEU-1, ROUGE-L, perplexity, user-ranking, Mean Reciprocal Rank (MRR), user matching accuracy (UMA), recipe-level coherence, and step entailment score. \n", " BLEU-1/4, ROUGE-L, Distinct-1/2, and BPE perplexity. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and Mean Reciprocal Rank. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, BLEU-4, Distinct-1/2, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), and recipe-level coherence scores. ", " Perplexity, user-ranking, BLEU-1, BLEU-4, ROUGE-L, Distinct-1/2, and a new metric called Recipe-Level Coherence and Recipe Step Entailment, and also human evaluation. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and user-ranking. \n", " perplexity, BLEU, ROUGE, Distinct, Recipe-level coherence and Recipe Step Entailment. \n", " Perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and step entailment score, as well as human evaluation. \n", " BPE perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, and human evaluation. \n", " Perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and user-ranking are used. Recipe-level coherence and recipe step entailment scores are also used. Additionally, mean reciprocal rank (MRR) and user matching accuracy (UMA) are used. Human evaluation is also performed to compare generated recipes with baseline models. \n\n", " BLEU-1/4, ROUGE-L, Distinct-1/2, and perplexity are used for evaluation, in addition to subjective human evaluation. \n\n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and user-ranking, recipe-level coherence, and step entailment score. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU, ROUGE, User Matching Accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, step entailment, and human evaluation. \n", " Perplexity, BLEU, ROUGE, Distinct, Mean Reciprocal Rank (MRR) and User Matching Accuracy (UMA), and a recipe-level coherence metric. \n\n", " perplexity, BLEU-1/4, BLEU, ROUGE-L, BLEU-1, BLEU-4, user-ranking, and distinct-1/2 are used for evaluation. \n", " BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence, and step entailment. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, step entailment, human evaluation. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence, and Mean Reciprocal Rank (MRR). \n", " Perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and step entailment. \n", " perplexity, user-ranking, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence, and step entailment. \n"], "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": [" LSTM, ", " Encoder-decoder architecture with character-based LSTM. \n", " LSTM, ", " An encoder-decoder architecture with LSTM encoder and decoder. \n", " LSTM \n", " A forward LSTM to encode the past context and a backwards LSTM to represent the future context. \n", " LSTM. ", " ", " LSTM, ", " A forward LSTM followed by a backwards LSTM. ", " ", " An LSTM, ", " LSTM. ", " ", " It has an LSTM layer. ", " ", " LSTM. \n", " ", " LSTM encoder, ", " LSTM \n", " A forward LSTM and a backwards LSTM. \n", " A forward LSTM and a backwards LSTM, ", " ", " A forward LSTM to encode the entire past context and a backwards LSTM to represent the future context, ", " An LSTM architecture. \n", " ", " Encoder with a forward LSTM and a backward LSTM. \n", " ", " LSTM, ", " LSTM and a forward LSTM. ", " An LSTM encoder, ", " An LSTM encoder. ", " LSTM, ", " An encoder-decoder architecture on character sequences. \n", " ", " LSTM encoder. ", " ", " An LSTM. ", " A forward LSTM to encode the entire past context and a backward LSTM to represent the future context. ", " An LSTM encoder, ", " LSTM, ", " A forward LSTM. ", " A forward LSTM and a backward LSTM. \n", " LSTM, ", " LSTM, ", " ", " ", " A forward LSTM, ", " ", " A forward LSTM, ", " ", " LSTM, ", " LSTM architecture, ", " An LSTM. ", " ", " LSTM. ", " A forward LSTM to encode the past context and a backwards LSTM to encode the future context, ", " A forward LSTM to encode the past context and a backwards LSTM to encode the future context. ", " LSTM \n", " A forward LSTM to encode the past context and a backwards LSTM to represent the future context, ", " LSTM architecture. \n", " LSTM architecture, ", " ", " LSTM encoder. \n", " An LSTM architecture. \n", " LSTM with a fixed-size vector, ", " LSTM, ", " LSTM. \n", " A forward LSTM followed by a backwards LSTM. ", " LSTM encoder with attention mechanism, ", " LSTM, ", " ", " An LSTM encoder. \n", " LSTM with one layer, ", " Encoder with LSTM layers, ", " A forward LSTM, ", " ", " A forward LSTM to encode the entire past context and a backwards LSTM to encode the future context. ", " ", " An LSTM encoder, ", " ", " An LSTM encoder with a forward LSTM to encode the entire past context and a backwards LSTM to represent the future context. \n", " The encoder is an LSTM. \n", " A LSTM encoder with a forward LSTM to encode the entire past context and a backwards LSTM to encode the future context, ", " Encoder-decoder with LSTM architecture. ", " Forward LSTM and backward LSTM. \n", " LSTM, ", " LSTM, ", " An LSTM. \n", " LSTM, ", " LSTM, ", " LSTM encoder. \n", " ", " LSTM, ", " LSTM, ", " A forward LSTM for past context and a backwards LSTM for future context, ", " A forward LSTM and a backwards LSTM. \n", " LSTM \n", " A forward LSTM for encoding the entire past context and a backwards LSTM for encoding the entire future context. ", " "], "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": [" yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, "], "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": [" 20,000. \n", " 22,880, ", " 22,880 users, ", " 22,880, ", " over 20,000 \n", " 20,000, ", " 20,000, ", " Over 20,000 users. \n", " 22,880 users, ", " 22,880 users. \n", " 22,880 users, ", " 22,880, ", " 20,000, ", " Over 20,000 blog users. \n", " 22,880, ", " Over 20,000 users. \n", " 22,880, \n", " 22,880 users, ", " Over 20,000. \n", " 22,880 users. \n", " 22,880 users. \n", " 22,880 users, \n", " 22,880, ", " 22,880 users, ", " 22,880, ", " 22,880, ", " 22,880 users, \n", " 22,880, ", " 22,880, ", " 22,880 users, ", " 22,880 users. \n", " 22,880, ", " 22,880 users, ", " 20,000, ", " 22,880, ", " 22,880 users. \n", " 22,880 users. \n", " 22,880 users. \n", " Over 20,000 users, ", " 22,880 users. \n", " 22,880 users, ", " 20,000 users \n", " 22,880 users, \n", " Over 20,000, ", " 22,880 users. \n", " 22,880, ", " Over 20,000 users. \n", " 22,880, ", " 22,880 users, \n", " Over 22,000 users, \n", " Over 20,000. \n", " 22,880 users. \n", " 20,000, ", " over 22,000 users, ", " Over 20,000 users. \n", " 22,880 users. \n", " 22,880, ", " over 22,880 users. \n", " 22,880 users, ", " 22,880 users, ", " 22,880 users, ", " Over 20,000. \n", ", ", " 22,880 users. \n", " Over 20,000, ", " 20,880 users. \n", " over 20,000, ", " 22,880, ", " 22,880 users. \n", " 22,880 users. ", " 22,880 users, ", " 22,880 users. \n", " 22,880 users, ", " 22,880 users, ", " 22,880 users, ", " Over 20,000. \n", " 20,000 users. \n", " 22,880, ", " 22,880, ", " 22,880 users, ", " 22,880 users, \n", " Over 20,000 users, ", " 22,880, ", " 22,880, ", " 22,880 users. \n", " 22,880 users. \n", " 22,880, ", " Over 22,000 users. \n", " 22,880 users, ", " 20,880 users, ", " 22,880 users. \n", " 20,000, ", " 20,000, ", " 22,880 users, ", " 20,000. ", " over 20,000, \n", " 22,880, ", " 20,000, ", " 20,000 users, ", " 22,880 users, "], "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": [" MT, ", " MT task, ", " MT, ", " MT task, ", " MT, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " MT, ", " unanswerable, ", " \n", " unanswerable, ", " ST attention module does not benefit from the pre-training. ", " MT task, ", " It is pre-trained on ASR, MT and ST tasks. \n", " The pre-trained attention module, ", " unanswerable, ", " MT, ", " MT, ", " MT, ", " MT, ", " MT, ", " ST, ", " MT, ", " MT \n", " MT, ", " MT, ", " unanswerable, ", " MT, ", " unanswerable, ", " unanswerable, ", " Not directly pretrained, ", " MT, ", " ST, ", " MT, ", " The attention module of ST is pre-trained on ASR, MT and ST tasks. \n", " MT, ", " ST, \n", " unanswerable, ", " MT, ", " MT \n", " unanswerable, ", " ST model's attention module is pre-trained on the ST task. However, according to the article, the attention module of ST does not benefit from the pre-training. The article does mention that the attention module of the MT model is pre-trained on the MT task. However, based on the above statement, it seems like the attention module is pre-trained on the ST task but does not benefit from it. According to the article, the attention module is not pre-trained on ASR and MT tasks separately, it is task-specific. However, it also mentions that the attention module of ST does not benefit from the pre-training.", " MT, ", " unanswerable, ", " MT, ", " ST, ", " unanswerable, ", " unanswerable, ", " MT, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " MT \n", " MT, ", " MT, ", " unanswerable, ", " MT, ", " unanswerable, ", " unanswerable, ", " MT, ", " MT, ", " ST, ", " MT \n", " unanswerable, ", " unanswerable, ", " MT, ", " MT, ", " \n", " MT, ", " unanswerable, ", " ST, ", " MT, ", " unanswerable, ", " MT, ", " It is pretrained on both ASR and MT tasks. \n", " unanswerable, ", " ST,", " MT, ", " unanswerable, ", " The attention module is not pretrained. ", " unanswerable, ", " MT, ", " MT, ", " unanswerable ", " unanswerable, ", " Not pretrained on all tasks. \n", " MT task, ", " unanswerable, ", " The attention module is pre-trained on the ASR task, ", " unanswerable, ", " Unanswerable, ", " MT. \n", " MT, ", " MT, ", " unanswerable, ", " MT, ", " MT, ", " unanswerable, ", " ST, "], "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": [" no, ", " yes, ", " unanswerable, ", " yes, ", " No, ", " No, ", " no, \n", " yes, ", " no, \n", " yes, ", " unanswerable, ", " unanswerable, ", " No, ", " yes, ", " no, ", " no, ", " unanswerable, ", " unanswerable, ", " no, ", " no, ", " unanswerable, ", " No, ", " unanswerable, ", " yes, ", " no, ", " No, ", " unanswerable, ", " yes, ", " unanswerable, ", " no, ", " no, ", " no, ", " unanswerable, ", " unanswerable, ", " No, ", " no, ", " Yes, ", " no, ", " yes, ", " no, ", " no, \n", " No, ", " no, ", " no, ", " no, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " no, ", " no, ", " Yes, ", " no, ", " no, ", " unanswerable, ", ", ", " no, ", " no, ", " No, ", " no, ", " no, ", " [Your concise final answer here], ", " yes, ", " no, ", " yes, ", " unanswerable, ", " yes, ", " no, ", " unanswerable, ", " no, ", " yes, ", " unanswerable, ", " no, ", " unanswerable, ", " yes, ", " no, ", " No, ", " unanswerable, ", " unanswerable, ", " no, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " yes, ", " yes, ", " yes, ", " no, ", " No, ", " unanswerable, ", " unanswerable, ", " no, ", " No, ", " yes, ", " no, ", " no, \n", " no, ", " [Your concise final answer here], ", " no, ", " no, ", " unanswerable, ", " Yes, "], "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": [" labels for inquiry types and response types. \n", " They label the dataset into various types of inquiry and response categories.\n", " They create various labels on their dataset including open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, and response with topic drift. \n", " yes, the labels include information such as symptom triggers, extent of seriousness, frequency occurrence of the symptom, and location of symptom.\n\n", " They create labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", \"Response with Topic Drift\", \"Response with Transitional Clauses\" to categorize inquiry-response pairs in their dataset. \n", " inquiry-response pairs. \n", " ", " Inquiry, Response, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, Response with Transitional Clauses. \n", " Utterance types, \n", " Labels on 11 topics and 9 symptoms. \n", " They create 5 labels. \n", " Anaphors, thinking aloud, topic drift, and transitions. Additionally, other labels include open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. \n", " They create a set of linguistic expressions, ", " The labels on their dataset include open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. \n", " They create labels for various utterance types, including open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. \n", " Labels are created to characterize the answer types of the query into five categories: (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom. Additionally, they create labels for the answer span that points to the tag “[SEQ]” and answerable probability for the case of “No answer”. The groundtruth output of the QA system is automatically labeled based on the template generation rules, but also manually verified to ensure annotation", " They create labels of the following types: open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses., ", " they create labels for symptom checking dialogues as open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, and response with topic drift. \n", " Inquiry and response labels. ", " They create various labels for their dataset, such as open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, and yes/no response. ", " Labels of various expression types. \n", " Labels on symptom attributes. \n", " Labels of symptom monitoring conversations, ", "  Detailed Inquiry, Open-ended Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, Response with Transitional Clauses. \n", " Utterance types and answer types. \n", " labels for dialogue, symptom, attribute, and answer types, as well as expression pools for each symptom and attribute. \n", " Labels of 5 attributes and 9 symptoms, ", " labels such as open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, and response with topic drift. \n", " They create labels such as Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, and Response with Transitional Clauses. \n", " Labels,", " Various labels, including Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, and Response with Topic Drift, ", " They create 5 labels. \n", " , ", " They create several labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", \"Response with Topic Drift\", and \"Response with Transitional Clauses\". \n", " Inquiry types, utterance types, and labels for symptom and attribute. \n", " Labels of utterance types, ", " Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, Response with Transitional Clauses, and two labels of Yes/No Response \n", " Inquiry-response pairs are labeled as one of the following: open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses, ", " Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, Response with Transitional Clauses. \n", " Labels of 11 topics and 9 symptoms. \n", " They create labels to classify the utterances as different types of inquiry or response, including open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, and response with topic drift. \n", ", ", " utterance types, \n", " Table TABREF14 labels. \n", " Various labels, ", " Answer types of clinical significance, including time, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, and the location of symptom. \n", " Anaphora, Thinking Aloud, Topic Drift, Response with Revision, Response with Transitional Clauses, Inquiry with Transitional Clauses, Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Yes/No Response, Detailed Response. ", " Various expression pools, ", " Labels including five attributes, ", " Response with Revision, ", " Labels on utterance types and entities. ", " Table TABREF14 along with the corresponding occurrence frequency statistics. ", " Various expressions, ", " Utterance types, ", " Various labels. ", " Labels with five attributes, specifying certain details of clinical significance, to characterize the answer types. ", " No Answer, ", " They create various expression pools for each symptom and each attribute to account for different types of paraphrasing and descriptions. ", " The labels are manually categorized into different types, including open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, and others. ", " Inquiry-response pairs in the dataset are labeled as different types, which are summarized in Table TABREF14. \n", " utterance types, ", " 5, ", " they create labels such as yes/no response, detailed inquiry, open-ended inquiry, multi-intent inquiry, inquiry with transitional clauses, reconfirmation inquiry, detailed response, response with revision, response with topic drift, response with transitional clauses, ", " Labels including five attributes, specifying certain details of clinical significance, such as time, activities, extent, frequency, and location, that characterize the answer types of the dialogue comprehension system. \n", " labels such as open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, and response with topic drift. \n", " They create five labels on their dataset: the time the patient has been experiencing the symptom, activities that trigger the symptom (to occur or worsen), the extent of seriousness, the frequency occurrence of the symptom, and the location of symptom. \n", " \n", " Label types for symptom checking dialogues, including open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. \n", " labels for inquiry and response, ", " Utterance labels and groundtruth output. \n", " Utterance types, ", " labels are manually annotated on the dataset, ", " linguistic expression labels. \n", " Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, and Response with Transitional Clauses. \n", " Anaphora, thinking aloud, and topic drift, ", " They create five attributes that are defined to characterize the answer types of a symptom. \n", " labels for utterance types, ", " utterance types, ", " No Answer, \n", " Inquiry-response pairs are labeled with five utterance types. ", " They create labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", \"Response with Topic Drift\", and \"Response with Transitional Clauses\". \n\n", " Labels for entities, attributes, and answer types, ", " They create labels that can take on different linguistic expressions, defined as entities. ", " Inquiry-response pairs are labeled with one of the types such as \"open-ended inquiry\" and \"yes/no response\". \n", " they create labels such as Open-ended Inquiry, Detailed Inquiry, Multi-Intent Inquiry, Reconfirmation Inquiry, Inquiry with Transitional Clauses, Yes/No Response, Detailed Response, Response with Revision, Response with Topic Drift, and Response with Transitional Clauses. \n", " Various expression pools for each symptom and each attribute, \n", " Labels, Answer types and Groundtruth output. ", " Annotating the dataset with labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", \"Response with Topic Drift\", and \"Response with Transitional Clauses\". \n", " Various labels such as \"Open-ended Inquiry\", \"Detailed Inquiry\", \"Multi-Intent Inquiry\", \"Reconfirmation Inquiry\", \"Inquiry with Transitional Clauses\", \"Yes/No Response\", \"Detailed Response\", \"Response with Revision\", \"Response with Topic Drift\", and \"Response with Transitional Clauses\". \n", " They create a list of labels including open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. \n", " labels for symptom, attribute, and their combinations, ", " Linguistic characterization, ", " Utterance types, ", " utterance types. \n", " various expressions from the seed data and synonyms, \n", " [Your concise final answer here], ", " Five attributes, ", " Label types that include open-ended inquiry, detailed inquiry, multi-intent inquiry, reconfirmation inquiry, inquiry with transitional clauses, yes/no response, detailed response, response with revision, response with topic drift, and response with transitional clauses. \n", " They create 5 attributes labels: time, activities, extent, frequency, and location, and 9 symptoms labels. ", " \n"], "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": [" unanswerable. ", " unanswerable, \n", " conventional LSTM models, \n", " 2-layers regular-trained model, ", " unanswerable, ", " They compared the performance of 6-layers LSTM model, 7-layers LSTM model, 8-layers LSTM model and 9-layers LSTM model. \n", " Regular-trained LSTM, \n", " Xavier initialization algorithm and CE models, ", " unanswerable, ", " 2-layers model and 9-layers model, ", " unanswerable, ", " The 6-layers and 9-layers sMBR models, ", " regular-trained models, ", " unanswerable \n", " conventional RNNs, ", " unanswerable \n", ", ", " 2-layers, 6-layers, 7-layers, 8-layers, 9-layers LSTM models, ", " unanswerable, ", " 2-layers and 9-layers unidirectional LSTM models, \n", " unanswerable, \n", " 2-layers regular trained model, ", " unanswerable, \n", " the 2-layers model and 9-layers model \n", " regular-trained lstm models and layer-wise trained lstm models. \n", " unanswerable, ", " regular trained models \n", " 2-layers regular-trained model, \n", " layer-wise trained 6-layers, 9-layers and 2-layers sMBR models, and 2-layers regular trained LSTM. \n", " conventional RNNs, ", " The original LSTM model, the layer-wise trained model, the 9-layers distilled model, and the 2-layers distilled model. ", " CE models, \n", " 2-layers and 9-layers models, ", " unanswerable, \n", " The regular LSTM models trained with Xavier initialization and the CE models, ", " 2-layers and 9-layers, ", " unanswerable, ", " \n", " 6-layers and 9-layers, ", " None. \n", " unanswerable, ", " unanswerable, ", " They compared with 2-layers regular-trained model. \n", " 2-layers, 6-layers and 9-layers models, \n", " unanswerable, ", " they compared with 2-layers regular trained model, \n", " conventional rnn and bidirectional lstm. ", " unanswerable, ", " unanswerable, ", " regular-trained Amap model and 2-layers regular-trained Amap model. \n", " unanswerable \n", " regular-trained LSTM, ", " unanswerable \n", " 2-layers LSTM model and 6-layers LSTM model, ", " Xavier initialization algorithm and LSTM models trained with layer-wise pre-training method. \n", " 2-layers regular-trained model. \n", " regular-trained LSTM models and unidirectional LSTM models. \n", " The 6-layers model, the 9-layers model and the 2-layers model. \n", " unanswerable, ", " unanswerable, ", " 6-layers and 9-layers sMBR models, ", " the 6-layers model, the 9-layers model, and the 2-layers model. \n", " They compared with bidirectional LSTM models. \n", " unanswerable \n", " layer-wise training with soft target and hard target, \n", " Shenma model and Amap model. \n", " conventional rNns, ", " unanswerable \n", " regular-trained 2-layers LSTM model, ", " unanswerable, ", " unidirectional and bidirectional lstm models, \n", " regular-trained Amap model and 2-layers Shenma model further trained with sMBR \n", " unidirectional LSTM and conventional LSTM, ", " unanswerable, ", " unanswerable \n", " unanswerable, \n", " 2-layers distilled LSTM and 2-layers regular-trained LSTM, ", " unanswerable, \n", " unanswerable, ", " 6-layers and 9-layers sMBR models, \n", " layer-wise trained and regular-trained LSTM models, ", " unanswerable. \n", ", ", " unanswerable, ", " unidirectional LSTM model and conventional RNNs, \n", " unanswerable, ", " unanswerable, ", " conventional RNNs and bidirectional LSTM models, ", " 6-layers and 9-layers sMBR models, ", " conventional rnn, unidirectional lstm. \n", " Xavier initialization, 6-layers model, 9-layers model, 2-layers model, 2-layers regular-trained LSTM. \n", " The models they compared with include regular-trained models and CE models., ", " 2-layers regular trained LSTM model, ", " unanswerable, \n", " unanswerable, \n", " Shenma model and Amap model, ", " regular-trained Amap model, \n", " regular-trained LSTM models, ", " \n", " the 2-layers regular-trained model. \n"], "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": [" \n", " Slight improvement. \n", " \n", " Significant, ", " Slightly higher than 10%.  \n", " Significant improvement, ", " Not mentioned, ", " Significant improvements with ELMo. \n", " Significant, \n", " Significant improvement, ", " ELMo embeddings show the largest improvement over fastText embeddings. \n", " No significant improvement, ", " Slight, ", " Significant improvement, ", " Significant improvement over fastText embeddings, \n", " Significant improvement, ", " Not directly stated, but can be inferred that it's not very significant, ", " A significant improvement, ", " Improvement in performance for Estonian in the NER task is relatively small compared to other languages. \n", " Unanswerable \n", " Significant, ", " Significant improvement, ", " Significant, ", " Significant, \n", " The ELMo embeddings improve the results over fastText embeddings by 11-25%. \n", " Significant improvement over fastText embeddings, ", " \n", " The improvement in performance for Estonian in the NER task is significant, especially when compared to the fastText embeddings. \n\n", "  Significant, ", ", ", " The improvement is significant, as shown in the results. \n", " Significant improvement over fastText embeddings, \n", " Slight improvement, ", " Significant, ", " ELMo embeddings show a significant improvement over fastText embeddings. \n", " Significant improvement, ", " ELMo embeddings show the improvement over fastText embeddings, but the exact percentage or number is not specified. \n", " A significant improvement. \n", " Unanswerable, \n", " Significant, ", " \n", " Unanswerable, \n", " A significant improvement, \n", " Significant improvements over fastText embeddings, \n", " Significant, ", " Slight improvement, ", " Significant, ", " Slight improvement, ", " The performance of ELMo embeddings for Estonian improves, but the exact percentage is not stated. \n", " The results show a significant improvement in the Estonian NER task. \n", " \n", " Improvement of 12 percentage points compared to fastText. \n", " Significant improvement, ", " ELMo embeddings show a significant improvement over fastText embeddings. \n", " \n", " Significant improvement, \n", " Significant, ", " Significant improvements, ", " Significant improvement, ", " Significant improvements were observed, ", " Significant improvement, ", " Significant improvement over fastText embeddings, ", " Significant, ", ", ", " \n", " Unanswerable, ", " The improvement in performance for Estonian in the NER task is shown in Table TABREF21. \n", " A significant improvement. \n", " Significant improvement, ", " A significant improvement. \n", " Improvement is seen in comparison to fastText but exact numbers not explicitly stated. \n", " significant improvement over fastText, ", " ", " Significant improvement. \n", " The ELMo model improves the results, ", " Significant improvement over fastText embeddings, ", " Significant, ", " Significantly better than ELMoForManyLangs. \n", " Unanswerable, \n", " Significant improvement, \n", " Significant improvement, ", " Significant improvement, ", " Unanswerable, ", " \n", " Unanswerable, \n", " Significantly improved. \n", " ELMo embeddings improve the results. \n", " Significant improvements, ", " Significant, ", " Significant, \n", " Improvement in performance for Estonian in the NER task is not explicitly reported. However, the improvement in performance for other languages is significant, especially for languages with the smallest NER datasets, such as Croatian and Lithuanian. \n\n", " Significant improvement with ELMo also on Estonian. \n", " significant improvement, ", " Significant improvement. \n", "  No significant improvement in the NER task, ", " Significant improvement. \n", " Significant improvements with ELMo also on Estonian. \n", " Significant improvement over fastText embeddings. \n", " Significant improvement, ", " Slight improvement, "], "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": [" Machine translation tasks. ", " Machine translation tasks, ", " Machine translation tasks. \n", " Machine translation tasks, ", " Neural machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks, ", " Machine translation tasks. \n", " Neural Machine Translation. ", " Machine translation tasks. ", " Neural machine translation, ", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation, ", " Machine translation tasks. \n", " Machine translation tasks, ", " Neural machine translation, ", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks. \n", " Neural machine translation, ", " Neural machine translation tasks. \n", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. ", " Neural machine translation tasks. \n", " Neural machine translation tasks. \n", " Machine translation tasks. \n", " Machine translation, ", " Machine translation tasks. ", " Neural machine translation, ", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks. \n", " Machine translation tasks. \n", " Machine translation tasks. \n", " Machine translation tasks. ", " Neural machine translation, ", " Neural machine translation, ", " Machine translation tasks. ", " Neural machine translation, ", " Machine translation tasks, ", " Machine translation tasks, ", " Machine translation tasks. \n", " Machine translation tasks, ", " Neural machine translation. ", " Neural Machine Translation, ", " Machine translation tasks. \n", " Neural machine translation. \n", " Neural machine translation. ", " Machine translation, ", " Neural machine translation, ", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks. \n", " Neural machine translation, ", " Neural machine translation tasks, ", " Machine translation tasks. \n", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks. \n", " Machine translation tasks, ", " Machine translation tasks. \n", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks. ", " Machine translation tasks, ", " Neural machine translation tasks. \n", " Machine translation tasks. ", " Neural machine translation, ", " Machine translation tasks, ", " Neural machine translation tasks. \n", " Neural machine translation, ", " Neural machine translation, ", " Machine translation tasks. ", " Machine translation, ", " Neural machine translation, ", " Neural machine translation tasks. \n", " Machine translation tasks, ", " Machine translation, ", " Neural machine translation tasks. \n", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks, ", " Machine translation tasks. \n", " Machine translation tasks, ", " Machine translation tasks. ", " Neural Machine Translation. ", " Machine translation tasks. ", " Machine translation tasks. ", " Machine translation tasks. \n", " Machine translation tasks. ", " Machine translation tasks. \n", " Neural machine translation. \n"], "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": [" The Nguni languages and the Sotho languages, ", " The Nguni languages are similar to each other and harder to distinguish, and the same is true of the Sotho languages. \n", " The Nguni languages and the Sotho languages are similar to each other. \n", " Nguni languages, ", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other. ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages. \n", " Nguni languages, Sotho languages, and English and Afrikaans, ", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other. \n\n", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other. \n", " Nguni languages and Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other. \n", " The Nguni languages and the Sotho languages, ", " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages are similar to each other, and the same is true of the Sotho languages. \n", " The Nguni languages and the Sotho languages. ", " The Nguni languages are similar to each other, and the Sotho languages are similar to each other. \n", " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages., ", " The Nguni languages and the Sotho languages. ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " Nguni and Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages are similar to each other, ", " Nguni languages and Sotho languages, ", " The Nguni languages and the Sotho languages, ", " Nguni languages and Sotho languages. \n", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other. \n\n", " Nguni languages and Sotho languages, ", " Nguni languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages are similar to each other. \n", " Nguni languages and Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn), ", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages, ", " ", " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. \n", " Nguni languages and Sotho languages, ", " Nguni languages and Sotho languages, ", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn)., ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages are similar to each other. \n\n", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages. \n", " Nguni languages and Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages, ", " Nguni languages and Sotho languages, ", " Nguni and Sotho languages, ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages, ", " Nguni languages (zul, xho, nbl, ssw) and Sotho languages (nso, sot, tsn) are similar to each other. \n", " The Nguni languages and the Sotho languages. ", " The Nguni languages are similar to each other. ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages., ", " Nguni and Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages, ", " Nguni languages and Sotho languages, ", " Nguni languages, ", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages are similar to each other. ", " The Nguni languages and the Sotho languages. \n", " Nguni languages (zul, xho, nbl, ssw) and Sotho languages (nso, sot, tsn) are similar to each other. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages, ", " Nguni languages and Sotho languages are similar to each other. \n", " The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tsn) are similar to each other. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages are similar to each other. \n", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages. \n", " Nguni languages and Sotho languages. \n", " Nguni languages (zul, xho, nbl, ssw) and Sotho languages (nso, sot, tsn) are similar to each other. ", " The Nguni languages and the Sotho languages, ", " The Nguni languages and the Sotho languages. \n", " The Nguni languages and the Sotho languages, "], "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": [" interdisciplinary. \n", " They have diverse disciplinary backgrounds. \n\n", " Scholarly background. ", " They have a diversity of disciplinary backgrounds. \n", " They have a diversity of disciplinary backgrounds and research practices. \n", " The humanities and social sciences. ", " They have a background in computational analysis and social science. \n", " computational methods, social sciences, humanities, machine learning, natural language processing. \n", " Interdisciplinary background with a diversity of disciplinary backgrounds and research practices. ", " From diverse disciplinary backgrounds. \n", " Interdisciplinary backgrounds. ", " Researchers in the humanities and the social sciences. ", " Humanistic and social scientific background. \n", " No specific background is mentioned in the article, but it is mentioned that the authors have \"diversity of disciplinary backgrounds and research practices\". \n\n", " They have a diversity of disciplinary backgrounds. \n", " Interdisciplinary collaborations. ", " Humanities and social sciences. \n", " A background in thick cultural and societal questions on the one hand, and the computational analysis of rich textual data on larger-than-human scales on the other. \n", " From very different disciplines. ", " Interdisciplinary backgrounds and research practices. ", " Interdisciplinary backgrounds and research practices. \n", " interdisciplinary backgrounds and research practices\n\n", " Scholarly background in both the humanities and the social sciences. ", " Unanswerable ", "  ", " They have a background in both humanities and social sciences. ", " None, ", " They have a diversity of disciplinary backgrounds. \n", " They have a diversity of disciplinary backgrounds and research practices. \n", " Interdisciplinary. ", " A diverse background. ", " Researchers in humanities and social sciences. ", " They have diversity of disciplinary backgrounds and research practices. \n", " None, ", " Interdisciplinary backgrounds and research practices. \n", " The authors have a background in a variety of disciplines. \n", " Varied disciplinary backgrounds. \n", " A background in humanities and social sciences. ", " Interdisciplinary backgrounds and research practices. \n", " Background in the humanities and the social sciences, as well as computational analysis. ", " Interdisciplinary backgrounds. ", " Mixed. ", " Interdisciplinary collaborations, ", " They have a diversity of disciplinary backgrounds. ", " A diverse disciplinary background, including social sciences and humanities. \n", " Interdisciplinary backgrounds. \n", " Interdisciplinary backgrounds. ", " Unanswerable, ", " Researchers come from humanities and social sciences. \n", " Interdisciplinary backgrounds and research practices. ", " They have a diversity of disciplinary backgrounds and research practices. ", " Humanities and social sciences. ", " They have a background in social and cultural concepts. ", " Scholarly. ", " Humanitarians and social scientists. \n", " The authors have diverse disciplinary backgrounds. ", " Scholarly background. ", " Diverse disciplinary backgrounds. \n", " interdisciplinary backgrounds and research practices. \n", " A diverse disciplinary background. ", " Interdisciplinary backgrounds. ", " Social and cultural. ", " Expertise from humanities and social sciences. ", "  scholars from very different disciplines, ", " diverse disciplinary backgrounds and research practices. ", "  The authors have diverse disciplinary backgrounds and research practices. \n", " Humanists and social scientists\n", " Research, ", " The authors have a diversity of disciplinary backgrounds. \n", "  ", " Interdisciplinary backgrounds. ", " They have a diversity of disciplinary backgrounds. \n", " Interdisciplinary backgrounds. \n", " diverse disciplinary backgrounds. ", " Interdisciplinary. ", " interdisciplinary backgrounds and research practices. ", " They have a diverse background. \n", "  A diverse disciplinary background. ", " Interdisciplinary backgrounds. ", " Interdisciplinary backgrounds and research practices. ", " A diverse disciplinary background. ", " They have diverse disciplinary backgrounds and research practices. ", " Unanswerable ", " They have a diversity of disciplinary backgrounds. ", ", ", " Research backgrounds from various disciplines. ", " Yes, they have a diversity of disciplinary backgrounds.\n\n", " Scholarly. ", " diverse disciplinary backgrounds, ", " They have diverse disciplinary backgrounds. \n", " They have a diversity of disciplinary backgrounds and research practices. ", " Scholars from the humanities and social sciences. \n", " Interdisciplinary. ", " Unanswerable, ", "  \n", " A background of thick cultural and societal questions on the one hand, and the computational analysis of rich textual data on larger-than-human scales on the other. \n\n", " They come from diverse disciplines. ", " Interdisciplinary. ", " Turing fellow. \n", " The authors come from various disciplines. "], "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": [" unanswerable, ", " unanswerable \n", " unanswerable \n", " unanswerable ", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable ", " 5k, \n", " unanswerable ", " The data needed to train the task-specific encoder is not specified in the article, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " Unanswerable, \n", " unanswerable, ", " 15 epochs with a dataset of 5k abstracts, ", " 5k instances, ", " Unanswerable, ", " unanswerable, ", " unanswerable ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " \n", " 5k instances, ", " unanswerable, \n", " unanswerable, ", " unanswerable \n", " unanswerable, ", " Unanswerable, ", " unanswerable, \n", " unanswerable \n", " unanswerable \n", " unanswerable ", " unanswerable \n", " unanswerable \n", " 9 folds of the training data, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " 9/10ths of the data, ", " unanswerable, \n", " unanswerable \n", " unanswerable\n", " unanswerable, ", " Unanswerable, \n", " The data needed to train the task-specific encoder is not specified in the article. \n", " unanswerable, ", " unanswerable \n", " The article does not specify the exact amount of data needed to train the task-specific encoder, ", " The article does not explicitly mention the exact data size needed to train the task-specific encoder. \n\n", " unanswerable, ", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable \n", " unanswerable \n", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " 9 folds of the training data, \n", " The data size is not mentioned in the text, ", " unanswerable\n", " unanswerable, ", "  Question2: How much data is needed to train the task-specific encoder, specifically for the sentence difficulty prediction task?\nQuestion1: Can a system be trained using solely lay annotations?\nQuestion2: What is the ratio of expert to crowd annotations for specialized tasks?\nQuestion3: Does the ratio of expert to crowd annotations depend on the cost and availability of domain experts?\nQuestion4: Is expert annotation preferable to crowd annotation for biomedical information extraction tasks?\nQuestion5: Can a system be trained using solely lay annotations?\nQuestion6: Can a system trained on combined data achieve better predictive performance when experts annotate difficult examples?\nQuestion7: What is", " The task-specific encoder is not trained to require any additional data beyond the training set. \n", " unanswerable \n", " 4,741, ", " unanswerable ", " unanswerable \n", " Unanswerable \n", " unanswerable, ", " unanswerable\n", " unanswerable, ", " 4,741, ", " unanswerable, ", " unanswerable \n", " Unanswerable, \n", " unanswerable \n", " unanswerable, \n", " 9 folds of the training data, ", " unanswerable \n", " unanswerable, ", " unanswerable\n", " unanswerable\n", " unanswerable, ", " 9 folds of the training data. \n", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable \n", " Unanswerable, ", " unanswerable \n", " Unanswerable \n"], "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": [" The source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with few or no labeled data. ", " The source domain and the target domain, ", " The source domain and the target domain. \n", " Source and target domains are two different application domains, with the source domain having sufficient labeled data and the target domain having few or no labeled data. \n", " The source domain and the target domain. \n", " Source domain and target domain. \n", " The source and target domains are different application domains with sufficient labeled data in the source domain but very few or no labeled data in the target domain. \n", " Source domain and target domain. \n", " The source domain and the target domain. \n", " Source and target domains. \n", " The source domain and target domain. ", " The source and target domains are application domains. \n", " Source domain and target domain, ", " The source and target domains are defined as existing and new domains where we have sufficient labeled data and few or no labeled data respectively. \n", " Source domain and target domain. ", " The source and target domains are two different data distributions used for training and adaptation. \n\n", " The source domain and the target domain. \n", " Source and target domains are existing and new application domains with sufficient and few or no labeled data respectively. \n", " The source and target domains are two different domains for sentiment analysis, with the source domain having sufficient labeled data and the target domain having very few or no labeled data. \n\n", " Source and target domains. ", " The source domain and the target domain. ", " Source and target domains. ", " The source domain and the target domain. \n", " Source domain and target domain, ", " The source domain and the target domain. \n", " The source and target domains, ", " Source domain and target domain, ", " The source and target domains are the domains with sufficient and insufficient labeled data, respectively. \n", " The source and target domains, ", " Source and target domains, ", " The source domain is a domain with sufficient labeled data, while the target domain is a new domain with very few or no labeled data. \n", " The source domain and the target domain. ", " Source and target domains, ", " Source domain and target domain. \n", " The source and target domains. ", " Source and target domains, ", " The source and target domains, ", " Source and target domains, ", " The source domain is the existing domain with sufficient labeled data, while the target domain is the new domain with few or no labeled data. \n", " The source domain and the target domain. \n", " Source domain and target domain. ", " The source domain and the target domain are the existing domain and the new domain where we have sufficient labeled data and very few or no labeled data respectively. ", " Source and target domains, ", " Source and target domains. ", " Source and target domains. \n", " The source and target domains, ", " Source domain and target domain, ", " The source domain and the target domain are two different domains with different distributions of data. ", " The source domain and the target domain are two different domains where the source domain has labeled data while the target domain has little or no labeled data. ", " The source domain is the existing domain with sufficient labeled data, and the target domain is the new domain with very few or no labeled data. ", " Source and target domains, ", " Source and target domains are the existing and new application domains, respectively. \n", " Source and target domains, ", " Source domain and target domain. \n", " The source and target domains. ", " Source domain and target domain, ", " Source and target domains, ", " Source and target domains, ", " Source and target domains are defined as the existing domain with sufficient labeled data and the new domain with few or no labeled data. \n\n", " The source and target domains are existing and new domains with sufficient and no labeled data respectively. \n", " The source and target domains. \n", " The source domain is a domain with sufficient labeled data, while the target domain is a new domain with very few or no labeled data. \n\n", " Source and target domains are respectively domains with sufficient labeled data and no labeled data. ", " Source and target domains, ", " Source domain and target domain. \n", " Source domain and target domain. \n", " The source and target domains, ", " The source domain is where we have sufficient labeled data, whereas the target domain is where we have very few or no labeled data. \n", " Source and target domains. ", " The source domain and the target domain are application domains that are used for training data, where the source domain has sufficient labeled data, and the target domain has very few or no labeled data. \n", " Source and target domains, ", " The source and target domains are called different application domains, where the source domain has sufficient labeled data and the target domain has very few or no labeled data. \n", " Source domain and target domain, ", " The source domain is the existing domain with sufficient labeled data, and the target domain is the new domain with very few or no labeled data. \n", " The source domain and the target domain. \n", " Source and target domains. ", " The source and target domains. ", " Source and target domains, ", " Source and target domains, ", " The source domain is an existing domain with sufficient labeled data and the target domain is a new domain with very few or no labeled data. ", " Source domain and target domain, ", " Source and target domains are application domains with different distributions. ", " Source and target domains. ", " The source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with very few or no labeled data. \n", " The source domain and the target domain are two sets of data drawn from different distributions, with the source domain having sufficient labeled data and the target domain having very few or no labeled data. ", " The source and target domains, ", " The source domain and the target domain. \n", " The source domain and target domain are two domains used in cross-domain sentiment classification, where the source domain has sufficient labeled data and the target domain has very few or no labeled data. ", " The source domain is the existing domain with sufficient labeled data, and the target domain is the new domain with few or no labeled data. ", " Source and target domains, ", " Source domain and target domain. ", " Source and target domains. ", " The source and target domains are the domains from which we have sufficient labeled data and few or no labeled data, respectively, for the task at hand. ", " Source and target domains, ", " Source and target domains. \n", " The source and target domains. ", " Source domain and target domain. ", " The source and target domains. ", " Source domain and target domain, ", " The source domain and target domain are defined as two different domains, where the source domain is an existing domain with sufficient labeled data, and the target domain is a new domain with very few or no labeled data. \n\n"], "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": [" The Wikipedia dataset consists of 29,794 articles. \n", " The Wikipedia dataset consists of around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles. The total dataset size is around 29,794 articles. ", " 29,794, ", " \n", " 29,794 articles, ", " 29,794 articles. \n", " ", " The data set has 29,794 articles. ", "  ", " The size of their dataset varies, ", " The Wikipedia dataset consists of around 29,794 articles, ", " 29,794 articles, \n", " 29,794 articles, ", " The data set contains 29,794 articles. \n", " Approximately 29,794 articles. ", " 29,794, ", " Over 29,000, \n", " 29,794 articles. ", " 29,794 articles, ", " 29,794 articles. \n", " Approximately 29,794 articles. \n", " unanswerable, ", " 29,794 articles, \n", "  29,794 articles. ", " The data set consists of 29,794 articles, which was randomly sampled from 5,000 articles from each of six quality classes. \n", " 29,794 articles, ", " 29,794 articles, ", " unanswerable, ", " 29,794 articles. \n", " 29,794 articles. ", " 29,794 articles, ", " 29,794 articles, ", " 29,794 articles, ", " ", " 29,794 articles.  \n", " 29,794, \n", " 29,794, ", " Around 29,794 articles, ", " 29,794 articles. ", " Around 5,000 articles from each quality class, ", " 29,794 articles, \n", " ", " around 29,794 articles, ", " Their dataset is 29,794 articles, ", " They constructed a dataset with around 29,794 articles. ", " 29,794 articles. \n", " ", " Approximately 29,794 articles. \n", " 29,794 articles, ", " The dataset is 29,794 articles. ", " Around 29,794 articles. \n", " 29,794, ", ", ", " 29,794 articles, ", " 29,794 articles, ", " They have a dataset of 29,794 articles. ", " \n", " 29,794 articles. ", " The Wikipedia dataset has around 29,794 articles. \n", " 29,794 articles. \n", " It contains 29,794 articles. \n", " 29,794 articles, ", " Around 5K articles from each quality class, with a total of 29,794 articles. \n", " 29,794, ", " 29,794 articles. \n", " The dataset contains around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles, ", " Approximately 29,794 articles, ", " 29,794 articles, ", " ", " It is quite large. ", " The Wikipedia dataset has a total of 29,794 articles. \n", " Over 29,794 articles. \n", " \n", " Around 5,000 articles from each quality class. \n", " 29,794 articles. \n", " 29,794 articles. \n", " 29,794 articles, ", " 29,794 articles, ", " unanswerable, ", " Their data set is about 29,794 articles. \n", " 29,794 articles, ", " 29,794 articles. ", " 29,794, ", " 29,794 articles, ", " They have a data set of 29,794 articles, ", " unanswerable. \n", " 29,794 articles, ", " 29,794, ", " \n", " 29,794 articles. \n", " The Wikipedia dataset consists of around 29,794 articles. ", " The Wikipedia dataset consists of around 5,000 articles from each of the six quality classes. There are 29,794 articles in total. ", " around 29,794 articles, ", " \n", " The data set consists of around 29,794 articles. ", " 29,794 articles, ", " Around 29,794 articles, \n", " 29,794 articles. \n", " Around 5,000 articles. ", " Approximately 29,794 articles. "], "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": [" NeuronBlocks includes modules such as embedding layer, CNN/RNN, Transformer, attention mechanisms, and regularization layers. \n", " Embedding Layer, Neural Network Layers, Loss Function, and Metrics, ", " Various, ", " Neural network modules including Embedding Layer, Neural Network Layers, Loss Function, and Metrics, and other modules like word/character embedding, CNN/RNN, Transformer, attention mechanisms, and regularization layers. \n", " ", " Word/character embedding, CNN/RNN, Transformer, attention mechanisms, and regularization layers such as Dropout, Layer Norm, Batch Norm, etc. \n", " Various neural network modules, including embedding layer, neural network layers, and loss function. \n", " Common layers like RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, etc. ", " \n", " Several end-to-end network templates for common NLP tasks and a suite of reusable and standard components, including embedding layers, neural network layers, attention mechanisms, regularization layers, and loss functions, are included in NeuronBlocks. \n", "  ", " embedding, CNN/RNN, Transformer, etc, ", ", ", " Embedding Layer, Neural Network Layers, Loss Function, Metrics, ", " They include embedding layers, neural network layers such as RNN, CNN, QRNN, Transformer, and attention mechanisms, and loss functions like Focal Loss. \n", " Embedding Layer, Neural Network Layers, Loss Function, Metrics. ", " \n", " Word/character embedding, CNN/LSTM, CRF, and other various layers, ", " \n", " These include embedding, CNN/RNN, Transformer, and other common components. \n", " The modules include word/character embedding, extra handcrafted feature embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers, and loss functions. \n", " Several common layers including word/character embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers, loss functions and metrics. ", " Various common components such as embedding, CNN/RNN, Transformer, etc. \n", " Various commonly used neural network modules, including Embedding Layer, Neural Network Layers, Loss Function, and Metrics, are included in NeuronBlocks, ", ", ", " ", " Word/character embedding, CNN/RNN, Transformer, Embedding Layer, Neural Network Layers, Loss Function, Metrics, , Attention mechanisms, Regularization layers. \n", " A variety of neural network modules, including embedding layer, neural network layers, loss function, and metrics., ", " Embedding Layer, Neural Network Layers, Loss Function, Metrics, knowledge distillation template for extractive machine reading comprehension, Knowledge distillation template, knowledge distillation template, knowledge distillation, Regularization layers, Attention mechanisms, Embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, Dropout, Layer Norm, Batch Norm, Focal Loss, AUC, Accuracy, Precision/Recall, F1 metrics, F1/Accuracy, MSE/RMSE, ExactMatch/F1, word/character embedding, CNN/LSTM and CRF, BiLSTM, Attention,", " Various common neural network components, ", " Various neural network modules. \n", " Neural network modules such as embedding, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, Dropout, Layer Norm, Batch Norm, and Focal Loss are included in NeuronBlocks. \n", " Word/character embedding, CNN/RNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers such as Dropout, Layer Norm, Batch Norm, loss functions built in PyTorch, and Focal Loss,  are included in NeuronBlocks. \n", " Various neural network components including embedding, CNN/RNN, Transformer, attention mechanisms, regularization layers, etc., are supported by NeuronBlocks. \n", " embedding, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, Dropout, Layer Norm, Batch Norm, Focal Loss, AUC, Accuracy, Precision/Recall, F1 metrics, MSE/RMSE, ExactMatch/F1. \n\n", " Neural Network Layers, Embedding Layer, and Loss Function are included. Additionally, the following are also supported: attention mechanisms, dropout, layer norm, batch norm, QRNN, Transformer, Highway network, Encoder Decoder architecture, focal loss, and pre-built models. ", " Commonly used modules such as word/character embedding, CNN/RNN, Transformer, and more. \n", " Various common components, including embedding, CNN/RNN, Transformer, and attention mechanisms, are supported in NeuronBlocks, which can be used as building blocks to construct networks with complex architecture. \n\n", " Word/character embedding, CNN/RNN, Transformer, and attention mechanisms. \n", " Neural Network Layers and Embedding Layer. \n", " word/character embedding, CNN/RNN, Transformer, attention mechanisms, dropout, layer norm, batch norm, and focal loss. \n\n\n", " Embedding Layer, Neural Network Layers, Loss Function, and Metrics. ", " Word/character embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, Dropout, Layer Norm, Batch Norm, Focal Loss, etc. \n", " \n", " Neural Network Layers, Embedding Layer, Loss Function, Metrics, \n", " Multiple neural network modules are included, such as embedding layers, common neural network layers, multiple attention layers, and regularization layers. \n", " Neural Network Layers, Embedding Layer, Loss Function, Metrics, ", " word/character embedding, CNN/RNN, Transformer, embedding, dropout, layer norm, batch norm, focal loss, etc. ", " \n", " Various neural network modules such as embedding layers, neural network layers, loss functions, and metrics. \n", " Various neural network modules, including embedding layer, neural network layers, loss function and metrics, are included in NeuronBlocks, with a specific breakdown of embedding layer, which includes word/character embedding and handcrafted feature embedding; neural network layers, which include RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, and attention mechanisms, as well as regularization layers; loss function, which includes Focal Loss; and metrics, which include AUC, Accuracy, Precision/Recall, F1 metrics, and more. \n", " Neural network modules included in NeuronBlocks include embedding layer, neural network layers, loss function, and metrics. ", " Word/character embedding, CNN/RNN, Transformer, extra handcrafted feature embedding, dropout, layer norm, and attention mechanisms, etc. \n", " Word/character embedding, CNN/RNN, Transformer and extra handcrafted feature embedding, regularisation layers such as Dropout, Layer Norm, Batch Norm, attention mechanisms, loss functions, and metrics. \n", " \n", ", ", " Word/character embedding, CNN/RNN, Transformer, embedding, QRNN, Highway network, Encoder Decoder architecture, etc. \n", " RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow. ", " \n", " \n", " Word/character embedding, CNN/RNN, Transformer, Dropout, Layer Norm, Batch Norm, and several attention mechanisms, among others. \n", " Neural Network Layers, Embedding Layer, Loss Function, Metrics, ", " \n", " Embedding Layer, Neural Network Layers, Loss Function, Metrics, ", " A wide range of modules including word/character embedding, CNN/LSTM, attention mechanisms, regularization layers, loss functions, and metrics. \n", " Embedding Layer, Neural Network Layers, Loss Function, Metrics, Knowledge Distillation, Extractive Machine Reading Comprehension. \n", "  ", " Embedding Layer, Neural Network Layers, Loss Function, and Metrics, ", " They include embedding layer, neural network layers, loss function, and metrics. ", " The modules include embedding layers, neural network layers, loss functions, and metrics. \n", " \n", " Embedding Layer, Neural Network Layers, Loss Function, Metrics, , ", " Word/character embedding, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, and multiple attention layers. \n", " Word/character embedding, CNN, RNN, Transformer, Highway network, Encoder Decoder architecture, Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow, Dropout, Layer Norm, Batch Norm, Focal Loss,  etc, ", " Various modules, including word/character embedding, CNN, RNN, Transformer, QRNN, etc. are included in NeuronBlocks. ", "  ", " \n", " Embedding Layer, Neural Network Layers, Loss Function, Metrics. ", " A variety of modules including embedding, CNN/RNN, Transformer, attention mechanisms, regularization layers, and loss functions, ", " Various neural network modules including embedding layer, neural network layers, loss functions, and metrics are included in NeuronBlocks. ", " Embedding layer, neural network layers, loss function, metrics, \n\n", " \n", " Embedding Layer, Neural Network Layers, Loss Function, and Metrics., ", " Embedding Layer, Neural Network Layers, Loss Function, Metrics. ", " Word/character embedding, CNN/RNN, Transformer, embedding, and attention mechanisms, etc. are included in NeuronBlocks. \n", " \n", " The most commonly used components of deep neural networks, ", " NeuronBlocks includes several common components, such as embedding, CNN/RNN, Transformer and so on. \n", " ", " \n", " Word/character embedding, CNN, RNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers, and Loss Functions, including Focal Loss. \n", " ", ", ", " \n", " NeuronBlocks includes RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, and regularization layers such as Dropout, Layer Norm, and Batch Norm, as well as a suite of other components. \n", " ", " Embedding layer, neural network layers, loss function, and metrics. \n", ", ", " Word/character embedding and extra handcrafted feature embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, etc. ", " Neural Network Layers and Loss Function and Embedding Layer, "], "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": [" By accuracy and efficiency tradeoffs. ", " The efficiency of a communication scheme is measured as the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " By accuracy and efficiency. \n", " By retention rate of tokens and the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. ", ", ", " By measuring the retention rate of tokens and accuracy. \n", " Tokens retention rate and accuracy. \n", " By the retention rate of tokens and the accuracy of the scheme. ", " Through user studies and experiments, ", " By retention rate of tokens and accuracy of sentences. ", " Through the accuracy and retention rate of tokens. ", " They are evaluated based on the accuracy of sentence generation and retention rate of tokens. \n", " By the retention rate of tokens and the accuracy of sentences. ", " Based on token retention rate and accuracy of reconstruction. ", " By the retention rate of tokens and the accuracy of a scheme. \n", " The models are evaluated by their retention rate of tokens and accuracy of sentence generation. \n", " By measuring the accuracy of the model and the retention rate of tokens. \n", " By measuring the retention rate of tokens and the accuracy of sentences generated by greedily decoding the model. \n", " By accuracy and retention rate. ", " By measuring the retention rate of tokens and the accuracy of sentences generated by greedily decoding the model. \n", " Retention rate and reconstruction accuracy. ", " Efficiency is measured by retention rate of tokens, accuracy is measured as the fraction of sentences generated by the model that exactly matches the target sentence. ", " Accuracy and retention rate are used to evaluate models, specifically in terms of fraction of sentences generated by greedily decoding the model that exactly matches the target sentence, and the fraction of tokens that are kept in the keywords, respectively. ", " The models are evaluated by measuring the retention rate of tokens and the accuracy of sentences generated by greedily decoding the model. \n", " The efficiency of a communication scheme is measured as the fraction of tokens that are kept in the keywords, and the accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. ", " The models are evaluated by the retention rate of tokens and accuracy. \n", " The accuracy of a model is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " By measuring their efficiency and accuracy, specifically by retention rate of tokens and the fraction of sentences generated that exactly matches the target sentence. \n\n", " Accuracy is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence, and efficiency is measured as the retention rate of tokens, which is the fraction of tokens that are kept in the keywords. \n", " They are evaluated by measuring the efficiency and accuracy of the models. ", " By the accuracy and efficiency tradeoffs. \n", " By retention rate of tokens and accuracy. \n", " By the retention rate of tokens and the accuracy of a scheme. \n", " By measuring the retention rate of tokens and the accuracy of schemes. \n", " By measuring the retention rate of tokens and accuracy of the generated sentences. \n", " By measuring efficiency and accuracy of the communication scheme, ", " The efficiency of a communication scheme is measured by the retention rate of tokens, which is the fraction of tokens that are kept in the keywords, while the accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " The accuracy of a model is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence, and the efficiency of a model is measured as the fraction of tokens that are kept in the keywords. \n", " Based on efficiency and accuracy. ", " The models are evaluated in terms of efficiency (retention rate of tokens) and accuracy (fraction of sentences generated by greedily decoding the model that exactly matches the target sentence). \n", " They are evaluated based on efficiency and accuracy. \n", " Through a combination of efficiency and accuracy metrics, including the retention rate of tokens and the fraction of sentences generated by the model that exactly matches the target sentence. \n", " By quantifying the efficiency and accuracy of a communication scheme. \n", " [Your concise final answer here], ", " Through evaluation metrics such as retention rate of tokens, and accuracy of a scheme. \n", " The models are evaluated in terms of the retention rate of tokens (a measure of efficiency) and the fraction of sentences that are accurately reconstructed. \n", " By quantifying efficiency and accuracy. \n", " They are evaluated by retention rate of tokens and accuracy of sentences. \n", " Accuracy is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence, and efficiency is measured as the retention rate of tokens. ", " The models are evaluated by measuring the efficiency and accuracy of the communication schemes. \n\n", " Through metrics including retention rate of tokens, and accuracy of sentences generated by greedily decoding the model. \n", " Accuracy and efficiency are the evaluation metrics. \n", " They are evaluated by quantifying the efficiency of a communication scheme, measured by the retention rate of tokens, and the accuracy of a scheme, measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n\n", " Efficiency and accuracy. \n", " They are evaluated by retention rate of tokens and accuracy of sentences. ", " \n", " By quantifying the efficiency of a communication scheme and its accuracy. \n", " They are evaluated by the retention rate of tokens, the fraction of tokens that are kept in the keywords, and the accuracy of a scheme, which is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " Accuracy and efficiency. ", " The efficiency and accuracy of models are evaluated. ", " They are evaluated based on retention rate of tokens and accuracy. \n", " By measuring the efficiency and accuracy of the communication scheme, specifically the retention rate of tokens and the fraction of sentences generated by the model that exactly match the target sentence. \n\n", " They are evaluated in terms of efficiency and accuracy. \n", " \n", " They are evaluated based on the efficiency of a communication scheme (measured as the retention rate of tokens) and the accuracy of a scheme (measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence), ", " By quantifying efficiency and accuracy. \n", " By measuring the retention rate of tokens and the accuracy of the scheme. ", " By the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " Using a user study, retention rate of tokens, and accuracy of sentences. \n", " By measuring the retention rate of tokens, which is the fraction of tokens that are kept in the keywords, and the accuracy, which is the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n\n", " By the retention rate of tokens and the accuracy of the model. \n", " Through a tradeoff between efficiency and accuracy. \n", " by the retention rate of tokens and the accuracy of a scheme. \n", " They are evaluated by retention rate of tokens and reconstruction accuracy. ", " By measuring efficiency (retention rate of tokens) and accuracy (fraction of sentences that exactly matches the target sentence). \n", " By the retention rate of tokens and the accuracy of a scheme. \n", " By measuring retention rate of tokens and accuracy of sentences. \n", " Accuracy and efficiency are used to evaluate models. ", " By quantifying efficiency and accuracy. \n", " They are evaluated based on the efficiency-accuracy tradeoff and the retention rate of tokens. \n", " By the retention rate of tokens and the accuracy of sentences. ", " Accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " By measuring retention rate and accuracy, specifically through token retention rate and sentence accuracy. \n", " By quantifying the efficiency of a communication scheme through the retention rate of tokens and the accuracy of the scheme through the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n\n", " They are evaluated by quantifying the efficiency of a communication scheme by the retention rate of tokens, and the accuracy of a scheme as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n\n", " The efficiency and accuracy of models are evaluated. \n", " Retention rate and accuracy, \n", " They are evaluated by the retention rate of tokens and the accuracy of a scheme. \n", " By the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n", " The models are evaluated based on the efficiency and accuracy of their communication schemes. ", " By retention rate of tokens and accuracy of sentences. ", " By retention rate of tokens and accuracy of a scheme. \n", "  The models are evaluated by their accuracy and efficiency in the human-machine communication game. \n", " They are evaluated based on the efficiency and accuracy of the communication scheme, specifically the retention rate of tokens and the accuracy of the reconstructed sentences. \n", " By quantifying the efficiency and accuracy of communication schemes. \n", " By retention rate and accuracy. \n", " Through the efficiency-accuracy tradeoff. \n", " By measuring the retention rate of tokens and the accuracy of a scheme. \n", " By the retention rate of tokens and the accuracy of a scheme. \n", " By measuring the efficiency and accuracy of a communication scheme. \n"], "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": [" Yes, ", " Yes, they do test on such commonly used language pairs, specifically English-to-German. \n", " Yes, \n", " Yes, they do. \n", " yes, \n", " yes, ", " yes, \n", " Yes, ", " \n", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, they do. \n", " Yes, \n", ", ", " Yes, ", " Yes, \n", " yes, \n", " yes, ", " Yes, \n", " Yes, \n", " yes, \n", " Yes, ", " Yes, \n", " yes, ", " yes, \n", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes \n", " Yes, \n", " yes, ", " yes, ", "  ", " Yes, \n", "  ", " Yes \n", " Yes, ", " yes, \n", " yes, ", " yes \n", " yes \n", " yes, ", " yes, \n", " Yes, \n", " Yes, they test their framework performance on English-to-German. \n", " Yes, ", " yes \n", " yes, ", " yes, \n", " yes, ", " Yes \n", " yes, ", " yes, ", " Yes, ", " Yes\n", " yes, ", " Yes, \n", " Yes, they test their framework performance on commonly used language pairs, such as English-to-German. \n", " Yes, \n", " yes, ", " Yes, they test their framework performance on the English-German language pair. \n", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, they do test their framework performance on commonly used language pairs, such as English-to-German. \n", " yes, ", " Yes, ", " Yes, they do. \n", " yes, ", " Yes, \n", " yes \n", " yes, \n", " \n", " yes, ", " Yes, \n", " Yes, \n", " yes, ", " Yes, \n", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, they do test their framework performance on English-to-German language pairs. \n", " Yes, they test the performance on the under-resourced English-to-German translation task. \n", " yes\n\n", " Yes, \n", " Yes, ", " Yes, they do test their framework performance on commonly used language pairs, such as English-to-German. \n", " yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": [" LSTMs, ", " LSTM, ", " PRUs, LSTMs. \n", " AWD-LSTM, ", " AWD-LSTM and LSTM. \n", " LSTM, ", " AWD-LSTM, ", " LSTMs, ", " AWD-LSTM. \n", " state-of-the-art methods including LSTMs. \n", " LSTM. \n", " LSTMs, ", " AWD-LSTM, ", " AWD-LSTM, RAN, QRNN, NAS, LSTM. \n", " AWD-LSTM and recent state-of-the-art methods, ", " LSTMs, ", " AWD-LSTM, ", " LSTMs, \n", " AWD-LSTM. \n", " PRUs and LSTMs, ", " LSTMs and state-of-the-art methods, \n", " LSTM, ", " AWD-LSTM, ", " state-of-the-art methods, including LSTMs. ", " LSTMs, ", " AWD-LSTM and LSTMs, ", " AWD-LSTM, \n", " LSTM, ", " AWD-LSTM, ", " AWD-LSTM, ", " LSTMs, ", " AWD-LSTM, ", " LSTMs, ", " AWD-LSTM, ", " LSTMs and other recent state-of-the-art methods, \n", " LSTM, \n", " AWD-LSTM, ", " PRUs, LSTMs, and recent state-of-the-art methods,", " AWD-LSTM and other state-of-the-art methods. \n", " LSTMs, ", " AWD-LSTM, ", " AWD-LSTM, ", " AWD-LSTM, ", " PRU, \n", " AWD-LSTM. \n", " AWD-LSTM. \n", " LSTMs. \n", " AWD-LSTM, ", " AWD-LSTM, ", " PRU with LSTM, ", " LSTM and state-of-the-art methods. \n", " AWD-LSTM, ", " LSTM, ", " AWD-LSTM, ", " AWD-LSTM, ", " PRUs and LSTMs, ", " PRU with LSTMs, ", " LSTMs, ", " PRUs, LSTMs, \n", " AWD-LSTM, ", " LSTM, ", " AWD-LSTM. \n", " AWD-LSTM. \n", " AWD-LSTM, ", " LSTM and state-of-the-art methods, ", " LSTMs. \n", " AWD-LSTM, ", " State-of-the-art methods, ", " state-of-the-art methods, ", " LSTM and other state-of-the-art methods. \n", " AWD-LSTM, ", " AWD-LSTM, ", " AWD-LSTM, ", " AWD-LSTM, ", " state-of-the-art methods,", " state-of-the-art methods, \n", " AWD-LSTM, \n", " state-of-the-art methods. \n", " LSTM, ", " LSTMs, \n", " LSTMs, ", " state-of-the-art methods. \n", " LSTMs. ", " LSTMs and other previous RNN models, \n", " PRUs outperform LSTMs by about 4 points on the PTB dataset and by about 3 points on the WT-2 dataset. PRUs achieve either the same or better performance than LSTMs. \n", " AWD-LSTM, ", " AWD-LSTM, ", " LSTM, ", " LSTMs, ", " AWD-LSTM, ", " LSTMs, ", " LSTMs, ", " AWD-LSTM, ", " AWD-LSTM, ", " LSTMs, \n", " AWD-LSTM, ", " state-of-the-art methods and LSTMs. \n", " AWD-LSTM. ", " LSTMs, ", " AWD-LSTM. \n"], "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": [" F-measure, ", " Precision, Recall and F-measure, ", " Precision, recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall, F-measure, \n", " Overall accuracy, Precision, Recall and F-measure. \n", " Precision, Recall, and F-measure, ", " Precision, recall and F-measure. ", " Precision, Recall and F-measure, ", " Precision, recall, and F-measure, ", " Precision, Recall and F-measure. \n", " Precision, Recall, F-measure, and ROUGE unigram score, \n", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure. \n", " F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall, and F-measure. \n", " F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Overall accuracy, precision, recall and F-measure are looked at for classification tasks, ", " Precision, Recall, F-measure, and ROUGE, ", " Precision, Recall, F-measure, ROUGE unigram f1 scores. \n", " Precision, Recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall, F-measure, ROUGE unigram score, ROUGE unigram f1 scores, overall precision and recall. \n", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure. ", " F-measure, ", " Precision, Recall and F-measure, ", " F-measure. \n", " Precision, Recall, F-measure, ROUGE unigram score, and ROUGE f1 scores, ", " Precision, Recall and F-measure, ", " Precision, recall and F-measure, ", " Precision, Recall, and F-measure. \n", " Precision, Recall and F-measure, ", " Precision, Recall, F-measure, ROUGE unigram score, etc., ", " F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure, ", " Precision, recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall, F-measure, ROUGE unigram score, and F1 scores. \n", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall, F-measure, ROUGE, and ROUGE unigram f1 scores, Average precision, Average recall. ", " Precision, Recall and F-measure. \n", " Overall precision, recall, and F-measure, ", " F-measure, ", " F-measure, ", " Recall, Precision and F-measure, ", " Precision, Recall, F-measure, and ROUGE unigram score, ", " Precision, Recall and F-measure, ", " Precision, Recall, and F-measure. \n", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure, ", " F-measure and precision and recall. \n", " Precision, Recall and F-measure. \n", " Overall accuracy, precision, recall, F-measure, and instance-level F-measure, ", " Precision, Recall, F-measure, ROUGE unigram f1 scores, inlineform0 etc., ", " Overall accuracy for a classifier, Precision, Recall, F-measure for multi-label classification, ROUGE unigram score. \n", " F-measure, precision, and recall, ", " Precision, Recall, F-measure, ROUGE unigram score. \n", " Precision, Recall and F-measure, ", " Precision, Recall, F-measure, and ROUGE unigram score. \n", " F-measure and ROUGE unigram score, ", " Precision, Recall, F-measure, and ROUGE unigram score., ", " Precision, recall and F-measure, ", " Precision, recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall, and F-measure. \n", " Precision, recall, and F-measure, ", " Precision, Recall, and F-measure, ", " F-measure, ", " Precision, Recall, F-measure, and Accuracy, ", " Precision, Recall and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall and F-measure, ", " Precision, Recall and F-measure. \n", " Precision, Recall, and F-measure, ", " Precision, recall and F-measure, ", " F-measure, ", " F-measure, ", " Precision, recall, and F-measure, ", " Precision, Recall, and F-measure, ", " Precision, Recall, F-measure, ROUGE unigram score, Accuracy. \n"], "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": [" Human judgements were assembled by having a group of 50 native speakers rate adequacy and fluency of translations. ", " They were calculated with respect to Kappa co-efficient. \n", " The human judgements were assembled using a group of 50 native people, who were well-versed in both English and Tamil languages, as annotators for the evaluation. \n", " 50 native people were used as annotators for the evaluation, ", " The judgements were assembled from a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators. \n", " They were collected from a group of 50 native people who were well-versed in both English and Tamil languages, ", " Using a collection of samples of about 100 sentences from the test set results for comparison. \n", " The human judgements were assembled by 50 native people who were well-versed in both English and Tamil languages acting as annotators for the evaluation. \n", " Human evaluation metrics like adequacy, precision and ranking values were estimated, ", " Human judgements were assembled through a 5-point scale. ", " They were assembled using a 5-point scale, ", " Using a 5-point scale, ", " They were assembled based on a 5-point scale for fluency and adequacy, ", " They were assembled based on annotators' fluency and adequacy ratings of translations. \n", " By 50 native people who were well-versed in both English and Tamil languages acting as annotators. \n", " They were based on a randomized selection of the translation results from the test set. \n", " Human evaluators ranked the translations on a 5-point scale, ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. ", " By a group of 50 native people who were well-versed in both English and Tamil languages. \n", " The human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages. \n", " By a group of 50 native people who were well-versed in both English and Tamil languages. \n", " By a group of 50 native people who were well-versed in both English and Tamil languages, ", " By means of a 5-point scale of how much of the meaning is conveyed by the translation and a 5-point scale of grammatical correctness. \n", " The human judgements were assembled using a randomized selection of the translation results to ensure the objectivity of evaluation. ", " By a group of 50 native people who were well-versed in both English and Tamil languages. \n", "  They were assembled by a group of 50 native speakers who were well-versed in both English and Tamil languages. \n", " by a group of 50 native people who were well-versed in both English and Tamil languages. \n", "  Intra-annotator values were computed and scores are shown in Table TABREF32, BIBREF12, BIBREF13. The ranking Kappa co-efficient for intra-annotator ranking of the RNNMorph model was at 0.573, higher that the 0.410 of the RNNSearch+Word2Vec model, implying that the annotators found the RNNMorph model to produce better results when compared to the RNNSearch + Word2Vec model. \n", " , ", " The human judgements were assembled based on a collection of samples of about 100 sentences taken from the test set results for comparison, which included a randomized selection of the translation results to ensure the objectivity of evaluation., ", " By a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators for the evaluation, ", " Through a randomized selection of the translation results. ", " They were assembled by annotators through individual ranking of translations. ", " Unanswerable, ", " The human judgements were assembled by collecting 100 test set results, dividing them randomly among a group of 50 native annotators well-versed in both English and Tamil languages, and having them rank RNNMorph and RNNSearch+Word2Vec model translations of the sentences on a scale of 5 for adequacy and fluency. \n", " Human judgements were assembled by a group of 50 native speakers who were well-versed in both English and Tamil languages, ", " They were assembled from a group of 50 native Tamil speakers who were well-versed in both English and Tamil languages. \n", " The human judgements were assembled using a collection of samples of about 100 sentences, a group of 50 native people who were well-versed in both English and Tamil languages, and a randomized selection of the translation results. ", " Human annotators were used., ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators, ", " Using the modified 1-gram, 2-gram, 3-gram and 4-gram precision values of BLEU metric, a collection of samples of about 100 sentences were taken from the test set results for comparison, and a group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. \n", " Human judgements were assembled based on a 5-point scale, \n", " Based on a collection of samples of about 100 sentences from the test set results, ", " A collection of samples of about 100 sentences were taken from the test set results for comparison. \n", " The human judgements were assembled using a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None) for adequacy, and a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive) for fluency. ", " The human judgements were assembled using a 5-point scale. \n", " They were assembled by a group of 50 native people. \n", " The human judgements were assembled by a group of 50 native people. ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. They evaluated a collection of samples of about 100 sentences taken from the test set results for comparison. These samples were a randomized selection of the translation results to ensure the objectivity of the evaluation., ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation, ", " The human judgement of adequacy, fluency, and relative ranking of adequacy was calculated based on 50 native annotators rating a set of 100 sentences for each. However, the precise method of assembly of the human judgements is not clearly stated in the article. Therefore, the final answer is unanswerable. \n", " A group of 50 native speakers of Tamil acted as annotators for the evaluation. \n", " The human judgements were assembled using a 5-point scale of adequacy and a 5-point scale of fluency. ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators, ", " They were calculated on a 5-point scale of how much of the meaning is conveyed by the translation. ", " The human judgements were assembled using a collection of samples of about 100 sentences taken from the test set results for comparison. A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation., ", " Human evaluators were asked to rank a collection of samples of about 100 sentences for comparison. ", " Using a randomized selection of the translation results to ensure the objectivity of evaluation. \n", " A group of 50 native people were used as annotators., ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators, ", " They were assembled based on a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators for the evaluation. They provided adequacy, precision, and ranking values (between RNNSearch and RNNMorph outputs) for 100 sentences taken from the test set results for comparison, ensuring the objectivity of evaluation. \n", " They were assembled from 50 native speakers,", " Through the Kappa co-efficient. \n", " Through a group of 50 native speakers who were well-versed in both English and Tamil languages, acting as annotators for the evaluation. \n\n", " They were estimated using 50 native people who were well-versed in both English and Tamil languages as annotators for the evaluation. ", " Using a 5-point scale of how much of the meaning is conveyed by the translation, a collection of samples of about 100 sentences were taken from the test set results for comparison. \n", " By a group of 50 native people who were well-versed in both English and Tamil languages, ", " They were assembled using Kappa co-efficient results. \n", " By 50 native people who were well-versed in both English and Tamil languages, ", " Through a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators for the evaluation. \n", " They were assembled using a 5-point scale. ", " The human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators. ", " Using 50 native speakers, the evaluation was done based on a collection of samples of about 100 sentences taken from the test set results for comparison, ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. They were asked to rate the fluency and adequacy of the RNNMorph results. ", " The human judgements were assembled based on a 5-point scale of how much of the meaning is conveyed by the translation. The fluency rating was also calculated on a 5-point scale of grammatical correctness. Moreover, the intra-annotator values were computed for these metrics., ", " The human judgements were assembled by 50 native annotators who compared the output of RNNMorph model with RNNSearch + Word2vec model. \n", " They were assembled by collecting 50 native annotators and assigning them to evaluate 100 sentence translations. \n", " Human judgements were assembled using a group of 50 native people who were well-versed in both English and Tamil languages. \n", " A collection of samples of about 100 sentences were taken from the test set results for comparison. \n", " The human judgements were assembled from a collection of samples of about 100 sentences. \n", " The human judgements were assembled using a collection of samples of about 100 sentences. ", " They were assembled on a 5-point scale. \n", " The human judgements were assembled using a group of 50 native people who were well-versed in both English and Tamil languages. \n", " A group of 50 native speakers acted as annotators for the evaluation, ", " The human evaluation Kappa co-efficient results were calculated with respect to: the intra-annotator ranking of the RNNMorph model was at 0.573, higher that the 0.410 of the RNNSearch+Word2Vec model., ", " Human judgements were assembled using a collection of 100 sentences from the test set results for comparison, taken from 50 native people who were well-versed in both English and Tamil languages. \n", " By a group of 50 native people who were well-versed in both English and Tamil languages, ", " By a group of 50 native people who were well-versed in both English and Tamil languages. \n", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation, and a collection of samples of about 100 sentences were taken from the test set results for comparison. \n", " They were assembled based on 50 native speakers' evaluation of adequacy, fluency and relative ranking of 100 test sentences. \n", " Human judgements were assembled using an intra-annotator Kappa coefficient method, ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation, ", " A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. The human evaluation metrics such as adequacy, precision and ranking values were estimated in Table TABREF30., ", " Human judgements were assembled from a collection of 100 sentences randomly selected from the test set results for comparison. The samples were taken from the test set results for comparison and a randomized selection was made to ensure the objectivity of evaluation. This was done by a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators for the evaluation. ", " By a group of 50 native people who were well-versed in both English and Tamil languages acting as annotators for the evaluation. \n", " They were assembled through a randomised selection of translation results from a test set. \n", " The human judgements were assembled based on adequacy and fluency ratings. \n", " They were estimated using a 5-point scale. ", " Unanswerable, ", " They were assembled by 50 native speakers who rated the adequacy and fluency of the translations. \n"], "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": [" yes, ", " yes, ", " No, unanswerable. \n", " Yes,  ", " yes, ", " Yes, \n", " unanswerable, ", " yes, ", " yes, ", " Yes, \n", " unanswerable, ", " unanswerable, ", " unanswerable \n", " No, unanswerable, ", " yes, ", " Yes, according to the article, the text encoding sub-networks were empirically evaluated on the response retrieval task using Reddit, OpenSubtitles, and AmazonQA conversational test data. However, it is not mentioned that PolyResponse was evaluated against some baseline.  It can only be inferred that the performance of PolyResponse's text encoding sub-networks was strong and robust because of its architecture.  \n\n", " Yes, ", " unanswerable, ", " unanswerable, ", " Yes, ", " Yes, ", " yes, ", " unanswerable, ", " No, the article does not explicitly state that PolyReponse was evaluated against some baseline. \n\n", " unanswerable, \n", " yes, ", " No, unanswerable, ", " No, the evaluation of PolyResponse was not mentioned in the article. \n", " yes, ", " no, ", " no, ", " No, unanswerable, ", " No, \n", " Yes, the model shows strong and robust performance in empirical evaluations on the response retrieval task using Reddit, OpenSubtitles, and AmazonQA conversational test data, see BIBREF12 for further details.\n", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, \n", " unanswerable, \n", " unanswerable, ", " Yes, ", " no, \n", " unanswerable, ", " unanswerable, ", " yes, ", " unanswerable \n", " Yes, ", " yes, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " yes, \n", " Unanswerable, ", " unanswerable, ", " no, ", " unanswerable, ", " yes, ", " No, unanswerable, ", " yes, \n", " Yes, \n", " yes, ", " Yes, ", " yes, ", " yes, \n", " Yes, ", " no, \n", " yes, \n", " yes, ", " yes, ", " yes, ", " unanswerable, ", " yes, ", " Yes, \n", " yes, \n", " unanswerable \n", " unanswerable, ", " yes, ", " no, ", " yes, ", " yes, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " yes, \n", " unanswerable, ", " Yes, ", " yes, \n", " unanswerable, ", " yes, ", " Yes, \n", " unanswerable, ", " yes, \n", " no, ", " yes, it was evaluated against a state-of-the-art architecture based on transformers. \n\n", " unanswerable, \n", " unanswerable, \n", " unanswerable, \n"], "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": [" Multilingual pronunciation corpus and Wiktionary. \n", " Multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. ", " The multilingual pronunciation corpus collected by deri2016grapheme, and the Carnegie Mellon Pronouncing Dictionary, and the corpus provided by Wiktionary. \n", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus, and the Wiktionary pronunciation data. \n", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, and the wiktionary pronunciation data. \n", " They used multilingual pronunciation corpus and Carnegie Mellon Pronouncing Dictionary. \n", " the multilingual pronunciation corpus and the Carnegie Mellon Pronouncing Dictionary. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. ", " The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, and the raw IPA transcriptions extracted from Wiktionary. ", " The multilingual pronunciation corpus collected by deri2016grapheme, and the Carnegie Mellon Pronouncing Dictionary is also used, but it is monolingual and unsuitable for the multilingual task. \n", " the multilingual pronunciation corpus and the Carnegie Mellon Pronouncing Dictionary, ", " Wiktionary pronunciation corpus. \n", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, and the Wiktionary. \n", " [Your concise final answer here], ", " multilingual pronunciation corpus collected by deri2016grapheme, ", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " multilingual pronunciation corpus collected by deri2016grapheme for all experiments, and Carnegie Mellon Pronouncing Dictionary for a monolingual English resource. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. ", " the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme, and corpus collected by deri2016grapheme. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " multilingual pronunciation corpus collected by deri2016grapheme, the Carnegie Mellon Pronouncing Dictionary. \n", " Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. ", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " The multilingual pronunciation corpus collected by deri2016grapheme for all experiments, and the Carnegie Mellon Pronouncing Dictionary. \n", " multilingual pronunciation corpus collected by deri2016grapheme, the Carnegie Mellon Pronouncing Dictionary, and the multilingual corpus collected by ostling2017continuous. ", " The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. ", " the multilingual pronunciation corpus collected by deri2016grapheme, and the Carnegie Mellon Pronouncing Dictionary. \n", " Carnegie Mellon Pronouncing Dictionary and multilingual pronunciation corpus collected by deri2016grapheme, \n", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, and the corpus statistics.  ", " wiktionary and the multilingual pronunciation corpus collected by deri2016grapheme, ", " Carnegie Mellon Pronouncing Dictionary, multilingual pronunciation corpus, Wiktionary, Phoible, and URIEL. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. \n", " Carnegie Mellon Pronouncing Dictionary, multilingual pronunciation corpus collected by deri2016grapheme, and the raw IPA transcriptions extracted from Wiktionary. \n", " The multilingual pronunciation corpus collected by deri2016grapheme for all experiments, and the Carnegie Mellon Pronouncing Dictionary. ", " The multilingual pronunciation corpus collected by deri2016grapheme, and the Carnegie Mellon Pronouncing Dictionary BIBREF12. \n", " Carnegie Mellon Pronouncing Dictionary, multilingual pronunciation corpus, Wiktionary, Phoible, and URIEL. \n", " Wiktionary pronunciation data, the Carnegie Mellon Pronouncing Dictionary, and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " They used the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary., ", " The multilingual pronunciation corpus collected by deri2016grapheme, Carnegie Mellon Pronouncing Dictionary. \n", " Wiktionary pronunciation corpus collected by deri2016grapheme and Carnegie Mellon Pronouncing Dictionary BIBREF12. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. ", " The datasets they used were the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. ", " the multilingual pronunciation corpus collected by deri2016grapheme, and the Carnegie Mellon Pronouncing Dictionary. \n", " They used multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary, \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. ", " Wiktionary pronunciation data and Carnegie Mellon Pronouncing Dictionary. \n", " The Carnegie Mellon Pronouncing Dictionary, and the multilingual pronunciation corpus collected by deri2016grapheme, the data is extracted from Wiktionary. ", " The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. ", " Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, the Wiktionary data, and the cleaned transcriptions extracted from Wiktionary. ", " the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " Wiktionary, Carnegie Mellon Pronouncing Dictionary, and Phoible. \n", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n\n", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " multilingual pronunciation corpus collected by deri2016grapheme. ", " They used the Carnegie Mellon Pronouncing Dictionary and a multilingual pronunciation corpus collected by deri2016grapheme. ", " Multilingual pronunciation corpus and Carnegie Mellon Pronouncing Dictionary. ", " They used the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " multilingual pronunciation corpus collected by deri2016grapheme, the Carnegie Mellon Pronouncing Dictionary BIBREF12, and Wiktionary. \n", " The Carnegie Mellon Pronouncing Dictionary, and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " the Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme, ", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme, which consists of spelling–pronunciation pairs extracted from Wiktionary. ", " The Carnegie Mellon Pronouncing Dictionary and a multilingual pronunciation corpus collected by deri2016grapheme. ", " multilingual pronunciation corpus and Wiktionary. \n", " the Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme. ", " The multilingual pronunciation corpus collected by deri2016grapheme, which consists of spelling–pronunciation pairs extracted from Wiktionary. \n", " The multilingual pronunciation corpus collected by deri2016grapheme, and the Carnegie Mellon Pronouncing Dictionary. ", " Wiktionary pronunciation corpus and Carnegie Mellon Pronouncing Dictionary. ", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme for Wiktionary, and the Phoible database. ", " Carnegie Mellon Pronouncing Dictionary, and multilingual pronunciation corpus collected by deri2016grapheme. ", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme, and Wiktionary. ", " [Your concise final answer here], ", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary \n", " Carnegie Mellon Pronouncing Dictionary, multilingual pronunciation corpus, and Phoible. ", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary is a standard dataset for g2p. ", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. ", " The multilingual pronunciation corpus collected by deri2016grapheme. ", " Carnegie Mellon Pronouncing Dictionary, multilingual pronunciation corpus collected by deri2016grapheme. \n", " multilingual pronunciation corpus and Carnegie Mellon Pronouncing Dictionary (CMPD), ", " The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary, ", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. ", " The multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme, ", " The multilingual pronunciation corpus collected by deri2016grapheme for all experiments. \n", " [Your concise final answer here], ", " The multilingual pronunciation corpus collected by deri2016grapheme, the Carnegie Mellon Pronouncing Dictionary, and the Wiktionary. \n", " The multilingual pronunciation corpus collected by deri2016grapheme, the Carnegie Mellon Pronouncing Dictionary. \n", " Carnegie Mellon Pronouncing Dictionary and multilingual pronunciation corpus collected by deri2016grapheme. ", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme, which consists of spelling–pronunciation pairs extracted from Wiktionary. \n", " The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme. \n", " The Carnegie Mellon Pronouncing Dictionary, the multilingual pronunciation corpus collected by deri2016grapheme. \n", " Multilingual pronunciation corpus collected by deri2016grapheme. ", " multilingual pronunciation corpus collected by deri2016grapheme, Carnegie Mellon Pronouncing Dictionary, Wiktionary data. ", " multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary BIBREF12. \n", " the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary BIBREF12, ", " Carnegie Mellon Pronouncing Dictionary, multilingual pronunciation corpus collected by deri2016grapheme, and data from Wiktionary. \n"], "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": [" Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling.\n", " Text classification, language modeling, and other NLP tasks. \n", " Text classification. \n", " Text classification, ", " Text classification and language modeling. \n", " Text classification, POS tagging, language modeling, and document recommendation. \n", " text classification and language modeling, ", " They test their method on hashtag prediction for social media posts. ", " Named Entity Recognition, POS tagging, text classification, language modeling. ", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " text classification, POS tagging, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " They test their method on text classification. \n", " Named Entity Recognition, POS tagging, text classification, language modeling, document recommendation, and tracking infectious diseases. \n\n", " They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Text classification, language modeling, Named Entity Recognition, POS tagging. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Text classification and language modeling. ", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " None in particular, \n", " Text classification and language modeling. \n", " Named Entity Recognition, POS tagging, text classification. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " They test their method on Named Entity Recognition, POS tagging, text classification, language modeling. \n", " text classification, \n", " Text classification, named entity recognition, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling, document recommendation. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. ", " Named Entity Recognition, POS tagging, text classification. \n", " Named Entity Recognition, POS tagging, text classification. ", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Text classification, language modeling. ", " [Your final answer here], ", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling, in addition to hashtag prediction. \n\n", " Named Entity Recognition, POS tagging, text classification, language modeling. ", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " text classification and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification. \n", " Text classification. \n", " Named Entity Recognition, POS tagging, text classification. \n", " Named Entity Recognition, POS tagging, text classification, language modeling, generating vector-space representations of tweets. \n", " They test their method on named entity recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " unanswerable \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification. \n", " Text classification. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. ", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. ", " Text classification, Named Entity Recognition, POS tagging, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " text classification, POS tagging, and language modeling, ", " Named Entity Recognition, POS tagging, text classification. \n", " Text classification. \n", " text classification, language modeling.  \n", " Text classification, \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " They compare their method's performance on tasks such as Named Entity Recognition, POS tagging, text classification, and language modeling. \n\n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Text classification. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, and language modeling. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Text classification, POS tagging, and Named Entity Recognition. \n", " Named Entity Recognition, POS tagging, text classification. \n", " Text classification, POS tagging, Named Entity Recognition. \n", " Named Entity Recognition, POS tagging, text classification. ", " Text classification, POS tagging, Named Entity Recognition, language modeling, and document recommendation. \n", " Text classification, Named Entity Recognition, POS tagging, language modeling.\n", " text classification. \n", " text classification, named entity recognition, POS tagging. \n", " Named Entity Recognition, POS tagging, text classification, language modeling. \n", " Text classification, POS tagging, Named Entity Recognition, language modeling. \n", " Text classification. \n", " text classification. \n"], "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": [" English, Spanish, and Finnish, and potentially other languages. \n", " English, Spanish, Finnish, and multiple languages in the test sets. \n", " English, Spanish, Finnish, and multiple languages including 11 languages in TyDi QA dataset. \n", " English, Spanish, Finnish, and 14 target languages. \n", " English, Spanish, and Finnish, ", " English, Spanish, and Finnish, ", " English, Spanish, Finnish, and other 12 languages, ", " English, Spanish, Finnish, and 11 others. \n", " English, Spanish, Finnish, and 15 other languages, ", " English, Spanish, Finnish, and others, ", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and others. \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and possibly others. \n\n", " English, Spanish, Finnish, and other 11 languages. \n", " English, Spanish, Finnish, and other 11 languages. \n", " English, Spanish, Finnish, 14 other languages, 11 languages, \n", " English, Spanish, Finnish, and other languages, ", " English, Spanish, Finnish, and multiple languages. \n", " English, Spanish, Finnish, and the rest of languages. \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and 11 other languages. \n\n", " English, Spanish, Finnish, and others, ", " English, Spanish, Finnish, and others. \n", " English, Spanish, Finnish, English and Finnish, or other 11 languages, Spanish, Finnish, 15 other languages, and 6 other languages. \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and others, ", " English, Spanish, Finnish, and 14 other languages, ", " Spanish and Finnish, ", " English, Spanish, Finnish, and 15 other languages. \n", " English, Spanish, Finnish, French, German, Italian, Dutch, Russian, Arabic, Hindi, Chinese, Japanese, Korean, Vietnamese, Turkish. ", " Spanish, English, and Finnish, among others. ", " English, Spanish, and Finnish, ", " English, Spanish, Finnish, and 14 other languages, ", " English, Spanish, and Finnish. \n", " English, Spanish, Finnish, and multiple other languages. \n", " English, Spanish, Finnish, and the rest of languages, ", " Spanish, Finnish, English, and 11 other languages, ", " English, Spanish, Finnish, and multiple other languages. \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and the rest of languages. \n", " Spanish and Finnish. \n", " English, Spanish, Finnish, and 14 other languages, ", " English, Spanish, Finnish, and multiple other languages, ", " English, Spanish, Finnish, and 11 other languages. \n", " Spanish, Finnish, and English, ", " English, Spanish, and Finnish, ", " English, Spanish, Finnish, and 15 other languages, ", " English, Spanish, Finnish, and 10 other languages, ", " English, Spanish, Finnish, and the other 15 languages. \n\n", " Spanish, Finnish, English, and others (e.g. French, German), ", " Spanish, Finnish, and English, ", " English, Spanish, Finnish, among others, \n", " English, Spanish, Finnish, German, French, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Vietnamese, Thai, Arabic., ", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and other 11 languages. \n", " Spanish, Finnish, English. \n", " English, Spanish, Finnish, and 11 other languages. \n", " Spanish and Finnish. \n", " English, Spanish, and Finnish. \n", " English, Spanish, Finnish, and 14 other languages. \n", " English, Spanish, Finnish, and others, \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and 11 other languages. \n", " English, Spanish, and Finnish, ", " English, Spanish, Finnish, and multiple languages, ", " English, Spanish, Finnish, and others, \n", " English, Spanish, Finnish, and 11 other languages. \n", " English, Spanish, Finnish. ", " Spanish and Finnish, ", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and others, \n", " English, Spanish, Finnish, and others. \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and 11 other languages, ", " English, Spanish, Finnish, and the rest of the languages, ", " Spanish and Finnish, \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and the rest of 15 languages. \n", " English, Spanish, Finnish, 15 other languages. \n", " English, Spanish, Finnish, and others, ", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and 11 other languages. \n", " English, Spanish, Finnish, and other 11 languages. \n", " English, Spanish, Finnish, and 15 other languages. \n", " English, Spanish, Finnish, and 14 other target languages, ", " English, Spanish, Finnish, English, Spanish, Finnish, 10 other languages, 6 other languages, 15 languages, English, English, English, 14 target languages, 1 non-English language, English, 6 other languages. \n", " English, Spanish, Finnish, and the rest of 15 languages in XNLI, \n", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and the rest of 11 other languages, ", " English, Spanish, Finnish. \n", " English, Spanish, Finnish, and 14 other languages, ", " English, Spanish, Finnish, and others. \n", " Spanish and Finnish, ", " Spanish and Finnish, \n", " English, Spanish, Finnish, and 10 other languages. ", " English, Spanish, Finnish, and other languages, ", " English, Spanish, Finnish, and 10 other languages. \n", " English, Spanish, Finnish, 14 other languages, \n\n"], "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": [" They use psycholinguistic and semantic word categories, ", " They measure the usage of words related to people's core values. \n", " By using maps for psycholinguistic categories and word categories that reflect a certain psycholinguistic or semantic property. \n", " They measure the usage of words related to people's core values, ", " They use psycholinguistic and semantic maps, ", " By using the distribution of words related to psycholinguistic and semantic properties, \n", " By utilizing the distribution of individual words in psycholinguistic or semantic categories, ", " They use LIWC categories, ", " They obtain psychological dimensions of people by analyzing the distribution of psycholinguistic and semantic word classes in blog posts. \n", " By analyzing LIWC categories, ", " They utilize LIWC. \n", " They use LIWC, Roget, and the Meaning Extraction Method (MEM) to extract psychological dimensions from the blog data. \n", " By measuring the usage of words related to core values. \n", " By analyzing word categories that reflect a certain psycholinguistic or semantic property. \n", " They use the distribution of the individual words in a category, ", " By creating maps of the U.S. that reflect the distribution of psycholinguistic and semantic word classes. \n", " They use psycholinguistic and semantic word categories. \n", " By using LIWC, Meaning Extraction Method (MEM), and Roget, they compile distributions of psycholinguistic and semantic word classes. \n", " Using word categories that reflect psycholinguistic properties. ", " Using LIWC and semantic maps. \n", " They measure the usage of words related to people's core values and psycholinguistic or semantic categories. \n", " By using blog posts to create maps that reflect psycholinguistic properties. \n", " By analyzing language use and word categories. \n", " By analyzing distributions of psycholinguistic and semantic word classes, \n", " They compile distributions of individual words in a psycholinguistic or semantic category. ", " By analyzing the usage of words related to people's core values. ", " By analyzing the geolocated information available on blogs, particularly through linguistic and psycholinguistic properties. \n", " By generating maps that reflect the distribution of words related to people's core values, ", " By analyzing the LIWC categories, ", " By examining linguistic patterns, ", " By analyzing word distributions of psycholinguistic and semantic categories. \n", " By compiling distributions of individual words in categories, ", " They use word categories such as Money, Positive Feelings, Religion and Hard Work to reflect psycholinguistic and semantic properties, ", " By measuring the usage of words related to people's core values. \n", " They use LIWC. \n", " They measure the usage of words related to people's core values.  \n", " By analyzing blog posts. ", " They utilize the distribution of individual words in psycholinguistic categories, ", " They utilize a dataset of blogs that can provide valuable insights into linguistic geography, regional personality, language analysis, and psychology. ", " By analyzing blog posts through word categories that reflect certain psycholinguistic or semantic properties. \n", " They measure the usage of words related to people's core values. \n", " By analyzing the distribution of psycholinguistic and semantic word categories, ", " By analyzing word categories, ", " They utilize word categories that reflect certain psycholinguistic or semantic properties. \n", " They utilize psycholinguistic and semantic word classes. ", " By analyzing the psycholinguistic and semantic word categories. \n", " By analyzing word categories that reflect certain psycholinguistic or semantic properties. \n", " By analyzing the distribution of the individual words in LIWC categories, ", " By analyzing blog posts with LIWC categories and values, ", " By analyzing the words used in blog posts. ", " They measure the usage of words related to people's core values. \n", " By analyzing the blog posts, ", " By analyzing the usage of words, ", " By analyzing blog posts to measure usage of words related to people's core values, ", " \n", " By analyzing language use. ", " By analyzing their linguistic features. ", " They obtain psychological dimensions by analyzing the LIWC categories, such as money, positive feelings, religion and hard work, and so forth, on a geographical basis. \n", " By using lexical resources such as Roget or Linguistic Inquiry and Word Count. ", " They measure the usage of words related to people's core values as reported by Boyd et al. \n", " By examining distributions of psycholinguistic and semantic word categories. ", " By creating psycholinguistic and semantic maps. ", " By analyzing blog posts, ", " They obtain psycholinguistic dimensions using word categories, ", " They use LIWC. ", " By analyzing distributions of psycholinguistic and semantic word categories. \n", " By analyzing words used in blog posts, ", " By analyzing LIWC categories such as Positive Feelings and Money. \n", " By analyzing word usage in blogs. ", " They measure the usage of words related to people's core values, and the usage of psycholinguistic word categories such as Positive Feelings and Money. \n\n", " By analyzing the word categories related to psycholinguistic or semantic properties, ", " They measure the usage of words related to people's core values as reported by Boyd et al. \n", " By analyzing the distributions of psycholinguistic and semantic word classes, ", " By creating maps that reflect a certain psycholinguistic or semantic property, ", " By analyzing the distribution of individual words and categories that reflect psycholinguistic and semantic properties, ", " They utilize LIWC categories. ", " By analyzing the words bloggers write about, \n", " By analyzing psycholinguistic and semantic word classes, ", " They use word categories that reflect psycholinguistic or semantic properties. \n", " They measure the usage of words related to people's core values and psycholinguistic and semantic properties, ", " They use linguistic maps, ", " \n", ", ", " By creating maps for word categories that reflect a certain psycholinguistic or semantic property. \n", " They use lexical resources, ", " They use LIWC and MEM to measure the usage of words related to people's core values, ", " By utilizing psycholinguistic and semantic word classes, ", " They obtain these dimensions by analyzing LIWC categories. \n", " By examining the usage of words related to people's core values, ", " By analyzing language use from blog posts, ", " By compiling distributions for entire word categories from individual words. \n", " By analyzing psycholinguistic and semantic properties of the bloggers' language use, ", " By analyzing the language used in blogs, ", " They obtain psychological dimensions of people through analyzing words, ", " They use LIWC to analyze word categories that reflect a certain psycholinguistic or semantic property. \n", " They measure the usage of words related to people's core values, \n", " By analyzing LIWC categories, ", " By analyzing the distribution of words such as money and positive feelings, which are related to people's psycholinguistic and semantic properties. \n", " They measure the usage of words related to people's core values and psycholinguistic categories. ", " By compiling distributions for word categories that reflect psycholinguistic or semantic properties, "], "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": [" yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, they use pretrained embeddings. ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, they use 300 dimensional Glove embeddings. ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, they use pretrained Glove embeddings. \n", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, they use GloVe embeddings, ", " Yes, ", " yes, ", " yes, ", " Yes, they use pretrained embeddings. \n", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, they use pretrained embeddings. \n", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, they use 300 dimensional Glove embeddings. ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": [" BERT, RoBERTa. \n", " unanswerable. \n", " \n", " BERT, RoBERTa, \n", " unanswerable, ", " RoBERTa, ", " baselines. ", " BERT. \n", " BERT, RoBERTa, and XLNet were the baselines used in the experiment for training on a single dataset and jointly on multiple datasets. ", " Maximum Entropy Classifiers, ", " unanswerable, ", " \n", " BERT. \n", " BERT, RoBERTa. \n", " \n", " \n", " The baselines were state-of-the-art results on negation scope resolution on BF, BA and SFU datasets. \n", " BERT, XLNet, and RoBERTa. \n", " [Your concise final answer here], ", " The models used were the baselines. \n", " unanswerable, ", " BERT, ", " Maximum Entropy Classifiers, ", " unanswerable \n", " bert-base-uncased for BERT, roberta-bases3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin for RoBERTa, and xlnet-base-caseds3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin for XLNet. \n", " Maximum Entropy Classifiers, Support Vector Machines, and simple rule-based systems. \n", " unanswerable \n", " BERT \n", "  ", " unanswerable, ", " unanswerable, ", " XLNet consistently outperformed BERT and RoBERTa. \n", " The baselines were the existing systems for speculation detection and scope resolution. \n", " BERT, ", " Maximum Entropy Classifiers, Support Vector Machines, and rule-based systems. \n", " The baselines were results from existing systems on the task of speculation detection and scope resolution.\n", " unanswerable \n", " BERT, RoBERTa. \n", " Maximum Entropy Classifiers and Support Vector Machines, ", "  Rule-based systems, Maximum Entropy Classifiers, Support Vector Machines, Recursive Neural Networks, Convolutional Neural Networks, and Bidirectional Encoder Representation from Transformers. \n", " unanswerable, ", " RoBERTa. \n", " \n", " Yes, the baselines were BERT and RoBERTa. \n", " BERT. \n", " BERT. \n", " Yes. \n", " The baselines were simple rule-based systems. \n", " unanswerable \n", " unanswerable, ", " Yes, the baselines are RoBERTa and BERT, ", " unanswerable, ", " BERT, ", " RoBERTa. \n", " BERT \n", " No baselines were mentioned. \n", " Not specified. \n", " The baselines were reported results for BERT. \n", " bert-base-uncased, roberta-base, xlnet-base-cased \n", " The BERT model. \n", " The baselines. \n", " unanswerable, ", " Maximum Entropy Classifiers, ", " BERT. ", " unanswerable \n", " XLNet and RoBERTa. \n", " The previous models used were Maximum Entropy Classifiers, Support Vector Machines, Recursive Neural Networks, and Convolutional Neural Networks. \n", " RoBERTa, ", " unanswerable \n", " Yes, the baselines are other models including BERT, RoBERTa. \n", "  Maximum Entropy Classifiers, Support Vector Machines, rule-based systems. ", " unanswerable \n", " Unanswerable \n", " BERT, ", " RoBERTa, ", " BERT \n", " RoBERTa. \n", ", ", " unanswerable\n", " RoBERTa, BERT. \n", " BERT \n", " No baselines are provided in the article, ", " The Machine Learning techniques used in the past were Maximum Entropy Classifiers and Support Vector Machines. ", " Unanswerable, ", " RoBERTa and BERT \n", " The baselines were RoBERTa. \n", " The Machine Learning techniques used varied from Maximum Entropy Classifiers to Support Vector Machines. \n", " [Your concise final answer here], ", " Maximum Entropy Classifiers, ", " BERT, RoBERTa, XLNet. \n", " BERT, RoBERTa and XLNet. \n", " XLNet and BERT. \n", " \n", " unanswerable, ", " XLNet and BERT were used as baselines for comparison. \n", " unanswerable \n", " BERT, RoBERTa, XLNet. \n", " BERT. \n", " BERT, XLNet, and RoBERTa were used as the baselines. \n", " BERT. \n"], "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": [" The quality of the data is empirically evaluated using five different methods, including, BLEU, manual inspection, perplexity, ratio of English characters, and VizSeq similarity scores. ", " The quality of the data is empirically evaluated by a series of sanity checks including, but not limited to, BLEU scores, perplexity, and similarity scores, ", " The quality of the data is empirically evaluated through various sanity checks, ", " They are evaluated using various sanity checks such as BLEU scores, perplexity, and LASER cross-lingual sentence embeddings, ", " The quality of the data is empirically evaluated through several sanity checks, ", " The data quality is empirically evaluated by a variety of means, including: (1) computing sentence-level BLEU between human and automatic translations; (2) manually inspecting examples where the source transcript was identical to the translation; (3) measuring the perplexity of the translations using a language model trained on clean monolingual data; (4) computing the ratio of English characters in the translations; and (5) using VizSeq to calculate similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. Additionally, speaker demographics in terms of sample distributions with respect to speaker counts, accent counts,", " The quality of the data is empirically evaluated using a set of sanity checks including sentence-level BLEU, manual inspection, perplexity, ratio of English characters, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings.\n\n", " The quality of the data is empirically evaluated using various sanity checks., ", " The quality of the data is empirically evaluated through various sanity checks., ", " Through various sanity checks including sentence-level BLEU with automatic translation systems, manual inspection of examples where the source transcript is identical to the translation, perplexity using a language model trained on monolingual data, ratio of English characters in the translations, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. \n\n", " The data quality was empirically evaluated using sentence-level BLEU scores, perplexity, and similarity scores, among other checks, to identify and fix poor translations., ", " The data quality is evaluated empirically through various sanity checks, including BLEU score comparison between human translations and automatic translations, manual inspection of translations, perplexity calculation, character ratio checking, and LASER cross-lingual sentence embeddings comparison, to ensure the translations are of high quality. ", " The quality of the data is empirically evaluated through various sanity checks, including BLEU scores, perplexity, similarity scores between transcripts and translations, and manual inspection of low-scoring samples., ", " Translations with a score that was too low were manually inspected and sent back to the translators when needed., ", " The data quality is empirically evaluated using various methods including BLEU score, sentence-level BLEU with the NLTK implementation, perplexity of the translations using a language model, ratio of English characters in the translations, and VizSeq to calculate similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings.  ", " The quality of the data is empirically evaluated using various sanity checks. \n", " The quality of the data is empirically evaluated using various sanity checks, including BLEU score, perplexity, ratio of English characters, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. \n\n", ", ", " They applied various sanity checks, ", " The quality of the data is empirically evaluated through various sanity checks. \n", " 5 sanity checks were performed on translations in the CoVoST corpus, ", " The quality of the data is empirically evaluated using five different sanity checks.  ", " ", " ", " ", " The quality of the data is empirically evaluated through multiple sanity checks, including BLEU score computation, manual inspection of identical source transcripts and translations, perplexity measurement using a language model, and VizSeq calculation of similarity scores between transcripts and translations. ", " The quality of the data is empirically evaluated using various sanity checks including, BLEU scores between human translations and the automatic translations produced by a state-of-the-art system, manual inspection of examples where the source transcript was identical to the translation, perplexity of the translations, ratio of English characters in the translations, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings. \n\n\n", "  Various sanity checks are applied, including sentence-level BLEU, perplexity, and similarity scores., ", " Multiple methods are applied to control the quality of the data, ", " The quality of the data is empirically evaluated by applying sanity checks to the translations, ", " Quality of the data is empirically evaluated by using various sanity checks, including BLEU scores, perplexity, English character ratio, LASER cross-lingual sentence embeddings, and MD5 file hashing. ", " The data quality is evaluated by using various sanity checks, including manual inspection, BLEU scores, perplexity, character ratio, and cross-lingual sentence embeddings.  ", " The quality of the data is empirically evaluated by using a combination of several checks. First, sentence-level BLEU is computed between human translations and automatic translations produced by a state-of-the-art system. Second, manual inspections are performed on translations with low BLEU scores, identical source transcripts and translations, and high perplexity. Third, similarity scores between transcripts and translations are calculated using LASER cross-lingual sentence embeddings. Fourth, English character ratios are computed and inspected. Lastly, speaker demographics and overlap are sanity-checked for all datasets., ", ", ", " The quality of the data is empirically evaluated using a combination of different sanity checks, including sentence-level BLEU, perplexity, English character ratio, and similarity scores based on LASER cross-lingual sentence embeddings, ", ", ", " ", " The quality of the data is empirically evaluated by using several sanity checks. \n", " The quality of the data is empirically evaluated by applying various sanity checks, including calculating sentence-level BLEU, measuring perplexity, and using VizSeq to calculate similarity scores., ", " The quality of the data is evaluated using BLEU score between human translations and automatic translations produced by a state-of-the-art system, manually inspected examples, perplexity of the translations using a language model, ratio of English characters in the translations, and VizSeq to calculate similarity scores between transcripts and translations. \n", " Various sanity checks were applied to the translations, ", " Translations with a score that was too low were manually inspected and sent back to the translators when needed, and similar sanity checks were performed on the overlaps of train, development and test sets in terms of transcripts and voice clips., ", " The quality of the translations is empirically evaluated using various sanity checks, ", " The quality of the data is empirically evaluated through a series of sanity checks, including calculating the BLEU score, perplexity, and similarity scores based on LASER cross-lingual sentence embeddings, as well as manual inspection, to ensure the accuracy and consistency of the translations. ", " in ", ", ", " The quality of the data is empirically evaluated through various sanity checks, including BLEU scores, manual inspection, perplexity, and similarity scores., ", " [Your final answer here], ", " The quality of the data is empirically evaluated through several sanity checks. \n", "  It is done through various sanity checks, ", " The quality of the data is empirically evaluated through various sanity checks and methods, including BLEU score calculation, perplexity measurement, and LASER cross-lingual sentence embeddings, in order to identify and correct any poor-quality translations, ", " The quality of the data is empirically evaluated by checking for BLEU scores, perplexity of the translations, and similarity scores between transcripts and translations.\n", " Various sanity checks are applied to the translations including BLEU score computation, manual inspection of identical source and translation pairs, perplexity measurement using a language model, and similarity score calculation using LASER cross-lingual sentence embeddings.\n", " The quality of the data is empirically evaluated through various methods, including BLEU scores, sentence-level BLEU, perplexity of translations, and similarity scores between transcripts and translations, as well as manual inspection of translations and transcripts.  ", " Quality of the data is empirically evaluated by applying various sanity checks, ", " The quality of the data is empirically evaluated by using various sanity checks and quality metrics, ", " The quality of the data is empirically evaluated by computing various sanity checks., ", " The quality of the data is empirically evaluated through multiple sanity checks. \n", " The quality of the data is empirically evaluated by applying various sanity checks to the translations., ", " The quality of the data is empirically evaluated through a series of sanity checks, including sentence-level BLEU computation between human and automatic translations, manual inspection of identical transcript-translations pairs, perplexity measurement, English character ratio check, and LASER cross-lingual sentence embeddings similarity score calculation., ", " The quality of the data is empirically evaluated using BLEU, perplexity, and VizSeq, and a series of sanity checks including manual inspections and translator re-assignment as needed., ", " Various sanity checks, including BLEU scores and language model perplexity, are used to evaluate the quality of the data, ", " They use a combination of 5 methods, ", " Quality of the data is empirically evaluated using various methods, including BLEU score, perplexity, and similarity scores based on LASER cross-lingual sentence embeddings, as well as manual inspection of samples. ", " They apply various sanity checks, including BLEU, perplexity, character ratio, and similarity scores between transcripts and translations. \n", " The quality of the data is empirically evaluated through various sanity checks. \n", " The data quality is empirically evaluated by various sanity checks such as BLEU scores, perplexity, similarity scores and manual inspection. ", " through various sanity checks, ", " It is evaluated through a combination of BLEU, perplexity, LASER cross-lingual sentence embeddings, and manual inspection, as well as ensuring that the overlaps of train, development, and test sets are disjoint.  ", " The quality of the data is empirically evaluated by various sanity checks, including BLEU, perplexity, and manual inspections.  ", " It is empirically evaluated using various sanity checks, including BLEU score calculation, manual inspection of translations, perplexity measurement, and similarity score calculation. \n\n", " Empirical evaluations of the data quality include the use of BLEU score between the human translations and the automatic translations, the manual inspection of examples where the source transcript is identical to the translation, the measurement of perplexity of the translations using a language model, the computation of the ratio of English characters in the translations, and the use of VizSeq to calculate similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings.  ", " The quality of the data is empirically evaluated using multiple checks., ", " Empirically evaluated through various sanity checks, ", " iii) both human and automatic systems are used to evaluate, ", " ", " ", " By various sanity checks, ", " Quality is empirically evaluated by using various sanity checks. \n", " They applied various sanity checks to the translations, ", " through various sanity checks, ", " The quality of the data is empirically evaluated with various sanity checks, including BLEU score, perplexity, ratio of English characters, and similarity scores between transcripts and translations. ", " The quality of the data is empirically evaluated through various sanity checks, including sentence-level BLEU scores, manual inspection, perplexity measurement, character ratio inspection, and similarity score calculation between transcripts and translations. \n", " The quality of the data is empirically evaluated using various sanity checks including BLEU scores, perplexity, ratio of English characters, and similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings, ", " The quality of the data is empirically evaluated through BLEU scores, perplexity, ratio of English characters in the translations, and LASER cross-lingual sentence embeddings, ", " The quality of the data is empirically evaluated through a series of sanity checks, including BLEU score, perplexity, and similarity scores.\n\n", " The quality of the data is empirically evaluated with a series of sanity checks and quality controls. ", " The quality of the data is empirically evaluated using various sanity checks., ", "  The quality of the data is empirically evaluated through several sanity checks, including, BLEU scores, perplexity, overlap ratio of English characters, and similarity scores based on LASER cross-lingual sentence embeddings, ", " The quality of the data is empirically evaluated by the following steps: (1) compute sentence-level BLEU score with human translations and automatic translations; (2) manually inspect examples where the source transcript is identical to the translation; (3) measure the perplexity of translations using a language model; (4) manually inspect examples where the translation has a high perplexity; (5) compute the ratio of English characters in the translations; and (6) manually inspect examples with a low ratio; (7) calculate similarity scores between transcripts", ", ", "  The quality of the data is evaluated through various sanity checks, ", " Several methods are used including, checking BLEU score, perplexity, ratios of English characters, and similarity scores, ", " The quality of the data is empirically evaluated through various sanity checks including sentence-level BLEU scores, perplexity of the translations, ratio of English characters in the translations, and similarity scores between transcripts and translations.  ", " (To be filled in), ", " No, the BLEU scores for German in ASR task are not very high in the machine translation task, but in the ASR task, German has a good BLEU score of 29.8/25.4, ", " Through multiple sanity checks, ", " Various sanity checks were applied to the translations, ", " \n", " [Your concise final answer here], "], "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": [" Claim, premise, backing, rebuttal, and refutation. ", " claim, premise, backing, rebuttal, and refutation. \n", " Claims, premises, backing, rebuttal, refutation, backing and refutation. \n", " Claim, premise, backing, rebuttal, and refutation, ", " Claims, premises, backing, rebuttal, refutation. \n", " Claims, premises, backing, rebuttal, refutation, and non-argumentative text. ", " Claim, premise, backing, rebuttal, refutation, and none. \n", " Argument components, ", " Claims, premises, backing, rebuttal, refutation, and non-argumentative text. \n", " claim, premise, backing, rebuttal, and refutation. \n", " Argument components in the logos dimension. \n", " claim, premise, backing, rebuttal, refutation, backing. \n", " Claim, premise, backing, rebuttal, and refutation. \n", " Argument components. ", " claim, premise, backing, rebuttal, and refutation. \n", " Claim, premise, backing, rebuttal, and refutation. \n", " Claims, premises, backing, rebuttals, and refutations. \n", " claim, premise, backing, rebuttal, and refutation, ", " Argument components. \n", " Claim, premise, backing, rebuttal, refutation, backing. \n", " Claims, premises, backing, rebuttals, and refutations. \n", " Claim, premise, backing, rebuttal, and refutation, ", " argument components. ", " Claims, premises, backing, rebuttals, and refutations. \n", " Claim, premise, backing, rebuttal, refutation, and non-argumentative text. ", " claim, premise, backing, rebuttal, and refutation. \n", " Claims and argument components in the logos dimension of an argument. \n", " Argument components in the logos dimension of an argument. ", " Claims, premises, backing, rebuttal, refutation, and backing, ", " Claim, premise, backing, rebuttal, and refutation, ", " Argument components, ", " Premises, backing, claim, rebuttal, refutation, and backing. \n", " Claims, premises, backing, rebuttals, and refutations. \n", " Claim, premise, backing, rebuttal, and refutation, ", " claims, premises, backing, rebuttals, and refutations. \n", " Claim, premise, backing, rebuttal, and refutation. ", " Claim, premise, backing, rebuttal, refutation, and none. \n", " Claims, premises, backing, rebuttal, and refutation. ", " Claim, premise, backing, rebuttal, and refutation. ", " Claim, premise, backing, rebuttal, and refutation, ", " Claim, premise, backing, rebuttal, refutation, and none. \n", " Claims, premises, backing, rebuttal, and refutation. \n", " Claim, premise, backing, rebuttal, and refutation, ", " argument components \n", " argument components \n", " Claims, premises, backing, rebuttals, and refutations. \n", " Claim, premise, backing, rebuttal, and refutation. \n", " claim, premise, backing, rebuttal, and refutation. \n", " Claims, premises, backing, rebuttal, refutation, and non-argumentative text. \n", " claims, premises, backing, rebuttal, and refutation. \n", " argument components in the logos dimension. \n", " argument components in the logos dimension. ", " claims, premises, backing, rebuttals, refutations, and non-argumentative text. \n", " Claim, premise, backing, rebuttal, and refutation. \n", " Claim, premise, backing, rebuttal, refutation. \n", " Claims, backing, rebuttals, and refutations. \n", " Claims, premises, backing, rebuttal, refutation, and non-argumentative text. \n", " Claim, premise, backing, rebuttal, and refutation, ", " Claims, premises, backing, rebuttals, refutations, and non-argumentative text. \n", " Claim, premise, backing, rebuttal, and refutation. ", " Argument components, specifically claims, premises, backing, rebuttals, and refutations. \n", " Claims, premises, backing, rebuttals, and refutations. \n", " Claims, premises, backing, rebuttals, refutations, backing, and non-argumentative text. \n", " Claim, premise, backing, rebuttal, and refutation, ", " Claim, premise, backing, rebuttal, and refutation. \n", " Claims and premises. \n", " claim, premise, backing, rebuttal, refutation. \n", " Argument components. \n", " Argument components. \n", " Argument components. ", " argument components, ", " Argument components. ", " Claim, premise, backing, rebuttal, refutation, and backing. \n", " Claim, premise, backing, rebuttal, refutation, and none, ", " Claim, premise, backing, rebuttal, and refutation. \n", " Claim, premise, backing, rebuttal, and refutation. \n", " argument components \n", " Claim, backing, premise, rebuttal, refutation, and none, ", " Argument components, ", " Argument components. ", " Premise, backing, claim, rebuttal, and refutation. ", " Claim, premise, backing, rebuttal, and refutation. \n", " Claim, premise, backing, rebuttal, and refutation. \n", " claims, premises, backing, rebuttals, and refutations. ", " Argument components. ", " Claim, premise, backing, rebuttal, refutation, backing, and non-argumentative text. ", " argument components. ", " Argument components. ", " Claim, premise, backing, rebuttal, refutation, and backing. \n", " Claim, backing, premise, rebuttal, and refutation. ", " Claim, premise, backing, rebuttal, and refutation. \n", " Claims, premises, backing, rebuttal, and refutation. \n", " Claims, premises, backing, rebuttal, refutation, and non-argumentative text. \n", " The argument components, such as premises, backing, claim, rebuttal, and refutation. \n", " Argument components. \n", " claim, premise, backing, rebuttal, refutation, non-argumentative text. \n", " Claim, premise, backing, rebuttal, refutation, and non-argumentative text. ", " Claim, premise, backing, rebuttal, and refutation. ", " Claims, premises, backing, rebuttal, and refutation, ", " Argument components. \n"], "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": [" n-grams of order 1, \n", " n-grams of unspecified length, ", " n-grams of order INLINEFORM7 in INLINEFORM8 and INLINEFORM9, respectively. \n", " n-grams of any length.\n", " n-grams of order 3 or less \n", " n-grams of order 4, \n", " n-grams of order 1 to N \n", " N-grams of unspecified length, ", " n-grams of order 4, \n", " n-grams, ", " n-grams of order 4, ", " Ngrams of any order. \n", " N-grams of order 1, ", " n-grams of order 3 are aligned\n", " 4, ", " n-grams of order INLINEFORM7 (n), \n", " 3 \n", " n-grams of order 5, \n", " n-grams of order 5, ", " n-grams of order 8, ", " n-grams of order 4, ", " Ngrams of order 1 \n", " n-grams of any order, ", " 4-grams, ", " N-grams of order 4. ", " n-grams of any order INLINEFORM7, ", " 4 or 5, \n", " n-grams of order up to 5, \n", " 0, ", " n-grams of order 4, ", " n-grams of order k, \n", " n-grams of order 1 INLINEFORM7 \n", " 3, ", " n-grams of order 4, \n", " n-grams of order 4, \n", " [Your final answer], ", " n-grams of order 3, \n", " N-grams of order n. \n", " n-grams of order INLINEFORM7, ", " n-grams of order 4, \n", " n-grams of any order INLINEFORM7, ", " N-grams of order INLINEFORM7, ", "  n-grams of order INLINEFORM7 \n", " N-grams of any order \n", "  n-grams of order greater than 1 \n", " n-grams of order 4, ", " n-grams of order 1, \n", " Ngrams of order n, ", " n-grams of order 4, ", " n-grams of order inlineFORM7 \n", " 5, ", " n-grams of order INLINEFORM7 \n", " , ", " n-grams of order n, \n", " n-grams of order 1, ", " n-grams of order 3 or higher (no specific value mentioned), ", " n-grams of order 1 \n", " 3, \n", " n-grams of order 8, \n", " n-grams of length 1, n-grams of length 2, n-grams of length 3 \n", " Ngrams of order 4 \n", " n-grams of order INLINEFORM7, ", " N-grams of order n, ", " 2 or more \n", " N-grams of order 3, \n", " 3, ", "  n-grams, \n", " n-grams of order 4. \n", " Ngrams of length 4 \n", " 3-grams, ", "  n-grams \n", " 0 ", " n-grams of order 4, ", "  n-grams of order n, \n", " 2, \n", " ", " n-grams of order n \n", " n-grams of order 4, ", " Ngrams of order 4, ", " n-grams of order 4, ", " n-grams of order 4, ", " [Your answer here], ", " n-grams of order n, ", " Ngrams of order 4, ", " n-grams of order 3, \n", " n-grams of any order, ", " N-grams of order 5 \n", " n-grams of order 4 \n", " n-grams of order 4, ", " n-grams of any length, ", " n-grams of order 4. \n", " n-grams of order 3 \n", " n-grams of order 4 \n", " N-grams of order 4, \n", " N-grams of unspecified length are not mentioned in the article, however n-grams of orderINLINEFORM7 in INLINEFORM8 and INLINEFORM9, respectively are specified. Therefore, it is likely that n-grams of any unspecified length are not aligned by PARENT, and it is unclear what order of n-grams are aligned by PARENT, although it is possible to make an educated guess that n-grams of unspecified length might be of order 1, 2, or 3, and higher order n-grams are not aligned by PARENT. However, the article does not specify the length of n", " 0-length ngrams \n", "  Ngrams of order n (not specified), \n", " n-grams of order 1 \n", " n-grams of order 4, ", " n-grams of order 1, "], "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": [" Wikipedia data and Reddit CMV data. \n", " Two datasets, Wikipedia and CMV. \n", " Two datasets that the model is applied to are Wikipedia data and Reddit CMV data. \n", " Two datasets. \n", " Wikipedia and CMV (ChangeMyView) datasets. \n", " Wikipedia and ChangeMyView. \n", " The Wikipedia and CMV datasets. \n", " The Wikipedia and CMV datasets. \n", " The Wikipedia dataset and the CMV dataset, \n", " Wikipedia data and Reddit CMV data, ", " Two datasets are Wikipedia and CMV datasets. \n", " Wikipedia and ChangeMyView. \n", " The Wikipedia and CMV datasets. \n", " Wikipedia and ChangeMyView, ", " Wikipedia and CMV datasets, ", " Wikipedia conversations dataset and Reddit CMV dataset. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia and Reddit CMV, ", " Wikipedia data and Reddit CMV data, ", " Wikipedia and CMV (ChangeMyView), ", " Wikipedia and CMV, \n", " Wikipedia and Reddit CMV data. \n", " Two datasets are Wikipedia data and Reddit CMV data. \n", " Two datasets: Wikipedia and Reddit CMV. \n", " Wikipedia and CMV, ", " Two datasets, one for Wikipedia conversations and another for Reddit CMV conversations. \n", " Two datasets, Wikipedia data and Reddit CMV data. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia and ChangeMyView. \n", " Wikipedia and Reddit CMV, ", " Wikipedia data and Reddit CMV data, ", " Wikipedia and CMV (ChangeMyView) datasets, ", " Wikipedia and Reddit CMV datasets, ", " Wikipedia data and Reddit CMV data. \n", " Two datasets that the model is applied to are the Wikipedia conversations dataset and the subreddit ChangeMyView (CMV) dataset. \n\n", " Wikipedia and Reddit CMV datasets. \n", " Wikipedia and CMV. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia conversations dataset and Reddit CMV data. \n", " The Wikipedia dataset and the CMV dataset. \n", " Wikipedia and Reddit. \n", " Wikipedia data and Reddit CMV data, \n", " Wikipedia and Reddit CMV. \n", " Wikipedia and ChangeMyView, ", " Wikipedia and CMV datasets, ", " Wikipedia data and Reddit CMV data, ", " The Wikipedia and CMV datasets. \n", " Wikipedia and Reddit CMV, \n", " Wikipedia data and Reddit CMV data, ", " Two datasets. \n", " Wikipedia and Reddit CMV data. \n", " The Wikipedia and CMV datasets. \n", " Wikipedia data and Reddit CMV data. \n", " Wikipedia and CMV (ChangeMyView), ", " Wikipedia and CMV. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia and Reddit CMV. \n", " \n", " Wikipedia data and Reddit CMV data, \n", " Wikipedia and ChangeMyView (CMV). \n", " Wikipedia and CMV, \n", " Wikipedia and Reddit CMV data, ", " The two datasets are the expanded version of the annotated Wikipedia conversations dataset and the subreddit ChangeMyView dataset. \n", " Two datasets, Wikipedia and CMV, which are used to test the effectiveness of the proposed model. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia and CMV datasets, ", " Wikipedia and Reddit CMV datasets, ", " Wikipedia and Reddit, ", " Wikipedia and CMV datasets. \n", " Wikipedia and Reddit CMV data, ", " Wikipedia and ChangeMyView. \n", " The two datasets are Wikipedia data and Reddit CMV data. \n", " Wikipedia and ChangeMyView, ", " The two datasets are the Wikipedia dataset and the CMV dataset, which are used to evaluate the model's performance in forecasting conversational derailment. \n", " Two datasets are the Wikipedia and CMV (ChangeMyView) datasets. \n", " Wikipedia and ChangeMyView, ", " Wikipedia and CMV (ChangeMyView) datasets. \n", " Two datasets,  Wikipedia data and Reddit CMV data, \n", " Wikipedia conversations and Reddit CMV data. \n", " The Wikipedia and CMV datasets. \n", " Wikipedia and CMV, ", " \n", " Two datasets, \"Conversations Gone Awry\" dataset and ChangeMyView (CMV) dataset. \n\n", " Wikipedia and Reddit CMV, ", " Wikipedia and ChangeMyView. \n", " The two datasets are Wikipedia and Reddit CMV. ", " Wikipedia and CMV, ", " The two datasets are Wikipedia data and Reddit CMV data. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia and Reddit CMV. \n", " Wikipedia and CMV, ", " Wikipedia and CMV, ", " Wikipedia and Reddit CMV data. \n", " The two datasets are Wikipedia conversations dataset and Reddit CMV dataset. \n", " Wikipedia and CMV. \n", " Wikipedia data and Reddit CMV data, ", " Wikipedia and ChangeMyView. \n", " Two datasets are the Wikipedia-based dataset and the Reddit CMV dataset. \n", " Wikipedia and CMV. \n", " Two datasets are Wikipedia and Reddit CMV. \n"], "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": [" They combine audio and text sequences using dual RNNs and then combine the information from these sources using a feed-forward neural model. \n", " They combine them using a feed-forward neural network layer, ", " By using a dual recurrent encoder model, \n", " By concatenating the last hidden states of the audio-RNN and text-RNN. \n\n", " They combine them through concatenation and then passing through a fully connected neural network layer, ", " They combine audio and text sequences using a dual RNN and a feed-forward neural model. \n", " They combine them by concatenating them, \n", " They combine the information from these sources using a feed-forward neural model. \n", " By concatenating and passing through a fully connected neural network layer, then through a feed-forward neural network layer, finally applying the softmax function. \n", " They combine them in the feed-forward neural model, \n", " By concatenating them. \n", " By concatenating the final audio encoding vector and textual encoding vector, then passing it through a feed-forward neural network layer, ", " By concatenating the vectors, ", " They use a dual RNN model to encode audio and text sequences separately, and then combine the information using a feed-forward neural model. \n", " They combine them via a feed-forward neural network layer. \n", " By concatenating the vectors after encoding. \n", " By concatenating the final encoding vectors from the audio-RNN and text-RNN. \n", " By concatenating the final hidden state of the audio-RNN and the final hidden state of the text-RNN. \n", " They use a feed-forward neural model to combine the information from audio and text sequences. \n", " By concatenating the final hidden state of the audio-RNN with the prosodic features to generate a more informative vector representation of the audio signal, then pass it through a fully connected neural network layer, and then concatenate it with the textual encoding vector T to finally output the emotion class. \n", " They use a feed-forward neural model, ", " By concatenating the final hidden state of the audio-RNN with the prosodic features, and then passing it through a fully connected neural network layer. \n", " They combine them using a feed-forward neural model. \n", " They combine them using a feed-forward neural model after encoding information from both sources using dual RNNs. \n", " They combine them via a feed-forward neural model, ", " They combine audio and text sequences by concatenating their last hidden states. \n", " They combine the audio and text sequences using a feed-forward neural model. \n", " They combine them using a feed-forward neural network layer. \n", " They combine them by concatenating the last hidden state of the audio-RNN with the final encoding vector of the text-RNN. \n", " By concatenating the last hidden state of the audio-RNN with the prosodic features. \n", " They combine audio and text sequences in their RNN using a feed-forward neural model. \n", " By using a dual RNN architecture.\n", " They combine them in the feed-forward neural model. \n", " By using two RNNs to encode data from the audio signal and textual inputs independently, and then combining their final encoding vectors. \n\n", " By passing the final hidden states of the text-RNN and audio-RNN through fully connected neural network layers and concatenating the vectors. \n", " By using a dual RNNs and then combining the information using a feed-forward neural model, ", " By concatenating the final encoding vectors from the audio-RNN and text-RNN. \n", " They use two RNNs to encode data from the audio signal and textual inputs independently, then they concatenate the last hidden states of the two RNNs. \n", " They combine audio and text sequences in their RNN via a feed-forward neural network. \n", " They combine them using a feed-forward neural model. \n", " They use a dual recurrent encoder model to combine audio and text sequences in their RNN. \n", " They combine audio and text sequences using a feed-forward neural model. \n", " They combine the audio and text sequences using dual RNNs and then combine the information from these sources using a feed-forward neural model. \n", " They use a dual RNN to encode the information from audio and text sequences, and then combine the information from these sources using a feed-forward neural model. \n\n", " By using a dual recurrent encoder model, ", " They use dual RNNs to encode both audio and text sequences separately and then combine the information using a feed-forward neural model. \n", " By concatenating the last hidden states of the audio-RNN and text-RNN, ", " They combine the information from the two sequences using a feed-forward neural network layer. \n", " By using a dual recurrent encoder model, ", " By concatenating them and then passing through a fully connected neural network layer. \n", " They combine them in a feed-forward neural model using the final encoding vectors from the audio-RNN and text-RNN. \n", " They combine them by concatenating the final vectors, ", " They use a dual recurrent encoder, ", " They combine audio and text sequences in their RNN using a feed-forward neural network. \n\n", " They combine the information from audio and text sequences using a feed-forward neural model to predict the emotion class. \n", " They combine them using a feed-forward neural model. \n", " They combine them via a feed-forward neural network layer. \n", " They combine the information from these sources using a feed-forward neural model after they are encoded separately using dual RNNs, ", " They use a dual RNN to encode the information from the audio and text sequences separately, and then combine the information from these sources using a feed-forward neural model to predict the emotion class. \n", " They use a feed-forward neural network, ", " They combine audio and text sequences by concatenating their final encoding vectors. \n\n", " By concatenating and passing through a fully connected neural network layer, \n", " They use a feed-forward neural model to combine the information. \n", " Using dual RNNs and a feed-forward neural model, ", " By concatenating the vectors from the final encoding of the audio-RNN and the final encoding of the text-RNN.\n", " They combine the audio and text sequences using a feed-forward neural model. \n", " They use a feed-forward neural model to combine the information from audio and text sequences. \n", " By passing them through two RNNs and then concatenating their final encoding vectors. \n", " They use a dual recurrent encoder model, ", " They use a dual recurrent encoder model. \n", " They use a feed-forward neural network to combine the information. \n", " By concatenating the final hidden states of the audio-RNN and text-RNN. \n", " They combine the information from the audio-RNN and text-RNN using a feed-forward neural model. \n", " By concatenating the last hidden state of the audio-RNN with the final hidden state of the text-RNN, \n", " They combine audio and text sequences in their RNN by using a dual recurrent encoder model with a feed-forward neural model. \n", " They combine the information from these sources using a feed-forward neural model after encoding the information from audio and text sequences using dual RNNs. \n", " They combine the audio and text sequences using a feed-forward neural model. \n", " They combine the two sequences using a feed-forward neural model after encoding the audio and text information separately with dual RNNs. \n", " By concatenating the final encoding vectors from the audio-RNN and text-RNN. \n", " By concatenating the vectors A and T. \n", " They combine audio and text sequences by concatenating the final encoding vectors from the audio-RNN and text-RNN, and then passing the concatenated vector through a fully connected neural network layer. \n\n", " They combine audio and text sequences in their RNN by using a dual recurrent encoder. \n", " They combine the information from these sources using a feed-forward neural model. \n", " They combine them using a feed-forward neural network layer. \n", " They combine audio and text sequences in their RNN via concatenation. \n", " They combine them in a feed-forward neural network layer. \n", " They combine audio and text sequences using a dual RNN, ", " They combine the audio and text sequences in their RNN using a feed-forward neural model. \n\n", " They combine them in a feed-forward neural model. \n", " By concatenating the final encoding vectors from the audio-RNN and text-RNN, ", " They combine them via a feed-forward neural model to predict the emotion class. \n", " They use a dual RNN that encodes audio and text sequences separately and then combines the information from these sources using a feed-forward neural model. \n", " By concatenating the vectors from the audio-RNN and text-RNN, ", " They combine audio and text sequences in their RNN by concatenating the final encoding vectors from the audio-RNN and text-RNN. \n\n", " They combine them using a feed-forward neural model, \n", " By concatenating the vectors from the audio-RNN and text-RNN. \n", " By passing them through two RNNs independently. \n", " They combine them via a feed-forward neural model. \n", " They use a feed-forward neural network layer, ", " By concatenating the vectors from two RNNs, \n"], "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": [" No, ", " no, ", " No, ", " No, ", " no, ", " No, \n", " no, ", " No, ", " No, ", " No, ", " No, ", " unanswerable, ", " No, ", " unanswerable, ", " no, ", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " No, ", " No, ", " No, ", " No, ", " Yes, ", " No, ", " No, ", " No, ", " No, ", " unanswerable \n", " unanswerable, ", " No, ", " unanswerable, ", " No, ", " unanswerable, ", " unanswerable, ", " Yes, ", " unanswerable, ", " No, ", " No, ", " No, ", " no, ", " unanswerable, ", " No, ", " unanswerable, ", " no, ", " No, ", " no, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " No, ", " No, \n", " No, ", " Yes, ", " No, ", " unanswerable, ", " unanswerable, ", " no, ", " No, ", " no, ", " No, \n", " no, ", " No, ", " No, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " No, ", " No, ", " Yes, ", " No, ", " Yes, ", " unanswerable, ", " unanswerable, ", " No, ", " no, ", " No, ", " no, ", " no, ", " unanswerable \n", " Unanswerable, ", " No, ", " unanswerable, ", " unanswerable, ", " No, ", " unanswerable, ", " No, ", " No, ", " No, ", " unanswerable, ", " No, ", " unanswerable, ", " Unanswerable, ", " no, ", " No, ", " no, ", " No, ", " No, ", " No, ", " unanswerable, ", " No, "], "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": [" 1,873 Twitter conversation threads, roughly 14k tweets. \n", " 14k tweets. \n", " 1,873 conversation threads, roughly 14k tweets. \n", " 14k tweets, \n", " 14k tweets, ", " 1,873 conversation threads, roughly 14k tweets. \n", " 1,873 conversation threads, roughly 14k tweets. \n", ", ", " 14k tweets, \n", " 1,873 Twitter conversation threads, roughly 14k tweets. \n", " 14k tweets, ", " 1,873 conversation threads, roughly 14k tweets, \n", " 14k tweets. \n", " Roughly 14k tweets. \n", " 1,873 conversation threads, roughly 14k tweets, ", " 14k tweets, ", " 1,873 Twitter conversation threads, roughly 14k tweets. ", " 1,873 Twitter conversation threads, roughly 14k tweets, ", " 14k tweets, \n", " Roughly 14k tweets. ", " 14k tweets, ", " 1,873 Twitter conversation threads, roughly 14k tweets, \n", " 1,873 Twitter conversation threads. ", " Approximately 1.5 million comments. \n", " 14k tweets, ", " Roughly 14k tweets, \n", " 14k tweets, ", " 14k tweets, ", " 14k tweets. \n", " Roughly 14k tweets. ", " 14k tweets, ", " 1,873 Twitter conversation threads, roughly 14k tweets. \n", " Approximately 14k tweets. \n", " 14,000 tweets. \n", " 1,873 conversation threads, roughly 14k tweets, \n", " 1,873 conversation threads, roughly 14k tweets. \n", " 1,873 conversation threads. \n", " 1,873 conversation threads and 14k tweets. \n", " 1,873 conversations, ", " 1,873 conversation threads, approximately 14k tweets. \n", " Roughly 14k tweets. \n", " 1,873 conversation threads, roughly 14k tweets, \n", " 1,873 conversation threads, roughly 14k tweets. \n", " Roughly 14k tweets, \n", " 14,000 tweets. \n", " 1,873 conversation threads, roughly 14k tweets, ", " 14,000 tweets, ", " 1,873 conversation threads, roughly 14k tweets, \n", " 1,873 conversations and 14k tweets, \n", " 1,873 conversation threads, roughly 14k tweets. \n", " 14k tweets, ", " Roughly 14k tweets. ", " 14k tweets. \n", " Roughly 14k tweets, \n", " Approximately 14,000 tweets, ", " 1,873 conversations and 14,000 tweets, ", " 1,873 conversation threads, roughly 14k tweets,  ", " 1,873 Twitter conversation threads. ", " 1,873 conversation threads, roughly 14k tweets. \n", " 14k tweets, \n", " 14k tweets, \n", " 1,873 conversation threads, roughly 14k tweets. \n", " 14k tweets, \n", " 1,873 conversation threads, approximately 14k tweets, \n", " Roughly 14,000 tweets. ", " 1,873 conversation threads, \n", " 1,873 conversation threads, \n", " 1,873 conversation threads and roughly 14,000 tweets, ", " Roughly 14k tweets. \n", " 1,873 Twitter conversation threads, roughly 14k tweets. \n", " 1,873 Twitter conversation threads, roughly 14k tweets. ", " 1,873 conversation threads, roughly 14k tweets. ", " 1,873 conversation threads, roughly 14k tweets. \n", " 14k tweets, ", " 1,873 Twitter conversation threads, roughly 14k tweets. \n", " 1,873 conversation threads, and roughly 14k tweets, ", " 14k tweets, ", " 14k tweets, ", " 1,873 conversation threads, \n", " 1,873 conversation threads, roughly 14k tweets, ", " 1,873 conversation threads, roughly 14k tweets, \n\n", " Roughly 14k tweets. \n", " Roughly 14k tweets. \n", " 1,873 conversation threads, \n", " 14k, \n", " Roughly 14k tweets, ", " 14k tweets. \n", " 14k tweets, ", " 14k tweets, ", " 1,873 conversation threads. ", " 1,873 Twitter conversation threads. \n", " 1,873 Twitter conversation threads, roughly 14k tweets, \n", " 1,873 conversation threads, roughly 14k tweets. \n", " 14k tweets, ", " 14k tweets, \n", " 14,000 tweets, \n", " 1,873 conversation threads and 14k tweets. \n", " 14k tweets. \n", " 14k tweets. \n", " 14k tweets. \n"], "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": [" the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset, which has been extensively explored in our paper,  is of high quality as it is used as a test dataset in the research, ", " The benchmark dataset is social honeypot dataset. ", " unanswerable, \n", " The benchmark dataset used in the paper is the Social Honeypot dataset, ", " unanswerable \n", " Lee et al. lee2011seven use the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset. The benchmark dataset quality is extensively explored in this paper. \n", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " The Social Honeypot dataset is a benchmark dataset, and its quality is not explicitly stated in the article. ", " unanswerable, ", " unanswerable, ", " The benchmark dataset is built by alluring social spammers' retweet, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " It is called the Social Honeypot dataset and its quality is high. ", " unanswerable, ", " The benchmark dataset is the social honeypot dataset, and it is of high quality, ", " unanswerable, ", " unanswerable, ", " the so-called social honeypot dataset, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Lee et al. 2011 use the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset, which has been extensively explored in our paper. \n\n", " unanswerable, ", " unanswerable, ", ", ", " Lee et al. build a benchmark dataset called social honeypot. \n", " The benchmark dataset is the social honeypot built by alluring social spammers' retweet, ", " unanswerable, ", " unanswerable, ", " The benchmark dataset is the Social Honeypot dataset. The quality of this dataset is unknown. \n", " The social honeypot by alluring social spammers' retweet, ", " unanswerable, ", " unanswerable, ", " The benchmark dataset is the social honeypot, and its quality is extensively explored in the paper. \n", " unanswerable, ", " The benchmark dataset was built by Lee et al. to alluring social spammers' retweet. \n", " unanswerable, ", " unanswerable, ", " The benchmark dataset is the social honeypot, and its quality is high. However, its quality may be affected by the incorrectly classified spammers, ", " Lee et al. lee2011seven use the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset, which has been extensively explored in our paper. The quality of the dataset is not explicitly stated as high in the article, but it has been explored extensively in the current paper. \n", " Lee et al. lee2011seven use the so-called social honeypot by alluring social spammers' retweet to build a benchmark dataset., ", " unanswerable,    \n", " Lee et al. lee2011seven use the social honeypot by alluring social spammers' retweet to build a benchmark dataset, ", " The Social Honeypot dataset, ", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " The benchmark dataset is a social honeypot dataset created and deployed by Lee et al., and its quality is high. \n", " unanswerable, ", " unanswerable, ", " unanswerable,    \n", " The Social Honeypot dataset is a benchmark dataset and its quality is high. \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " It has been extensively explored in our paper. ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " The benchmark dataset is the Social Honeypot dataset, which is of high quality according to the article. \n", " unanswerable, ", " unanswerable, ", " It has been extensively explored in our paper, ", " unanswerable, ", " Lee et al. use the social honeypot to build a benchmark dataset. The quality of the dataset is extensively explored in the paper. However, its quality is not explicitly stated as high. \n", " Lee et al. built a benchmark dataset called the social honeypot by alluring social spammers' retweet, ", " unanswerable, ", " unanswerable, "], "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": [" 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI. ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " by 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. ", " 2.11 BLEU points. \n", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI. \n\n\n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 6.37 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11, ", " \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU points. \n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.  \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n\n", " 2.11, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU, ", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI, \n", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI, ", " [Your concise final answer here], ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n\n", " 2.11, \n", " 2.11 BLEU points \n", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI, ", " 2.11 BLEU, 1.7 FKGL, 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n\n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n\n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, ", " 2.11 BLEU points, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n\n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n\n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU points, \n", " 2.11 BLEU, 1.7 FKGL, and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI.  \n\n", " 2.11 BLEU, ", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n", " 2.11 BLEU, 1.7 FKGL and 1.07 SARI. \n"], "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": [" 52% of the annotators marked translations as having equal quality, with a total of 700 examples evaluated. \n", " 1, ", " 3, \n", " 5, ", " 700, ", " 700, ", " 10 \n", " 700, \n", " 700, ", " 1, ", " 1, ", " , ", " 52%, ", " 1, ", " 70, \n", " 1, ", " 700, ", " 700, \n", " 70 human evaluators. \n", " 700, \n", " 700, ", " 700, ", " ?, ", " 1, ", " 700, \n", " 1, ", " 1, ", " 1, ", " , ", " 700, ", " 7, ", " 1, ", " 1, ", " 700, ", " 1, ", " 7, ", " 1, ", " 1, ", " , ", " 700, \n", " , ", " 1, ", " 1 (note: although it's 1 human evaluator in the experiment, the number is not explicitly stated, we infer it from the experiment description), ", " 700, \n", " 700, ", " 3, ", " 52%, ", " 1, ", " 700, ", " 700, \n", " 70.0, ", " 70, ", " 700, ", " 700, ", " 1, ", " 3, \n", " 700, ", " 2, ", " 1, ", " 70, ", " 1, ", " 1, ", " 700, ", " 1, ", " 700, \n", " 1, ", " 52%, ", " 52%, ", " 700, ", " 700, ", " 7, ", " 1 person was not mentioned, however, a team of annotators worked on the human evaluation, and the total number of annotators is not clear but there are at least 2 people involved: Ivan Titov, and Rico Sennrich acknowledge support, and also the text mentions \"the annotators\" which is plural; also it was mentioned that \"we picked only examples where a DocRepair translation is not a full copy of the baseline one\" suggesting that more than one person was involved.  On the other hand it was mentioned \"a human evaluation on random 700 examples\", which makes it clear that at least", " 52%, ", " 1 person. \n", " 700, ", " 2, ", " 1, ", " , ", " 52%, ", " 700, ", " , ", " , ", " 52%, ", " 700, \n", " 700, ", " 700, ", " 700, \n", " 7, ", " 700, ", " 1, ", " 1, ", " 1, ", " 1, ", " 1, ", " 700, ", " 700, ", " 1, ", " 73%, ", " 7, ", " 700, "], "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": [" crowdsourcing, ", " Crowdsourcing, ", " [Your concise final answer here], ", " crowdsourcing, ", " crowdsourcing. ", " crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing using an Android application. ", " Crowdsourcing, ", " crowdsourcing, ", " crowdsourcing and Android application, \n", " \n", " crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " crowdsourcing and an Android application, ", " crowdsourcing, ", " crowdsourcing via an Android application, ", " Crowdsourcing, ", " crowdsourcing, \n", " crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, \n", " Crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing and an Android application. \n", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing and an Android application. \n", " Crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing. ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing using an Android application, ", " The DeepMine database was collected using crowdsourcing, \n", " Crowdsourcing. ", " crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing. ", " crowdsourcing via an Android application. \n", " Crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing via an Android application, ", " crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " The data was collected using crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, \n", " crowdsourcing, ", " crowdsourcing via an Android application. \n", " crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing. ", " crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing and an Android application, \n", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing via an Android application, \n", " \n", " crowdsourcing. \n", " Crowdsourcing via Android application, ", " Crowdsourcing and Android application, ", " crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " Crowdsourcing, ", " Crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, ", " crowdsourcing, "], "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": [" \n", " 11 languages in addition to English, including Mandarin, Russian, Spanish, French, Welsh, Kiswahili, Yue Chinese, Estonian, Finnish, Polish, and Hebrew. \n", " English, French, Spanish, Mandarin, Kiswahili, Welsh, Russian, Estonian, Finnish, Cantonese, Mandarin Chinese, and Yue Chinese, ", " 12 languages: English, French, Russian, Mandarin Chinese, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Spanish, Hebrew, and Polish. \n", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Polish, Finnish, and Italian. \n\n", " ", " Twelve languages are covered, including English, Mandarin Chinese, Russian, French, Kiswahili, Welsh, Estonian, Finnish, Spanish, Polish, Yue Chinese, and Hebrew. \n", " \n", " 12 languages, including a mixture of major languages and several low-resource ones, are covered. ", " English, French, Spanish, Mandarin Chinese, Russian, Polish, Estonian, Finnish, Welsh, Kiswahili, Yue Chinese, and Hebrew. \n\n", " English, Mandarin Chinese, Russian, French, Welsh, Kiswahili, Yue Chinese, Estonian, Finnish, Hebrew, Spanish, and Polish. \n", "  English, Russian, French, Chinese, Spanish, Mandarin, Finnish, Estonian, Polish, Welsh, Kiswahili, and Yue Chinese. \n", " English, Mandarin, Spanish, French, Russian, Polish, Welsh, Kiswahili, Estonian, Cantonese, Mandarin Chinese, and Yue Chinese. \n", " [Your concise final answer here], ", " ________, ", " English, Chinese Mandarin, French, Spanish, Kiswahili, Welsh, Russian, Estonian, Finnish, Cantonese, Hebrew, and Yue Chinese, \n", " English, Mandarin Chinese, Russian, French, Kiswahili, Welsh, Spanish, German, Italian, Estonian, Finnish, and Yue Chinese, ", " English, French, Russian, Spanish, Estonian, Finnish, Mandarin Chinese, Cantonese, Polish, Welsh, Kiswahili, and Yue Chinese, ", " English, Mandarin Chinese, Russian, French, Spanish, Welsh, Kiswahili, Finnish, Estonian, Cantonese, Arabic, and Japanese, \n\n", " [Your concise final answer here], ", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Polish, Finnish, and Hebrew, ", " English, Mandarin Chinese, Spanish, French, Polish, Russian, Welsh, Estonian, Finnish, Kiswahili, Hebrew, and Yue Chinese. \n", " 12 typologically diverse languages, ", " English, French, Spanish, Mandarin Chinese, Russian, Polish, Estonian, Finnish, Welsh, Kiswahili, Yue Chinese, and Hebrew. \n", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Finnish, Polish, and Italian are the 12 languages covered in the resource., ", " \n", " Twelve languages including a mixture of major languages and several low-resource languages. \n", " 12, ", " ", " English, Mandarin, Spanish, French, Russian, Italian, Polish, Estonian, Kiswahili, Welsh, Hebrew, and Yue Chinese. \n", " English, Spanish, Mandarin Chinese, French, Kiswahili, Welsh, Estonian, Finnish, Russian, Polish, Yue Chinese, and Hebrew. \n", " 12 languages: English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Finnish, Hebrew, and Arabic. \n", " English, Mandarin Chinese, Russian, French, Welsh, Kiswahili, Yue Chinese, Estonian, Spanish, Finnish, Polish, and Hebrew. \n\n", " English, French, Spanish, Russian, Mandarin Chinese, Estonian, Finnish, Polish, Portuguese, Arabic, Welsh, and Kiswahili\n\n", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Polish, Spanish, and Hebrew, ", " English, French, Spanish, Mandarin Chinese, Russian, Polish, Estonian, Finnish, Welsh, Kiswahili, German, Italian. However, the actual article mentioned more than 12 languages in the part where the authors mention the types of languages that they included a mixture of \"fusional, agglutinative, isolating, and introflexive languages from eight different language families\" and also the article mentioned \"11 languages in addition to English\". Hence, to provide a correct answer, the actual 12 languages covered in the article are: 1) English; 2) Mandarin Chinese (S", " ", " The 12 languages covered are English, Spanish, Mandarin Chinese, French, Russian, Polish, Estonian, Welsh, Kiswahili, Cantonese, Mandarin Chinese, and Yue Chinese. However, note that Mandarin Chinese is mentioned twice in the article and it seems to be an error. Therefore, I will remove Mandarin Chinese (already listed) and substitute it with Finnish as mentioned later in the text. ", " English, French, Spanish, Mandarin Chinese, Russian, Estonian, Polish, Finnish, Welsh, Kiswahili, Yue Chinese, and Hebrew. \n", " english, french, rus, cmn, swa, welsh, yue, cym, est, heb, spa, pol, Reference: Table TABREF10 \n", ", ", " ", " ", " in the Multi-SimLex resource?\n", " English, Mandarin Chinese, Spanish, French, Russian, Welsh, Kiswahili, Estonian, Hebrew, Yue Chinese, Finnish, and Polish. \n\n", " The 12 languages covered in the Multi-SimLex initiative are English, Chinese Mandarin, Spanish, French, Russian, Polish, Estonian, Welsh, Kiswahili, Yue Chinese, Hungarian, and Finnish, ", " 11 languages in addition to English and also mention the languages in the table, ", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Finnish, Polish, and Hebrew, ", " English, French, Mandarin, Russian, Spanish, Estonian, Finnish, Hebrew, Polish, Kiswahili, Welsh, and Yue Chinese. \n", " English, French, Spanish, Mandarin Chinese, Polish, Estonian, Finnish, Russian, Welsh, Kiswahili, Cantonese, Yue Chinese, German, Italian, and Hebrew, Hebrew, \n", " ", " English, Mandarin Chinese, Russian, French, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Polish, Spanish, and Hebrew. ", " \n", " English, Mandarin, Spanish, French, Russian, Estonian, Finnish, Polish, Welsh, Kiswahili, Cantonese, and Yue Chinese. \n", " _______________________, ", " English, French, Spanish, Mandarin, Russian, Polish, Estonian, Finnish, Hebrew, Chinese, Kiswahili, Welsh. ", ", ", " The 12 languages covered are English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Polish, Finnish, and German. \n", " English, Chinese Mandarin, Spanish, Russian, French, Polish, Kiswahili, Welsh, Estonian, Finnish, Cantonese and Yue Chinese. \n\n", " ", "  ", " English, Mandarin, Russian, French, Welsh, Kiswahili, Estonian, Finnish, Spanish, Hebrew, Yue Chinese, and Cantonese. \n", " English, Mandarin Chinese, Russian, French, Spanish, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Hebrew, and Arabic are the 12 languages covered. \n", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Polish, Spanish, and Hebrew. \n", " 12 languages including English. ", " English, Mandarin, Russian, French, Spanish, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Polish, Hebrew. \n", " English, French, Spanish, Russian, Chinese (Mandarin), Polish, Estonian, Finnish, Swedish, Welsh, Kiswahili, and Yue Chinese. \n", " \n", " English, Mandarin, Russian, French, Welsh, Kiswahili, Spanish, Polish, Estonian, Finnish, Cantonese, and Yue Chinese. \n\n", " ", " English, French, Kiswahili, Mandarin Chinese, Polish, Portuguese, Russian, Spanish, Estonian, Welsh, Swedish, and Cantonese Chinese, ", " [Your concise final answer here], ", " English, Mandarin, Russian, French, Welsh, Kiswahili, Spanish, Estonian, Cantonese, Finnish, Polish, and Yue Chinese, \n", " English, French, Mandarin, Russian, Spanish, Welsh, Kiswahili, Yue Chinese, Estonian, Finnish, German, and Polish, ", " ", " English, Mandarin, Spanish, Russian, French, Welsh, Kiswahili, Estonian, Finnish, Cantonese, Polish, Yue Chinese. ", " \n", " English, Mandarin Chinese, Spanish, Russian, French, Estonian, Finnish, Welsh, Kiswahili, Yue Chinese, Italian, and German, ", " English, Mandarin, Spanish, Kiswahili, French, Welsh, Russian, Estonian, Finnish, Cantonese, Portuguese, and Yue Chinese. \n", " 12 languages are covered including English, Chinese Mandarin, Spanish, Russian, French, Kiswahili, Welsh, Estonian, Finnish, Polish, Cantonese, and Yue Chinese., ", " ", " [Your concise final answer here], ", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Spanish, Italian, and Hebrew. \n", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Estonian, Finnish, Spanish, Hebrew, and Polish. \n", " Welsh, Kiswahili, Yue Chinese, Russian, Polish, Finnish, Estonian, Mandarin Chinese, Spanish, French, Cantonese, and English. \n", " English, Mandarin, Russian, French, Spanish, Welsh, Kiswahili, Estonian, Finnish, Cantonese, Yue Chinese, and Hebrew. \n", " English, French, Spanish, Mandarin Chinese, Yue Chinese, Estonian, Finnish, Russian, Polish, Welsh, Kiswahili, Hebrew. \n", ", ", " English, Mandarin, Russian, French, Welsh, Kiswahili, Estonian, Finnish, Spanish, Yue Chinese, Italian, and Polish. \n", " English, Chinese, Spanish, French, Russian, Estonian, Finnish, Welsh, Kiswahili, Yue Chinese, Mandarin, and Polish.\n\n", " 12 languages, ", " English, Mandarin, Russian, French, Welsh, Kiswahili, Estonian, Finnish, Cantonese, Yue Chinese, Spanish, and German., ", " English, Mandarin Chinese, Spanish, French, Kiswahili, Welsh, Russian, Polish, Estonian, Finnish, Cantonese, and Yue Chinese, ", " English, Mandarin Chinese, Spanish, French, Polish, Russian, Estonian, Finnish, Welsh, Kiswahili, Cantonese, and Yue Chinese. \n\n", " English, Mandarin Chinese, Russian, French, Spanish, Welsh, Kiswahili, Estonian, Finnish, Polish, Cantonese, and Yue Chinese. \n", " English, Mandarin Chinese, Spanish, French, Russian, Estonian, Finnish, Hebrew, Welsh, Kiswahili, Yue Chinese, Polish, Cantonese, Italian, Russian, German, and Arabic. \n\n", " English, Mandarin Chinese, French, Spanish, Kiswahili, Welsh, Russian, Estonian, Finnish, Polish, Cantonese, Yue Chinese. \n", " ", " Welsh, Kiswahili, French, Mandarin Chinese, Yue Chinese, Spanish, English, Estonian, Polish, Russian, Finnish, and Hebrew. \n\n", " English, Mandarin, Russian, French, Kiswahili, Welsh, Yue Chinese, Spanish, Estonian, Finnish, Polish, and Hebrew. \n"], "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": [" An LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. ", " LSTM. ", " ", " LSTM. \n", " An LSTM, ", " An LSTM decoder. \n", " An LSTM decoder, ", " An LSTM decoder. \n", " An LSTM decoder generates the characters in the output word form. ", " ", " An LSTM decoder, ", " LSTM, ", " An LSTM decoder, ", " LSTM, ", " An LSTM decoder with an attention mechanism, ", " An LSTM decoder, ", " LSTM decoder. \n", " LSTM, ", " Lstm decoder with an attention mechanism, ", " ", " An LSTM. ", " It has an LSTM decoder. \n", " An LSTM decoder, ", " LSTM decoder with attention mechanism, ", " The decoder has a single LSTM layer. \n", " LSTM, ", " LSTM decoder, ", " ", " ", " An LSTM decoder with an attention mechanism. ", " LSTM decoder \n", " LSTM, ", " The decoder has a LSTM structure. ", " It is an LSTM decoder. \n", " ", " ", " LSTMs with an attention mechanism, ", " LSTM, ", " ", " ", " LSTM decoder. ", " LSTM decoder. ", " A one component LSTM, ", " An LSTM decoder generates the characters in the output word form. \n", " ", " ", " ", " ", " LSTM with attention mechanism. \n", " An LSTM. ", " ", " An LSTM decoder with an attention mechanism. ", " An LSTM decoder. \n", " ", " An LSTM. ", " An LSTM decoder. ", " An LSTM decoder. ", " LSTMLSTM decoder, ", " LSTM one component at a time. \n", " ", " An LSTM decoder. ", " LSTM decoder, ", " ", " LSTM, ", " An LSTM decoder, ", " A single LSTM decoder, ", " An LSTM decoder with an attention mechanism. ", " LSTMs. ", " LSTM, ", " LSTMs. ", " ", " An LSTM decoder, ", " ", " ", " LSTM. \n", " LSTM decoder, ", " LSTM, ", " An LSTM decoder. ", " An LSTM decoder, ", " LSTM. ", " LSTM with an attention mechanism, ", " An LSTM. ", " ", " ", " An LSTM decoder. \n", " LSTM with attention mechanism, ", " An LSTM decoder, ", " An LSTM. \n", " An LSTM decoder. \n", " A single LSTM decoder, ", " LSTM decoder with attention mechanism. \n", " LSTM decoder, ", " A one-layer LSTM decoder, ", " An LSTM decoder. ", " It has an LSTM decoder with an attention mechanism. \n", " LSTM decoder with an attention mechanism. \n", " ", " An LSTM decoder, ", " ", " LSTM decoder with attention mechanism. \n"], "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": [" Logistic Regression and a Deep Learning Model, ", " Logistic Regression and a deep learning model using GloVe word embeddings, ", " Logistic Regression and Deep Learning Model. \n", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and deep learning model. \n", ", ", " Logistic Regression and deep learning model, ", " Logistic Regression and deep learning models, \n", " Logistic Regression and Deep Learning model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and a deep learning model, ", " Logistic Regression and deep learning model, ", " Logistic Regression and Deep Learning Model, ", " The authors use Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs) for RQE. The specific model used is Logistic Regression Classifier and a Deep Learning Model. The Deep Learning Model is based on a neural network proposed by Bowman et al. and adapted for our RQE task, with three 600d ReLU layers, and a bottom layer taking the concatenated sentence representations as input and a top layer feeding a softmax classifier. The Deep Learning Model uses GloVe as an unsupervised learning algorithm to generate vector representations for", " Logistic Regression and Recurrent Neural Network (RNN), ", " Logistic Regression and deep learning models, ", " Logistic Regression and deep learning model, ", " Logistic Regression and Deep Learning Model, \n", " Logistic Regression and a deep learning model, ", " Logistic Regression and deep neural network models, specifically a model proposed by Bowman et al., ", " Logistic Regression and a Deep Learning Model, ", " Logistic Regression and neural network, ", " Logistic Regression and Deep Learning Model with GloVe embeddings, ", " Logistic Regression and a deep learning model, ", " Logistic Regression and a deep learning model, ", " Logistic Regression and Deep Neural Networks, ", " Logistic Regression and a Deep Learning model with GloVe word embeddings, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and a deep learning model adapted from Bowman et al., ", " Logistic Regression, Deep Learning Model, ", " Logistic Regression and deep neural network model, ", " Logistic Regression and deep learning models, ", " Logistic Regression and a deep learning model with GloVe word embeddings, ", " Logistic Regression and neural networks. \n", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and Deep Learning Model. \n", " Logistic Regression and a deep neural network model, ", " Logistic Regression and Deep Learning Model, ", ", ", " Logistic Regression and deep neural networks, ", " Logistic Regression and deep neural network models, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and deep neural networks with GloVe embeddings. \n", " Logistic Regression and DL model with GloVe word embeddings, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and neural networks, ", " Logistic Regression and a deep learning model. \n", " Logistic Regression and a deep learning model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and deep learning models, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and a deep learning model adapted from the neural network proposed by Bowman et al. \n", " Logistic Regression and a Deep Learning Model, ", " Logistic Regression and a deep learning model adapted from Bowman et al., ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and DL model. \n", " Logistic Regression and DL model, ", " Logistic Regression and Deep Neural Network models, ", " Logistic Regression and a deep learning model, ", " Logistic Regression and Deep Learning Model with GloVe word embeddings, ", " Logistic Regression and a deep learning model, ", " Logistic Regression and Deep Learning, ", " Logistic Regression and Deep Learning Model, ", " Both machine learning and deep learning methods are used for RQE, with Logistic Regression and a deep learning model being specifically mentioned. \n", " Logistic Regression and Deep Learning Model. \n", " Logistic Regression, ", " Logistic Regression and a deep learning model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and Deep Learning Model. \n", " Logistic Regression and a deep learning model. \n", " Logistic Regression and a deep neural network, ", " Logistic Regression and Deep Learning model with GloVe word embeddings. \n", " Logistic Regression and Deep Learning model.\n", " Logistic Regression and neural networks. \n", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and neural networks, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and a deep learning model with GloVe word embeddings, \n", " Logistic Regression and deep learning model, ", " Logistic Regression and neural networks, ", " Logistic Regression and deep neural network, ", " Deep learning model and Logistic Regression classifier. ", " Logistic Regression and a deep learning model adapted from Bowman et al., ", " Logistic Regression and a deep learning model, ", " Deep Learning Model and Logistic Regression, ", " Logistic Regression and DL model. \n", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and a deep learning model with GloVe word embeddings, ", " Logistic Regression and DL model with GloVe word embeddings, ", " Logistic Regression and a Deep Learning model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and Deep Learning Model, ", " Logistic Regression and deep learning model, ", " Logistic Regression and Deep Learning model, ", " Logistic Regression and Deep Learning Model, ", " [Your concise final answer here], ", " Logistic Regression and a Deep Learning Model, "], "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": [" It is if a tweet was retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " [Your concise final answer here], ", " Retweeted more than 1000 times. ", " Retweeted more than 1000 times. \n", " A tweet is considered to have gone viral if it was retweeted more than 1000 times. ", " Retweeted more than 1000 times. ", " retweeted more than 1000 times, ", " It is defined as having been retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " Retweeted more than 1000 times, ", " A tweet went viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " More than 1000 retweets. \n", " Retweeted more than 1000 times. \n", " A tweet is considered viral if it is retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " It is defined as being retweeted more than 1000 times. \n", " They define tweets going viral if it was retweeted more than 1000 times. \n", " They define a tweet as going viral if it is retweeted more than 1000 times. \n", " A tweet is considered viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " They are defined as tweets retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " Retweeted more than 1000 times, ", " Tweets went viral if they were retweeted more than 1000 times, ", " A tweet went viral if it was retweeted more than 1000 times. \n", " Tweets are considered to have gone viral if they were retweeted more than 1000 times. ", " A tweet went viral if it was retweeted more than 1000 times. \n", " It is when a tweet is retweeted more than 1000 times, ", " A tweet went viral if it was retweeted more than 1000 times. \n", " They consider a tweet went viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. ", " When a tweet is retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " If a tweet is retweeted more than 1000 times. ", " Retweeted more than 1000 times, ", " Retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " Retweeted more than 1000 times, ", " It is retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. ", " A tweet that was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " Being retweeted more than 1000 times. \n", " A tweet is considered to have gone viral if it was retweeted more than 1000 times. \n", " More than 1000 retweets. \n", " They consider a tweet went viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " A tweet goes viral if it was retweeted more than 1000 times. \n", " It refers to tweets that were retweeted more than 1000 times. \n", " A tweet goes viral if it was retweeted more than 1000 times. ", " Retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. ", " When it was retweeted more than 1000 times. \n", " They define tweets going viral as being retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " They define a tweet as going viral if it was retweeted more than 1000 times. \n", " [Your concise final answer here], ", " Retweeted more than 1000 times, ", " Being retweeted more than 1000 times. ", " A tweet is considered to go viral if it was retweeted more than 1000 times. \n", " [Your concise final answer here], ", " Retweeted more than 1000 times, ", " They are retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " [Your concise final answer here], ", " More than 1000 retweets. \n", " Retweeted more than 1000 times. \n", " They consider a tweet went viral if it was retweeted more than 1000 times. \n", "  A tweet went viral if it was retweeted more than 1000 times. ", " It is defined as a tweet that was retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. \n", " A tweet went viral if it was retweeted more than 1000 times. \n", " \n", " If a tweet was retweeted more than 1000 times. ", " A tweet contains fake news if its text falls within any of the five categories described by Rubin et al., and it is retweeted more than 1000 times. ", "  retweeted more than 1000 times. ", " Retweeted more than 1000 times, ", " They considered that a tweet went viral if it was retweeted more than 1000 times. \n", " retweeted more than 1000 times, \n", " Being retweeted more than 1000 times, ", " If it was retweeted more than 1000 times. ", " Retweeted more than 1000 times. \n", " Retweeted more than 1000 times, ", " A tweet that was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. ", " Retweeted more than 1000 times, ", " A tweet went viral if it was retweeted more than 1000 times. \n", " A tweet is considered to have gone viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n", " \n", " Retweeted more than 1000 times. ", " retweeted more than 1000 times, ", " A tweet goes viral if it was retweeted more than 1000 times. \n", " Retweeted more than 1000 times. \n"], "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": [" unanswerable, ", " Yes, ", " No, ", " No, ", " No, ", " unanswerable, ", " unanswerable. \n", " No, ", " No, ", " No, \n", " No, \n", " No, ", " No, ", " No, ", " No, ", " no, ", " No, ", " No, ", " unanswerable, \n", " no, ", " No, ", " unanswerable, ", " unanswerable, \n", " No, ", " No, ", " No, ", " No, ", " unanswerable, ", " unanswerable, ", " Yes, \n", " No, ", " unanswerable, ", " no, ", " unanswerable, ", " No, ", " no, \n", " No, ", " Yes, ", " yes, ", " no, ", " No, ", " No, ", " No, ", " Yes, ", " No, ", " No, ", " no, ", " No, ", " No, ", " No, ", " No, \n", " no, ", " No, ", " unanswerable, ", " No, ", " No, ", " unanswerable, ", " unanswerable, ", " No, \n", " Yes, ", " yes, ", " No, ", " unanswerable, ", " yes, ", " unanswerable, ", " No, ", " No, ", " yes, ", " Yes, ", " No, ", " No, ", " no, ", " Yes, ", " No, \n", " no, ", " No, \n", " no, ", " unanswerable, ", " No, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " no, ", " No, ", " No, ", " no, ", " Yes, ", " no, ", " no, ", " No, ", " No, ", " No, ", " No, ", " no, ", " No, ", " No, ", " No, ", " no, ", " no, "], "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": [" no, ", " no, ", " No, ", " no, ", " no, ", " No, ", " No, ", " No, ", " No, ", " No, ", " yes, ", " no, ", " No, ", " no, ", " unanswerable, ", " no, ", " yes, ", " No, ", " no, ", " No, ", " unanswerable, ", " No, ", " unanswerable, ", " no, ", " No, ", " No, ", " No, ", " unanswerable, ", " No, ", " unanswerable, ", " no, ", " no, ", " no, ", " Yes, ", " no, ", " No, ", " unanswerable, ", " No, ", " no, ", " no, ", " no, ", " no, ", " No, ", " no, ", " No, ", " no, ", " yes, ", " no, ", " No, ", " no, ", " yes, ", " No, ", " yes, ", " no, ", " no, ", " no, ", " unanswerable, ", " unanswerable, ", " yes, ", " No, ", " no, ", " unanswerable, ", " unanswerable, ", " No, ", " Yes, ", " No, ", " Yes, ", " unanswerable, ", " no, ", " no, ", " no, ", " no, ", " Yes, ", " no, ", " no, ", " Yes, ", " yes, ", " yes, ", " yes, ", " unanswerable, ", " No, ", " No, ", " no, ", " Yes, ", " yes, ", " No, ", " Yes, ", " Yes, ", " no, ", " unanswerable, ", " yes, ", " Yes, ", " Yes, ", " unanswerable, ", " No, ", " No, ", " no, ", " unanswerable, ", " No, ", " no, "], "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": [" Ranked 3rd.\n", " Ranked 4th in sentence-level classification (SLC) task with an F1 score of 0.673. \n", " ensemble+ of (r4, r7, r12) had the best performance, with F1 score of 0.673. \n", " The ensemble+ of (r4, r7, r12) on test set was ranked 4th in SLC task with an F1 of 0.573, ", " CNN model with BERT and feature vectors, 0.673. \n", " Ranked 4th, ", " BERT with ensemble, 0.673 F1 score. \n", " BERT model, 4th and 3rd respectively for SLC and FLC tasks.\n", " The ensemble+ of (r4, r7 r12) was ranked at 4th position in sentence-level classification (SLC) task. \n\n", " The multi-granularity ensemble of LSTM-CRF with BERT, which had a score of 0.673, \n", " BERT, achieved state-of-the-art performance of 0.673. \n", " The best performing model among author's submissions was the ensemble+ of (r4, r7, r12) for the SLC task, with a binary-F1 of 0.673. \n", " BERT model, ", " The best performing model among author's submissions for the Fragment-Level Propaganda Detection is the ensemble+ of (II and IV), which achieved a performance of 0.674.\n\n", " The ensemble+ of (r4, r7 r12) for SLC task achieved the 4th position and the ensemble+ of (II and IV) for FLC task achieved the 3rd position. It is not clear what the performance was, however it is reported to be ranked 4th for SLC and 3rd for FLC,  ", " The best performing model among author's submissions is the ensemble of Logistic Regression, CNN and BERT with relax-voting, which achieved a sentence-level propaganda detection F1 score of 0.673. \n", " Ensemble of Logistic Regression, CNN and BERT with majority voting, ", " Ranked 3rd in FLC and S4th in SLC, ", " ensemble+ of (r4, r7, r12) submitted to SLC task, which obtained macro-F1 score of 0.673. \n", " Ranked 3rd in FLC task, ", " Ensemble+ of (r19 on SLC test set), ", " The best performing model among author's submissions for SLC task had a score of 0.673 in F1, the best performing model among author's submissions for FLC task was ranked at 3rd position. \n", " BERT ensemble model is ranked 3rd, ", " 4th position with a binary-F1 of 0.673, ", " The best performing model among author's submissions, specifically for the FLC task, is the ensemble+ of (II and IV) which boosts in recall and has an improved F1 on dev (external) and test set. \n", " The best performing model among author's submissions was the ensemble model that includes BERT and other models, achieving an F1 score of 0.673. \n", " ensemble+ of (r4, r7, r12), 0.673 \n", " Our submission using an ensemble of Logistic Regression, CNN and BERT achieved the 4th position in SLC and 3rd position in FLC tasks with F1 score of 0.673 in SLC and a macro-F1 score of 0.638 in FLC, ", " Ensemble+ of r3, r6 and r12 from each fold1-5, \n", " CNN + BERT + feature vectors had the best performance. \n", " The CNN model with ensemble achieved the best F1 score of 0.673. \n", " The system (MIC-CIS) is ranked 3rd (out of 12 participants) and 4th (out of 25 participants) in FLC and SLC tasks, respectively. The ensemble+ of (r4, r7, r12) was analyzed after test submission, and ensemble+ of (II and IV) from each of the folds 1-3, i.e., |M|=6 models to obtain predictions on test. \n\n", " The best performing model for FLC task, a multi-task neural sequence tagger based on LSTM-CRF with linguistic features and BERT, achieved an F1 of 0.673 with a recall of 0.81, and for SLC task, an ensemble of three different models including Logistic Regression, CNN, and BERT achieved an F1 of 0.673 and a recall of 0.81. \n", " Ranked 4th, ", " Our submission that performed majority-voting among predictions of r19 and BERT with LSTM-CRF with multi-granularity loss is ranked 4th, ", " Ranked 3rd or 4th, ", " Ensemble+ of (r4, r7, r12) for SLC task had best performance. \n", " ensemble of Logistic Regression, CNN and BERT with relax-voting. \n", " Ensemble+ (r19) with ensemble of (r4, r7, r12) had the best performance, ", " Ensemble of multi-granularity network with BERT achieved best performance, with the macro-F1 score of 0.673, ", " The ensemble of multi-grain, multi-task LSTM-CRF with BERT in Fragment-Level Propaganda Detection is best performing model among author's submissions. However, the performance of best performing model on the test set for FLC task is unanswerable, but on dev (internal) set, it achieved a performance of 0.783 and on dev (external) set it achieved a performance of 0.766. For SLC task, the ensemble+ of (r4, r7 r12) was analyzed after test submission and our submission is ranked at 4th position. \n\n", " ensemble+ of (r4, r7, r12) for SLC task and ensemble+ of (II, IV) for FLC task, ", " The best performing model was BERT with ensemble+ of r19 on the test set, achieving a binary F1 score of 0.673. \n", " CNN-based model with post-processing step, had a F1 of 0.673. \n", " Best performing model among the author's submissions was ensemble+ of r19 for SLC task with an F1 score of 0.673 and for FLC task it was ensemble+ of (II and IV) with an F1 score of 0.727, ", " The authors' submission ranked 4th, \n", " The multi-granularity network with ensemble of multi-task LSTM-CRF and BERT is best performing model, with 0.673 F1 on dev (external) set. \n", " BERT with ensemble, \n", " The best performing model among the author's submissions is the ensemble of Logistic Regression, CNN and BERT, which achieved a F1 score of 0.673. \n\n", " The best performing model in the Sentence-Level Propaganda Detection among author's submissions is ensemble+, which had a score of 0.673, \n", " Ranked 4th, \n", " Ranked 3rd, ", " Ensemble of (r4, r7, r12), \n", " The BERT model with ensemble and postprocess step achieved the best performance with an F1 score of 0.673. \n\n", " Ensemble+, \n", " The best performing model is ensemble+ of (r4, r7, r12) on the SLC task and ensemble+ of (II, IV) on the FLC task, which achieved F1 scores of 0.673 and 0.714, respectively. \n", " The ensemble+ of the three different models: Logistic Regression, CNN and BERT, with the relax-voting scheme on the test set is the best performing model, achieving a sentence-level F1 score of 0.673. \n\n", " The best performing model among the author's submissions is the ensemble of Logistic Regression, CNN and BERT with relax-voting, with a score of 0.673 on dev (external). \n\n", " The model was an ensemble of the three models, and it achieved a binary F1 score of 0.673. \n", " Logistic Regression, ", " BERT-based ensemble, \n", " Ensemble+, ", " 3rd position with F1 0.673 for FLC task. \n", " The best performing model was the ensemble of Logistic Regression, CNN and BERT with 0.673 F1 score. \n", " Ranked 3rd and 4th respectively with F1 score of 0.673 in FLC and SLC tasks. \n\n", " Our submission is ranked 3rd and 4th in FLC and SLC tasks, respectively. The best performing model in FLC task was ranked 3rd, while in SLC task it was ranked 4th.\n\n", " Our submission with ensemble+, best performing model among the different models, was ranked 4th in SLC task with F1 score of 0.673 and ranked 3rd in FLC task. \n\n", " ensemble+ of r19 on test set for SLC task and ensemble+ of (II and IV) from each of the folds 1-3 for FLC task, ranked 4th and 3rd respectively, with F1 of 0.673 for SLC task and improved F1 on dev (external) for FLC task. \n", " CNN with BERT word embeddings had the best F1 score, but exact score is unanswerable.\n\n", " The best performing model among author's submissions is a combination of (Logistic Regression, CNN, and BERT) in ensemble+ with configuration (r4, r7, r12), which achieved a performance of 0.673 in F1 score on SLC task. \n\n", " The BERT based ensemble model with CNN and logistic regression. \n", " The multi-granularity network, with F1 score of 0.673 on dev (external) for SLC task, 0.716 on dev (internal), 0.697 on dev (external) for FLC task, \n", " BERT model with multi-granularity network, 0.673. \n", " Best performing model, the ensemble+, had a F1 score of 0.673 on the dev (external) set for SLC task. \n", " Ranked 4th. \n", " BERT, 0.673. \n", " Ensemble of Logistic Regression, CNN, and BERT with relax-voting, ", " The ensemble of multi-grain, multi-task LSTM-CRF with BERT, it performed 0.673 on dev (external). \n", " Logistic regression classifier with BERTSentEmb outperformed all other models, achieving F1 of 0.673 after ensembling predictions with relax-voting, ", " BERT fine-tuned with ensemble+ of r3, r6 and r12 had 0.673 F1. \n", " 3rd position with 0.673 F1. \n", " The best performing model among author's submissions is the ensemble of Logistic Regression, CNN and BERT, and it had a F1 score of 0.673 on the test set for the SLC task. \n", " The multi-granularity network based on LSTM-CRF and BERT, with F1 score of 0.673. \n\n", " BERT, had performance of 0.673 in F1 score. \n", " The best performing model among author's submissions is their ensemble+ of (r4, r7 r12) for SLC task with an F1 of 0.673, ", " Ensemble+ of (r4, r7, r12) for SLC and ensemble+ of (II, IV) for FLC, ", " The best performing model is BERT in SLC task, with 0.673 F1 score. \n", " Third position in FLC, fourth in SLC. \n", " The BERT-based ensemble (ensemble+) submitted for the FLC task was best performing, with a F1 score of 0.803. \n", "  Our submission was ranked 4th in SLC task and 3rd in FLC task with an F1 score of 0.673 in SLC task. \n", " The best performing model among author's submissions, the ensemble of (r19), had a F1 score of 0.673 on the dev set. \n", " The ensemble+ of (r4, r7, r12) for SLC task was best performing model with an F1 score of 0.673. \n\n", " CNN with FastTextWordEmb and BERTSentEmb is ranked 4th in SLC, \n", " Our system (Team: MIC-CIS) is ranked 3rd and 4th in FLC and SLC tasks, respectively. For the best performing model, in SLC, the ensemble+ of (r4, r7 r12) was analyzed after test submission. For FLC, the ensemble+ of (II and IV) from each of the folds 1-3 was used to obtain predictions on test.\n", " The best performing model, is LSTM-CRF+Multi-task (ranked 1st in leaderboard, achieving an F1-score of 0.754 on the test set),  however the authors did not provide the exact information about this model, but they mentioned their submission was  ranked at 4th and 3rd in SLC and FLC tasks respectively.\n", " The ensemble of (r4, r7 r12) in SLC and the ensemble+ of (II and IV) from each of the folds 1-3 in FLC tasks.\n", " 3rd position in FLC with macro-F1 = 0.683. \n", " Ensemble of BERT, Logistic Regression, and CNN with a threshold of 0.35, \n\n", " The best performing model among author's submissions is the ensemble of Logistic Regression, CNN, and BERT with relax-voting, which has a F1 score of 0.673. \n", " Ensemble of the CNN and BERT in sentence-level propaganda detection, \n"], "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": [" Logistic Regression, ", " CNN, ", " BERT, ", " BERT, ", " BERT, ", " BERT, ", " BERT, ", " CNN, ", " BERT, ", " CNN, ", " BERT, ", " Logistic Regression, ", " BERT, ", " BERT, ", " BERT, ", " BERT, ", " BERT. \n", " BERT, ", " [Your concise final answer here], ", " Logistic Regression, ", " BERT, ", " Logistic Regression, ", " BERT, ", " BERT, ", " CNN, ", " BERT. \n", " BERT, ", " Logistic Regression, ", " CNN, ", " BERT, ", " BERT, ", " Logistic Regression, ", " CNN, ", " BERT, ", " CNN, ", " BERT, ", " BERT, ", " BERT, ", " CNN, ", " BERT, ", " CNN, ", " BERT, ", " BERT, ", " Logistic Regression, ", " BERT, ", " BERT, ", " BERT, ", " Logistic Regression, ", " BERT, ", " BERT, ", " Logistic Regression, ", " Logistic Regression, ", " Logistic Regression, ", " BERT, ", " BERT, ", " BERT, ", " BERT, ", " Logistic Regression, ", " CNN, ", " BERT, ", " CNN. \n", " BERT, ", " BERT, ", " CNN, ", " BERT, ", " BERT, ", " CNN, ", " BERT, ", " BERT, ", " CNN, ", " BERT, ", " BERT, ", " BERT, ", " Logistic Regression, ", " BERT, ", " BERT, ", " Logistic Regression, ", " CNN, ", " BERT, ", " Logistic Regression, ", " BERT, ", " CNN, ", " Logistic Regression, ", " CNN, ", " CNN, ", " BERT, ", " BERT, ", " BERT, ", " BERT, ", " Logistic Regression, ", " BERT, ", " Logistic Regression, ", " BERT, ", " CNN, ", " BERT, ", " BERT, ", " BERT, ", " CNN, ", " BERT, ", " BERT, "], "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": [" word embedding techniques, ", " word embedding techniques such as word2vec. ", " word2vec and retrofitting vector method, ", " Word embedding techniques such as word2vec. \n", " word embedding techniques. \n", " Word embedding techniques such as word2vec are explored in the paper. \n", " word2vec, \n", " Word2vec and second-order co-occurrence vectors, ", " word2vec, ", " Word embeddings. \n", " word embedding techniques such as word2vec. \n", " Word embedding techniques such as word2vec. \n", " Word embedding techniques such as word2vec and other distributional methods. ", " Word embedding techniques such as word2vec, \n", " Word embedding techniques such as word2vec. \n", " Word2vec and retrofitting vector method are not explored in the paper, but retrofitting vector method is actually a previous work very closely related to the current paper that incorporates ontological information into a vector representation. Word embedding is only briefly mentioned in the introduction, but the paper is focused on second–order co–occurrence vectors. \n\n", " word2vec, Retrofitting Vector Method, Word Embeddings. \n", " Word embedding techniques such as word2vec. \n", " Word embedding techniques, specifically including Skip–gram and CBOW models, are explored in the paper. Additionally, the retrofitting vector method proposed by YuCBJW16 is also explored, which incorporates ontological information into a vector representation by including semantically related words. \n\n", " Word embedding techniques, ", " Word2vec, Retrofitting vector, Skip–gram and CBOW. \n", " Word embedding techniques such as word2vec. ", " word2vec, \n", " Word2vec, Retrofitting vector method. \n", " Word embedding techniques such as word2vec. \n", " Word embedding techniques, ", " Word embedding techniques such as word2vec. ", " Word embedding techniques such as word2vec, and retrofitting vector method, are explored in the paper. \n", " Word embedding techniques such as word2vec. ", " word2vec, ", " word2vec, ", " word embedding techniques such as word2vec, \n", " Word embeddings, ", " Word embedding techniques such as word2vec. ", " word embedding techniques such as word2vec. ", " Word embedding techniques, ", " Word embedding techniques, ", " Word embedding techniques such as word2vec. \n", " Word embedding techniques, ", " Word embeddings, ", " Word embedding techniques such as word2vec are explored in the paper, and also retrofitting vector method and second–order co–occurrence vector measure. \n", " word2vec, ", " Word embedding techniques such as word2vec. \n", " Word embeddings techniques such as word2vec. \n", " Word embedding techniques such as word2vec. \n", " Word2vec, \n", " Word embedding techniques such as word2vec. \n", " Word embedding techniques such as word2vec are explored in the paper. \n", " Word embedding techniques such as word2vec, ", " word embedding techniques such as word2vec and retrofitting vector method. \n", " Word embedding techniques such as word2vec and skip-gram, \n", " Word2vec, retrofitting vector method, continuous bag of words (CBOW) approach, and Skip–gram approach. \n", " Word2vec and retrofitting vector method. \n", " Word embedding techniques such as word2vec. \n", " Word embeddings. ", " Word2vec and Retrofitting vector method, ", " Word embedding techniques such as word2vec, and retrofitting vector method. ", " Word2vec and retrofitting vector method are explored in the paper. ", " Word2vec, Intrinsic Information Content, and second-order co-occurrence vectors. However, in the paper,  word2vec is mentioned as a popular technique, but not explored. Intrinsic Information Content is used as an alternative method to estimate information content. Second-order co-occurrence vectors are the main method explored in the paper. Additionally, retrofitting vector method that incorporates ontological information into a vector representation is also explored.,", " Word embedding techniques such as word2vec, are explored in the paper. \n", " Word embedding techniques, \n", " Word embedding techniques such as word2vec are explored in the paper. \n", " word embedding techniques such as word2vec, second–order co–occurrence vectors, retrofitting vector method, and word embeddings, \n", " Word embedding techniques such as word2vec. ", " word2vec, Retrofitting vector method, Skip–gram and CBOW, and distributional techniques that typically start with a word by word co–occurrence matrix. ", " Word embedding techniques such as word2vec. \n", " Word embeddings, ", " Word embedding techniques such as word2vec. \n", " Word embedding techniques such as word2vec. \n", " Word2vec, second-order co-occurrence vectors, retrofitting vector method. \n", " Word embedding techniques such as word2vec and Skip-gram approach are explored in the paper. \n", " word2vec, \n", " Word2vec, Retrofitting vector method. \n", " Word embedding techniques such as word2vec, and retrofitting vector method, are explored. ", " Word embedding techniques, ", " Word2vec, \n", " Word embedding techniques, ", " Word embeddings techniques, ", " Word embedding techniques such as word2vec, second-order co-occurrence vectors, and retrofitting vector method are explored in the paper. ", " Word embeddings, ", " Word embedding techniques, ", " Word2vec, \n", " word embedding techniques such as word2vec. \n", " word2vec and retrofitting vector method. ", " word embedding techniques such as word2vec, Skip–gram, and CBOW. ", " Word embedding techniques, ", " Word embedding techniques. \n", " Word embedding techniques such as word2vec are explored in the paper. \n", " Word2vec, \n", " Word embedding techniques such as word2vec. \n", " Word embedding techniques such as word2vec and skip-gram approach, ", " Word embedding techniques such as word2vec. \n", " Word2vec and Retrofitting Vector Method, ", " word2vec and retrofitting vector method, ", " Word embedding techniques. ", " Word embeddings, ", " Word embedding techniques, ", " Word embedding techniques such as word2vec are explored. \n", " Word embedding techniques. ", " Word embedding techniques such as word2vec. \n"], "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": [" They were experts with legal training. \n", " Experts with legal training. \n", " \n", " Domain experts with legal training. \n", " They were seven experts with legal training. \n", " Legal experts with training. \n", " Experts with legal training. \n", " They were recruited seven experts with legal training. \n", " Experts with legal training, ", " Experts with legal training. \n", " Legal experts with training. ", " They had legal training. \n", " The experts were with legal training. \n", " Experts with legal training. \n", " Experts with legal training. \n", " Domain experts with legal training. \n", " Experts with legal training. \n", " Experts with legal training, ", " Domain experts with legal training. \n", " Domain experts with legal training. \n", " Domain experts with legal training. \n", " Seven experts with legal training. \n", " Experts with legal training, ", " Experts with legal training. \n", " Experts with legal training were used for annotation. ", " Experts with legal training. ", " They were experts with legal training. \n", " Domain experts with legal training. \n", " They were legal experts with seven in total. \n", " Experts with legal training. \n", " Domain experts with legal training. \n", " Domain experts with legal training. \n", " Experts with legal training. \n", " Experts with legal training. ", " Domain experts with legal training. \n", " Experts with legal training. \n", " They were recruited from a pool of experts with legal training. \n", " Experts with legal training. \n", " Experts with legal training. \n", " Seven experts with legal training. \n", " Experts with legal training. \n", " They were legal experts, \n", " Experts with legal training, \n", " Domain experts with legal training. ", " Experts with legal training. \n", " The experts with legal training. \n", " Experts with legal training. \n", " Experts with legal training, ", " Experts with legal training, ", " Experts with legal training, ", " Experts with legal training. \n", " Experts with legal training. \n", " The experts were with legal training. \n", " Experts with legal training. ", " Legal experts. \n", " Experts with legal training, ", " Experts with legal training. \n", " Experts with legal training, ", " Experts with legal training, ", " Experts with legal training were used for annotation. \n", " Experts with legal training. \n", " They were domain experts with legal training. \n", " They were seven experts with legal training. \n", " Domain experts with legal training. \n", " Experts with legal training. ", " They were legal experts. \n", " With legal training. \n", " Experts with legal training. ", " Experts with legal training. \n", " Experts with legal training. \n", " Legal experts with training, ", " Experts with legal training, ", " Experts with legal training, ", " Experts with legal training. \n", " Experts with legal training. \n", " Domain experts with legal training. \n", " Experts with legal training. ", " Experts with legal training. \n", " Experts with legal training. \n", " Domain experts with legal training. \n", " Legal experts. \n", " Experts with legal training were used for annotation. \n", " Domain experts with legal training. \n", " Experts with legal training, ", " The experts used for annotation were seven individuals with legal training. \n", " Domain experts with legal training. \n", " Legal experts with training. \n", " Experts with legal training were used for annotation. \n", " Legal experts, ", " Experts with legal training, ", " Domain experts with legal training. \n", " legal experts with seven domain experts who have legal training. \n", " Experts with legal training. \n", " Experts with legal training. \n", " Experts with legal training. \n", " Experts with legal training were used for annotation. \n", " Experts with legal training were used for annotation. \n", " Experts with legal training. \n", " Domain experts with legal training. \n", " Legal experts with training. \n"], "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": [" They apply generic rules and Hindi-tuned rules. \n", " They use a pre-ordering system that includes generic rules that apply to all Indian languages and Hindi-tuned rules to match words. \n", " They use a bilingual dictionary. ", " By using parse trees and generic/hindi-tuned rules. \n", " By using parse trees. \n", " They use generic rules or hindi-tuned rules. \n", " They match words using a bilingual dictionary, specifically Google Translate word translation. ", " By using generic and Hindi-tuned rules. ", " They match words based on a bilingual dictionary. ", " They use a bilingual dictionary. \n", " By using generic rules that apply to all Indian languages and bilingual dictionary, ", " By using a bilingual dictionary and word translation in their case. \n", " They use a generic set of rules that apply to all Indian languages and a Hindi-tuned set of rules that improve the generic rules for English to Hindi reordering. ", " They use a reordering rule to swap the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " Using re-ordering rules. \n", " They use generic rules that apply to all Indian languages and Hindi-tuned rules that improve the generic rules. ", " They use reordering rules. \n", " by using generic rules that apply to all Indian languages, and by incorporating improvements found through an error analysis. \n", " By swapping the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " They use bilingual dictionary for word-by-word translations or use CFILT-preorder system with two re-ordering systems: generic rules and Hindi-tuned rules. \n", " They use the CFILT-preorder system with generic and Hindi-tuned rules, \n", " By using generic and Hindi-tuned re-ordering rules. \n", " They use parse trees to identify the position of the noun phrase followed by a transitive verb. \n", " They swap the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " They use a set of generic re-ordering rules and also improve them with Hindi-tuned rules that were found through an error analysis. \n", " By using generic and Hindi-tuned rules. \n", " by using generic and hindi-tuned rules. \n", " Using generic rules that apply to all Indian languages or hindi-tuned rules that improve the generic rules. ", " They use re-ordering rules. \n", " They use a bilingual dictionary, Google Translate. \n", " Using pre-ordering rules. \n", " They use generic rules and Hindi-tuned rules for English-Hindi reordering. ", " By swapping the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " By using re-ordering systems that include generic rules and Hindi-tuned rules. \n", " They use pre-ordering rules. \n", " By using a bilingual dictionary to translate each word in the source language into English, and a CFILT-preorder system to reorder the English sentences. \n", " By using bilingual dictionary, such as Google Translate. \n", " By swapping the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " They use rules to reorder English sentences to match the Indian language word order, which is done using CFILT-preorder system with two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules. \n", " They use reordering rules, specifically a combination of generic rules and Hindi-tuned rules. \n", " By parsing them into a tree. \n", " They use bilingual dictionary and reordering rules. \n", " By pre-ordering the English sentences to match the word order of the source language, they use generic rules and Hindi-tuned rules to re-order them. \n", " They use generic and Hindi-tuned rules. \n", " They apply a rule that swaps the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " By swapping the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " they use a bilingual dictionary or Google Translate word translation. \n", " By applying reordering rules, \n", " They use bilingual dictionary. ", " By generating the pre-ordered parse trees. \n", " Using pre-ordering rules, ", " They use the CFILT-preorder system, which contains two re-ordering systems. ", " They apply re-ordering rules, ", " Using re-ordering rules. \n", " They use generic and Hindi-tuned rules, ", " They apply pre-ordering rules. \n", " Using generic and Hindi-tuned rules. \n", " Using generic rules, ", " They use a bilingual dictionary (Google Translate word translation). \n", " By using the parse trees and re-ordering rules. ", " They use generic and hindi-tuned rules. \n", " They use generic and Hindi-tuned reordering rules. \n", " By using parse trees, ", " Using re-ordering rules. ", " They match words using generic rules that apply to all Indian languages. ", " By applying reordering rules. \n", " They match words using a bilingual dictionary. \n", " They use pre-ordering rules. ", " Using generic rules and hindi-tuned rules. \n", " They match the noun phrase followed by a transitive verb with the transitive verb, \n", " They use bilingual dictionaries. \n", " They use pre-ordering rules based on the generic rules that apply to all Indian languages and hindi-tuned rules. \n", " Using pre-ordering rules, ", " By using generic rules that apply to all Indian languages and also Hindi-tuned rules. ", " Using generic rules that apply to all Indian languages, and/or hindi-tuned rules that incorporate improvements found through an error analysis of English-Hindi reordering. \n", " They use pre-ordering rules. ", " They use reorder rules that apply to all Indian languages, or specific hindi-tuned rules which improve the generic rules. ", " By applying reordering rules to the source language. \n", " Using a bilingual dictionary (Google Translate word translation), ", " They use pre-ordering rules. ", " Using generic rules and/or Hindi-tuned rules. ", " They match words using bilingual dictionary. \n", " Using generic rules and hindi-tuned rules. \n", " By using generic rules and also by incorporating improvements found through an error analysis, which includes Hindi-tuned rules. \n", " They use pre-defined generic rules or Hindi-tuned rules. \n", " They use a bilingual dictionary (Google Translate word translation) to translate each word in the source language into English. \n", " \n", " By applying pre-ordering rules. \n", " They use generic rules and Hindi-tuned rules for reordering English sentences. ", " They use a bilingual dictionary. \n", " One of the pre-ordering rule swaps the position of the noun phrase followed by a transitive verb with the transitive verb, another one is CFILT-preorder system which contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering. \n\n", " They use a re-ordering system to match words. \n", " They use pre-ordering rules and also bilingual dictionaries to match words. \n", " They use reordering rules. \n", " Using generic rules that apply to all Indian languages. \n", " They swap the position of the noun phrase followed by a transitive verb with the transitive verb. \n", " By using generic rules and Hindi-tuned rules. \n", " They use a bilingual dictionary, but not ideal for end-to-end solution. \n", " Using bilingual dictionary or Google Translate word translation. \n", " They use re-ordering rules to match words. \n"], "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": [" Model II', ", " The strong baseline established with monolingual data,  #10 in Table TABREF33. ", " #10, ", " no baseline is explicitly stated, ", " VII, ", " the weak baseline without using any monolingual data, ", " The weak baseline without using any monolingual data was a uni-directional M2M Transformer NMT model trained on the parallel corpus alone, and the strong baseline established with monolingual data was model #10 in Table TABREF33. \n\n", " The baseline is a weak NMT model without using any monolingual data. \n", " weak baseline without using any monolingual data, ", " the strongest model examined in the paper. \n", ", ", " the M2M Transformer NMT model (b3). ", " The M2M Transformer NMT model. ", " the weak baseline without using any monolingual data and the strong baseline established with monolingual data., ", " The baseline is weak and strong baselines. \n", " the M2M Transformer NMT model (b3), ", " model II', ", ", ", " V, ", " A weak baseline without using any monolingual data, ", " The M2M Transformer NMT model (b3). ", " The M2M Transformer NMT model. \n", " \n", " ", " the M2M Transformer NMT model (b3), ", " the M2M Transformer NMT model (b3) is the baseline. \n", " \n", " a weak baseline without using any monolingual data. ", " the model II', ", " The baseline NMT model without using any monolingual data, ", " ", " VII, ", " VII, ", " the model #10 in Table TABREF33, ", " the M2M Transformer NMT model (b3), ", " A weak baseline without using any monolingual data, and #10 in Table TABREF33, a strong baseline established with monolingual data, ", " a weak baseline without using any monolingual data, ", " 6.0, ", " The two baseline models. \n", " The weak baseline without using any monolingual data, ", " strong baseline, ", " model II' or the model trained on the balanced mixture of whole five types of corpora from scratch. ", " #10 in Table TABREF33 was a strong baseline. \n", " the baseline. \n", " baseline VII, ", " the baseline is a M2M Transformer NMT model (b3). ", " a strong baseline established with monolingual data. ", " A strong baseline established with monolingual data. ", " the baseline was the M2M Transformer NMT model (b3) and model #10, ", " #10 in Table TABREF33 \n", " the baseline was a strong model established with monolingual data, ", " the baseline models, ", " the two baseline models. ", " The strong baseline established with monolingual data., ", " the baseline, ", " #10 in Table TABREF33, ", " The M2M Transformer NMT model (b3) and model #10., ", " V and VII, ", "  A weak baseline without using any monolingual data, \n", " #10, ", " The strong baseline established with monolingual data. ", " #10 in Table TABREF33, ", " The baseline model. ", " The baseline was a strong NMT model trained on the mixture of in-domain and out-of-domain parallel data and monolingual data, ", " \n", " the M2M Transformer NMT model. \n", " baseline, ", " The baseline model. ", " weak baseline without using any monolingual data and #10 in Table TABREF33, a strong baseline established with monolingual data. ", " weak baseline without using any monolingual data, ", " the model VII, ", " (Answer should be a brief phrase or sentence summarizing the baseline) ", " \n", " strong baseline established with monolingual data., ", " the M2M Transformer NMT model (b3), ", " the M2M Transformer NMT model (b3) was a baseline for many of the results in the paper. ", " a strong baseline established with monolingual data, ", " The baseline was a weak model without using any monolingual data, and another baseline was a strong model established with monolingual data. \n", " 10 BLEU points, ", "  The baseline, referred to as #10 in Table TABREF33, was established with monolingual data. ", " The M2M Transformer NMT model (b3), ", " The baseline was trained on a balanced mixture of five types of corpora from scratch. ", " the M2M Transformer NMT model (b3) was a strong baseline, ", " VII, ", " the model trained on the balanced mixture of whole five types of corpora from scratch, ", " The baseline, ", " A weak baseline without using any monolingual data, ", "  the two baseline models, a weak baseline without using any monolingual data, and a strong baseline established with monolingual data, \n", " The baseline, ", " the uni-directional PBSMT models and the baseline NMT model without using any monolingual data, ", " The model with a BLEU score of #10 in Table TABREF33.  (Note: In the article, table numbers are labeled with an index, for example, TABREF33, TABREF27 etc.) \n", " Model (b3), ", " A weak baseline without using any monolingual data, ", " Model VII. ", ", ", " weak baseline without using any monolingual data, ", " the M2M Transformer NMT model (b3)  was the baseline. ", " The baseline, ", " a weak baseline without using any monolingual data and a strong baseline established with monolingual data. \n", " A weak baseline without using any monolingual data. \n"], "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": [" 1 percentage point, ", " \n", " higher quality with expert annotations, ", " \n", " It is not clear if the resulting data is higher quality, but the model trained on expert-annotated data outperforms the model trained on crowd annotations. ", " 2% higher F1.\n", " close to 10% improvement.\n", " 10 percentage points, ", " a difference of 1% in F1 score. \n", " Higher quality, ", " Significant. \n", " 68.1% higher. ", " 2%, ", " 3 to 4 percentage points. \n", " Unanswerable, \n", " about 3% higher, ", " \n", " 1%, \n", " Considerably higher, ", " Yes, higher quality is achieved by expert annotations.  \n", " Statistically significant. \n", " 68.1%, ", " The resulting annotated data from a combination of expert and lay annotators is higher quality than that from lay annotators alone, but the improvement is not quantified in terms of a specific percentage or value. However, it is noted that when the difficult instances are routed to domain experts, the F1 score is higher than when difficult instances are used, suggesting that the expert annotations are of higher quality. ", " Higher quality, ", "  The model trained with expert annotations achieves higher F1, indicating higher quality. The difference is less than 1% when expert annotations are used, but around 2% higher F1 when expert annotations are compared to crowdsourced annotations that are selected at i.i.d. random. \n", " \n", " Statistically significant, ", " High quality and more consistent annotations were observed. \n", " \n", " Significantly higher. \n", " 10 percentage points, ", " 68.1% \n\n", " less than 1% \n", " unanswerable, ", " The results clearly indicate that mixing expert and crowd annotations leads to better models than using solely crowd data, and better than using expert data alone. More importantly, there is greater gain in performance when instances are routed according to difficulty, as compared to randomly selecting the data for expert annotators. These findings align with our motivating hypothesis that annotation quality for difficult instances is important for final model performance. They also indicate that mixing annotations from expert and crowd could be an effective way to achieve acceptable model performance given a limited budget. \n\n", " Significantly,  \n", " 3 percentage points higher F1, but lower precision, with 10 percentage points higher recall.  \n", " 10 percentage points higher recall for i and o, ", " Higher. \n", " 10 percentage points. ", " Much higher, ", " 4 percentage points, ", " Significantly higher quality. \n", " 10 percentage points higher, ", " unanswerable \n", " The resulting annotated data with expert annotations is of much higher quality. \n", " Statistically significant under a sign test (p < 0.01). \n", " Much higher quality, ", " Significantly higher quality. \n", " 10 percentage points. \n", " Statistically significant under a sign test, ", " Up to 10% improvement, ", " Much higher. \n", " 10 percentage points, \n", " \n", " Unanswerable, ", " Unanswerable \n", " Significantly higher. \n", " unanswerable \n", " 10 percentage points. \n", " 10 percentage points, ", "  ", " higher quality, ", " Much higher, ", " \n", " Considerably higher quality. \n", " 3-4% higher in precision and 10 percentage points higher in recall. \n", " \n", " Unanswerable \n", " Unanswerable \n", " 11 percentage points in recall for both i and o. \n", " 10 percentage points \n", " Up to 68.1% in F1 score. \n", " \n", "  higher quality, ", " Higher. \n", " 10 percentage points, \n", " Much higher, ", " 4% higher than that trained on all crowd annotations. \n", " significantly higher. \n", " 10 percentage points. \n", " Unanswerable. ", " Statistically significant, \n\n", " No improvement was observed when re-annotating difficult instances. ", " 1%, ", " \n", " Higher quality, ", " It is unanswerable. ", " Consistently higher quality is achieved when difficult instances are routed to experts for annotation. ", " 1.07%, ", " No significant gain when compared with the initial quality of the data before annotation, ", " \n", " Significant. \n", " \n", " 10% \n\n", " No answer provided, unanswerable. \n\n", " High quality compared to the original crowd-sourced annotations. \n", " 20% improvement in F1 score, ", " High quality, ", " 2 percentage points, "], "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": [" An actor-critic architecture is used for painting embedding and seq2seq models with attention are used for language style transfer. \n", " An actor-critic architecture and seq2seq model with attention, respectively. \n", " Two models are used for painting embedding and one for language style transfer, ", " Image-to-poem actor-critic model, ", " Actor-Critic model and seq2seq models. \n", " Image-to-poem actor-critic model and seq2seq with global attention and pointer network are used for painting embedding and language style transfer respectively. \n", " The actor-critic model and sequence-to-sequence model with attention are used for painting embedding and language style transfer, respectively. \n", " Image-to-poem actor-critic model and Seq2Seq models, ", " An actor-critic architecture and a sequence-to-sequence model with global attention, ", " The actor-critic architecture model is used for painting embedding, and the seq2seq models with global attention are used for language style transfer. \n", " A CNN-RNN based model is used for painting embedding and a sequence-to-sequence model with parallel text corpus is used for language style transfer. \n", " Image to Poem Actor-Critic Model for painting embedding, and Seq2Seq with Attention or Pointer Network for language style transfer. \n", " Two separately trained models: a CNN-RNN model for image to poem task and a seq2seq model with pre-trained retrofitted word embeddings for style transfer task. \n", " An actor-critic model and sequence-to-sequence models are used. \n", " Image-to-poem actor-critic model and a seq2seq model, ", " A CNN-RNN actor-critic architecture is used for painting embedding and a sequence-to-sequence model with pre-trained retrofitted word embeddings is used for language style transfer. \n", " Actor-Critic Model and Seq2Seq Model with Global Attention, ", " \n", " Image-to-poem actor-critic model is used for painting embedding and a sequence-to-sequence model is used for language style transfer. \n", " The actor-critic architecture and CNN-RNN generative model are used for painting embedding, and the seq2seq model is used for language style transfer. \n", " Image-to-poem actor-critic model and sequence-to-sequence models with global attention. \n", " Image-to-poem actor-critic model and seq2seq with global attention respectively. ", " Image to poem actor-critic model and sequence-to-sequence model respectively. \n", " An actor-critic architecture is used for painting embedding, while a sequence-to-sequence model is used for language style transfer. \n", " A CNN-RNN model and a seq2seq model with global attention are used for painting embedding and language style transfer respectively, \n", " The actor-critic architecture is used for painting embedding and the sequence-to-sequence models with attention for language style transfer. \n", " Two separate models, \n", " Two separately trained models are used: one for painting embedding and another for language style transfer. \n", " The models used for painting embedding are CNN-RNN, while the models used for language style transfer are seq2seq models. \n", " Actor-Critic architecture and seq2seq models, ", " Image To Poem Actor-Critic Model and sequence-to-sequence models with attention and pointer networks, respectively, are used. \n", " \n", " Actor-critic model, sequence-to-sequence model with global attention and pointer networks, and generative adversarial networks are used for painting embedding and language style transfer respectively, \n", " A CNN-RNN based image-to-poem net and sequence-to-sequence model with parallel text corpus, respectively, are used. \n", " Actor-Critic Model and Seq2Seq with global attention and pointer networks, respectively. \n", " The CNN-RNN generative model and the sequence-to-sequence model with global attention are used for painting embedding and language style transfer respectively, ", " A CNN-RNN based image-to-poem net and a seq2seq model, respectively, ", " Two CNNs are used for image embedding, and a seq2seq model is used for language style transfer. \n", " The actor-critic architecture and a CNN-RNN generative model are used for painting embedding, while a sequence-to-sequence model is used for language style transfer. \n", " CNN-RNN and Seq2Seq, \n", " Image-to-poem actor-critic model and seq2seq model, respectively. \n", " Image-to-poem actor-critic model and seq2seq models, ", " A CNN-RNN actor-critic model is used for painting embedding and a seq2seq model is used for language style transfer, ", " A CNN-RNN based image-to-poem net is used for painting embedding, and a seq2seq model with global attention is used for language style transfer, ", " The actor-critic architecture is used for painting embedding, while the seq2seq model with global attention is used for language style transfer. \n", " Image-to-poem actor-critic model and a sequence-to-sequence model, respectively. \n", " Image To Poem Actor-Critic Model for painting embedding, ", " \n", " The CNN-RNN actor-critic model is used for painting embedding and sequence-to-sequence (seq2seq) models with attention are used for language style transfer. \n", " \n", " An actor-critic architecture for painting embedding and a sequence-to-sequence model with global attention for language style transfer. \n", " The actor-critic architecture is used for painting embedding, while a sequence-to-sequence model with pre-trained retrofitted word embeddings is used for language style transfer. \n", " An actor-critic architecture is used for painting embedding and a sequence-to-sequence model with attention is used for language style transfer. \n", " CNN-RNN and seq2seq models are used respectively, \n", " \n", " The actor-critic model and CNN-RNN generative model are used for painting embedding, and the sequence-to-sequence model with pre-trained retrofitted word embeddings are used for language style transfer. \n\n", " Image To Poem Actor-Critic Model and sequence-to-sequence model, respectively, ", " The image-to-poem net, which is a CNN-RNN generative model, is used for painting embedding, and the seq2seq model is used for language style transfer. \n", " An actor-critic model is used for painting embedding, and a sequence-to-sequence model with global attention is used for language style transfer, ", " The actor-critic architecture is used for painting embedding, while the sequence-to-sequence model with attention is used for language style transfer. \n", " \n", " A CNN-RNN model is used for painting embedding and a seq2seq model with global attention for language style transfer. \n", " The image-to-poem actor-critic model is used for painting embedding, and the seq2seq model with global attention is used for language style transfer, ", " Image-to-poem actor-critic model and sequence-to-sequence models. \n", " \n", " \n", " The models used for painting embedding include CNN-RNN actor-critic architecture, while for language style transfer, a seq2seq model with attention is used, specifically a seq2seq model with global attention.\n\n", " Image To Poem Actor-Critic Model and a Seq2Seq model with global attention or pointer networks, \n", " CNN-RNN actor-critic architecture and sequence-to-sequence models, respectively. \n", " The models used are a CNN-RNN generative model and a sequence-to-sequence model. \n", " CNN-RNN, sequence-to-sequence models. \n", " The models for painting embedding are actor-critic architecture with 3 parallel CNNs and CNN-RNN generative model, while for language style transfer are sequence-to-sequence models with attention, and pointer networks. \n", " Image-to-poem actor-critic model and sequence-to-sequence model with pre-trained retrofitted word embeddings, ", " \n", " Actor-critic and CNN-RNN models are used for painting embedding, while sequence-to-sequence and pointer network models are used for language style transfer. \n", " CNN-RNN and seq2seq models, \n", " Image-to-poem actor-critic model and sequence-to-sequence models with attention. \n", " \n", " A CNN-RNN actor-critic model and a sequence-to-sequence model are used for painting embedding and language style transfer, respectively. ", " The CNN-RNN actor-critic model is used for painting embedding, and the sequence-to-sequence model with a dictionary for language style transfer, ", " CNN-RNN model is used for painting embedding and sequence-to-sequence (seq2seq) model for language style transfer. \n", " The image-to-poem actor-critic model is used for painting embedding, while the seq2seq model with attention is used for language style transfer. \n", " Image to Poem Actor-Critic Model and Seq2Seq, ", " Actor-critic architecture and Sequence-to-Sequence model are used respectively. \n", " A CNN-RNN model for painting embedding and a seq2seq model for language style transfer. \n", " An actor-critic architecture for painting embedding, and a sequence-to-sequence model with attention for language style transfer, ", " Image-to-poem actor-critic model and seq2seq model with global attention, ", " The CNN-RNN actor-critic model is used for painting embedding and the seq2seq model is used for language style transfer. \n", " Image To Poem Actor-Critic Model and a sequence-to-sequence model with a dictionary. \n", " The CNN-RNN based image-to-poem net is used for painting embedding and the seq2seq model is used for language style transfer. \n", " CNN-RNN and seq2seq with global attention, ", " The CNN-RNN actor-critic model is used for painting embedding, while sequence-to-sequence models with attention are used for language style transfer, ", " The image-to-poem actor-critic model and a sequence-to-sequence model with a pointer network are used for painting embedding and language style transfer respectively. However, the article actually uses a sequence-to-sequence model with global attention for language style transfer. \n", " A CNN-RNN generative model is used for painting embedding and a sequence-to-sequence model is used for language style transfer. \n", " Actor-critic architecture and sequence-to-sequence model, ", " Image To Poem Actor-Critic Model is used for painting embedding and Seq2Seq models are used for language style transfer. \n", " Two separately trained models, ", " Image-to-poem actor-critic model and sequence-to-sequence models, ", " \n", " CNN-RNN actor-critic model is used for painting embedding and sequence-to-sequence (seq2seq) model for language style transfer. \n"], "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": [" ToBERT works better, ", " ToBERT works better, ", " ToBERT works better, ", " ToBERT works better, ", " Transformer layer, ", " ToBERT, ", " The transformer layer works better, ", " Transformer layer works better, ", " ToBERT works better, ", " ToBERT works better, \n", " The transformer layer works better. \n", " ToBERT works better. \n", " The Transformer layer works better. \n", " ToBERT, ", " Transformer, ", " ToBERT works better \n", " ToBERT works better, \n", " ToBERT, \n", " The transformer layer works better, ", " ToBERT works better, ", " ToBERT works better, \n", " The transformer layer, \n", " ToBERT works better, ", " Transformer layer works better. \n", " ToBERT works better, ", " ToBERT works better, ", " ToBERT outperforms RoBERT on Fisher and 20newsgroups dataset by 13.63% and 0.81% respectively. \n", " ToBERT, ", " ToBERT works better, ", " The transformer layer works better. \n", " ToBERT (Transformer over BERT) works better than RoBERT (RNN over BERT), \n", " Transformer layer, ", " ToBERT, ", " ToBERT works better. \n", " ToBERT works better, ", " The transformer layer works better, ", " The transformer layer works better. \n", " The transformer layer works better, ", " Transformer layer works better, \n", " ToBERT works better than RoBERT on pre-trained BERT features and fine-tuned BERT features for all tasks, except on CSAT where it performs slightly worse but not statistically significant. ", " Transformer layer, ", " The Transformer layer. ", " The transformer layer works better. \n", " The transformer layer works better on top of BERT, \n", " ToBERT works better. \n", " The transformer layer, ", " The transformer layer works better. \n", " The transformer layer works better, ", " ToBERT, ", " ToBERT works better than RoBERT. \n", " Transformer layer works better, ", " ToBERT works better than RoBERT, \n", " ToBERT, ", " ToBERT works better. \n", " The transformer layer works better. \n", " The transformer layer works better. \n", " ToBERT works better, ", " ToBERT outperforms RoBERT, \n", " ToBERT, ", " ToBERT works better, ", " Transformer layer, \n", " The transformer layer works better, ", " ToBERT works better. \n", " ToBERT works better, ", " The transformer layer works better. \n", " The transformer layer, ", " ToBERT works better, ", " The transformer layer works better. \n", " ToBERT outperforms RoBERT, \n", " The Transformer layer works better. \n", " ToBERT works better. \n", " Transformer layer works better, ", " The Transformer layer works better, ", " Transformer layer works better. \n", " ToBERT, ", " Transformer layer works better, ", " ToBERT (Transformer) works better. \n", " The transformer layer, ", " The transformer layer works better. \n", " ToBERT (Transformer over BERT) outperforms RoBERT (RNN over BERT) in most of the tasks. \n", " ToBERT (transformer layer) works better than RoBERT (RNN layer), \n", " ToBERT, ", " ToBERT, ", " ToBERT works better. \n", " The transformer layer works better. \n", " The transformer layer, ", " The transformer layer works better. \n", " The transformer layer works better, \n", " ToBERT outperforms RoBERT on Fisher and 20newsgroups dataset. \n", " ToBERT, ", " ToBERT works better, \n", " The transformer layer works better. \n", " ToBERT outperforms RoBERT, \n", " ToBERT works better, \n", " ToBERT, ", " ToBERT, ", " ToBERT works better than RoBERT, \n", " ToBERT, ", " The transformer layer works better, \n", " The transformer layer works better, "], "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": [" By combining the left context, the left entity and the middle context, and the middle context, the right entity and the right context. \n", " By using the extended middle context that focuses on all parts of the sentence. \n", " They propose extended middle context. ", " By combining the left context, the left entity and the middle context, and the middle context, the right entity and the right context. \n", " By splitting the context into three disjoint regions: left, middle, and right context. \n", " They propose extended middle context. ", " By splitting the context into three disjoint regions based on the two relation arguments. ", " They propose extended middle context, a new context representation for CNNs for relation classification. \n", " By using two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. \n", " They combine two contexts. \n", " They split the context into three disjoint regions based on the two relation arguments and then focus on the middle part. \n", " By combining the left context, the left entity, the middle context, the right entity and the right context. \n", " By combining the left context, the left entity and the middle context; and the middle context, the right entity and the right context. \n", " They obtain it by combining the left context, the left entity and the middle context, and combining the middle context, the right entity and the right context. \n", " By splitting the context into two parts and repeating the middle context. \n", " By combining the left context, the left entity and the middle context, and the middle context, the right entity and the right context. ", " They propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. \n", " They combine left context, left entity, middle context, middle entity, right context, and right entity. \n", " They use two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context.\n", " By splitting the context into three disjoint regions. \n", " They use a combination of the left context, the left entity and the middle context, and a combination of the middle context, the right entity and the right context. \n", " By proposing extended middle context. \n", " By concatenating two independent convolutional and max-pooling layers, after pooling, the results are concatenated to form the sentence representation. \n", " They split the context into three disjoint regions, ", " by proposing the extended middle context. \n", " By combining the left context, the left entity, and the middle context, and the middle context, the right entity, and the right context. \n", " They use the extended middle context. \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " By splitting the context into three disjoint regions: left, middle, and right, and combining two contexts: (1) left, left entity, and middle context; and (2) middle context, right entity, and right context. \n", " By using the left context, the middle context, the right context, the left entity and the right entity. \n", " by splitting the context into three disjoint regions. ", " They use the extended middle context, which combines the left context, the left entity, the middle context, the right entity, and the right context. \n", " They propose to use two contexts. \n", " They obtain the new context representation by combining the left context, the left entity and the middle context, and then processing the results through two independent convolutional and max-pooling layers. \n", " Using two independent convolutional and max-pooling layers. \n", " They use two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n", " They propose extended middle context. \n", " By splitting the context into three disjoint regions. \n", " By splitting the contexts into three disjoint regions based on the two relation arguments and combining two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " By combining the left context, the left entity, and the middle context, and also the middle context, the right entity, and the right context. \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " By concatenating the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n", " They propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. \n", " By combining the left context, the left entity and the middle context and the combination of the middle context, the right entity and the right context, and then repeating the middle context. \n", " They propose extended middle context, ", " They use a combination of the left context, the left entity, and the middle context.  ", " By proposing extended middle context, a new context representation for CNNs. \n", " By splitting the contexts into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context, and combining the middle context with the left entity and the right entity. \n", " They obtain the new context representation by splitting the context into three disjoint regions based on the two relation arguments and paying special attention to the middle part. \n", " They split the contexts into three disjoint regions based on the two relation arguments. ", " By splitting the context into two parts and repeating the middle context. \n", " They obtain the new context representation by combining the left context, the left entity and the middle context. \n", " By combining two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. \n", " They propose to use two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. ", " By splitting the context into two parts. \n", " By using the left context, the left entity and the middle context. ", " By using two independent convolutional and max-pooling layers to process two disjoint regions of the sentence. \n", ", ", " \n", " By using two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. \n", " They use two contexts. \n", " By combining the left context, the left entity, and the middle context. ", " By splitting the context into three disjoint regions and combining two of them with the middle context. \n", " They propose extended middle context. \n", " By proposing extended middle context, a new context representation for CNNs for relation classification. \n", " They use a combination of the left context, the left entity and the middle context and a combination of the middle context, the right entity and the right context. \n", " By combining the left context, the left entity and the middle context, and also by combining the middle context, the right entity and the right context. ", " By splitting the context into two parts and repeating the middle context. ", " They obtain the new context representation by combining the left context, the left entity, and the middle context, and combining the middle context, the right entity, and the right context. \n", " They obtain the new context representation by splitting the contexts into three disjoint regions based on the two relation arguments: the left context, the middle context, and the right context. \n", " By combining the left context, the left entity and the middle context, and combining the middle context, the right entity and the right context. \n", " \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " They combine the left context, the left entity and the middle context; and the middle context, the right entity and the right context. \n", " They use two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n", " They propose the extended middle context. \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " They propose extended middle context. \n", " By combining the left context, the left entity and the middle context and the middle context, the right entity and the right context. \n", " They propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. ", " They use the extended middle context. \n", " By using the combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n", " They propose to use two contexts, a combination of the left context, the left entity and the middle context, and a combination of the middle context, the right entity and the right context. \n", " They use a combination of the left context, the left entity and the middle context and a combination of the middle context, the right entity and the right context. \n", " They obtain the new context representation by combining left context, left entity, and middle context. \n", " By combining the left context, the left entity and the middle context, and by combining the middle context, the right entity and the right context. \n", " They combine two independent convolutional and max-pooling layers for left and right context. \n", " They split the context into three disjoint regions, and force the network to pay special attention to the middle part. \n", " By using two independent convolutional and max-pooling layers on the combination of the left context, the left entity and the middle context and the combination of the middle context, the right entity and the right context. \n", " They obtain the new context representation by using a combination of the left context, the left entity and the middle context, and a combination of the middle context, the right entity and the right context. \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " Using a combination of the left context, the left entity, and the middle context. \n", " By using all parts of the sentence and paying special attention to the middle part. \n", " By splitting the context into three disjoint regions based on the two relation arguments. \n", " By combining the left context, the left entity and the middle context, and also combining the middle context, the right entity and the right context. \n", " By combining the left context, the left entity and the middle context, and combining the middle context, the right entity and the right context. \n", " They obtain it by splitting the context into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. \n", " By splitting the context into three disjoint regions based on the two relation arguments and using two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n", " By combining two contexts, a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context. \n"], "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": [" 65%, \n", " 65%, ", "  The imbalance is about 65%, with men being in the majority, and 35% with women being in the minority. ", " 65% of the speakers are men, speaking more than 75% of the time. \n", " \n", " Unanswerable \n", " 65% of the speakers are men, speaking more than 75% of the time. \n", " 24% increase in WER for women compared to men. \n", " 65%, ", " The imbalance is very big, ", " 65%, ", " 65% men vs. 33.16% women. \n", " 65%, ", " 65%, ", " ", " Huge. \n", " The imbalance is huge. ", " 65% men, 33.16% women, ", "  ", " The imbalance in analyzed corpora is significant, with men speaking more than 75% of the time. \n", "  The imbalance is huge, with 65% men and 75% male speech time. \n", " 65% men, 33.16% women, ", " Significant, ", " The gender disparity is huge, with men speaking 75% of the time and women only 22.57% of the total speech time. \n", " 65%, ", " Significant. \n", " 31.8%, ", " 65%, ", " 65% men, 35% women, ", " Imbalance is significant. \n", " 65%, ", " Huge. \n", " 65% of men and 35% of women, ", " There is a 65% of men in analyzed corpora, ", "  in Answer  you should only provide the answer to the question, in ", " The imbalance is substantial, with men representing 65% of the speakers and speaking 75% of the time. \n", " Large. \n", " 24% and 31.8%, ", " The imbalance in analyzed corpora is 65% men to 35% women. \n", " There is an imbalance of gender representation in analyzed corpora with 65% of the speakers being men. \n", " 65%, ", " 65% men, 35% women. \n", " The imbalance is significant, with 65% of speakers being men, speaking more than 75% of the time.  ", " 65%, ", " Significant, ", " 65%, ", " 65% of men. \n", " The imbalance is huge. \n", " Imbalance is 65%, \n", " 27.2% higher error rate for women compared to men. \n", " The gender imbalance in the corpora is 65% of the speakers are men. 75% of the time is spoken by men. And 25% of the time is spoken by women., ", " A 27.2% WER increase for female speakers compared to male speakers. \n", " Huge, ", " 65%, ", " \n", " 65%, ", " 65%, ", " 65%, ", " Huge variation is observed. \n", " 65% of men are speakers. ", " 31.8%, ", " The imbalance is significant, with 65% of speakers being men. \n", " Big. \n", " The imbalance in analyzed corpora is huge. \n", " 65%, ", " 65% of the speakers are men. \n", " 65%, ", " 24%, ", " The imbalance is huge with 65% of the speakers being men and 33.16% being women, representing the speakers, and 75% of the speech time being men and 22.57% being women, in terms of speech time. \n", " The imbalance is 24% between men and women in terms of WER scores. \n", " Huge, \n", " Imbalance is around 65% of the speakers are men, speaking more than 75% of the time, with women speaking only 22.57% of the total speech time. \n", " 65%, ", " Big, \n", " 65%, ", " The imbalance is significant. \n", " Significant. ", " 31.8%, ", " 65% men. \n", " The analyzed corpora show a 65% of men and 35% of women imbalance, with men speaking more than 75% of the time. \n", " The imbalance in analyzed corpora is significant. \n", " Significant. \n", " Imbalance in corpora is significant, with men speaking 75% of the time and women 33.16% of the speakers, while women account for 22.57% of the total speech time.\n", " Unanswerable, ", " A significant imbalance. \n", " 65% men, 35% women, ", " The imbalance is significant. \n", " The gender imbalance is 65% men speaking more than 75% of the time. \n", " Imbalance is huge. \n", " 27.2%, ", " The imbalance in the analyzed corpora is large. ", " Huge, ", " Imbalance is huge, ", " 65%, ", " 65%, \n", " Huge, ", " The imbalance is huge. \n", " 65%, ", " The imbalance in analyzed corpora is huge. \n", " The imbalance is significant with 65% of the speakers being men.  \n"], "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": [" 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033. ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033. \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033. ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, \n", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033, ", " 0.7033. \n", " 0.7033, ", " 0.7033, "], "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": [" Four different types of entities exist in the dataset, ", " Four, ", " 4, ", " Four types, ", " 4 types of entities, ", " Four, \n", " 4, ", " 4, ", " Four, ", " 4, ", " 4, ", " Four, ", " Four different types of entities exist in the dataset: PER, LOC, ORG, and MISC. ", " 4, ", " 4 types of entities (PER, LOC, ORG, MISC) exist in the dataset. \n", " 4, ", " 4, ", " 4 types, ", " 4, ", " 4, ", " 4, ", " 4, ", " Four, ", " 4 types of entities exist in the dataset, ", " Four different types of entities exist in the dataset: PER (Person), LOC (Location), ORG (Organization) and MISC., ", " Four, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " Four, ", " 4, ", " 4, ", " Four, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " Four, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " Four, ", " Four types, ", " 4, ", " 4, ", " 4, ", " Four, ", " 4, ", " Four, ", " 4 types, ", " 4, ", " 4, ", " Four, ", " Four, ", " Four, ", " 4, ", " 4, ", " Four, ", " 4 different types of entities exist in the dataset. \n", " 4, ", " Four, ", " Four, ", " Four, ", " 4, ", " 4, ", " 4, ", " Four, ", " 4 different types of entities exist in the dataset.  \n", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " 4, ", " There are four different types of entities in the dataset. \n", " Four, ", " Four types, ", " 4, ", " 4, ", " Four, ", " 4, ", " 4, "], "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": [" yes ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, \n", " Yes, ", " yes, ", " yes, ", " yes, ", " yes \n", " yes, ", " yes, ", " yes, ", " yes \n", " yes ", " yes, ", " yes \n", " yes ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes \n", " Yes, \n", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes \n", " yes, ", " Yes, \n", " yes, ", " yes \n", " yes, ", " Yes. ", " yes, ", " yes, \n", " yes, ", " yes \n", " Yes, ", " yes \n", " yes \n", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes \n", " yes, ", " Yes, ", " Yes. ", " yes \n", " Yes, ", " yes, ", " yes \n", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, \n", " Yes, ", " \n", " yes, ", " yes, ", " Yes, ", " yes, ", " yes \n", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, \n", " yes, ", " yes \n", " yes, ", " yes ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " \n", " Yes, ", " yes, ", " yes ", " yes, ", " yes, ", " yes, ", " yes \n", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes \n", " Yes, ", " Yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": [" yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, they do. \n", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, they use attention. \n", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": [" English-German dataset, ", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " The English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " The Multi30K dataset, ", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset, ", " The English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset \n", " The English-German dataset, ", " English-German dataset, ", " The English-German dataset. \n", " English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset. ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " The English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset, ", " The Multi30K dataset \n", " English-German dataset. \n", " English-German dataset, ", " English-German, ", " The English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset, ", " The English-German dataset. \n", " English-German dataset. \n", " English-German dataset, ", " The English-German dataset. ", " The English-German dataset. \n", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. ", " English-German dataset, ", " English-German dataset, ", " English-German dataset. ", " English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " The Multi30K dataset for English-German translation. \n", " English-German dataset. \n", " The English-German dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset, ", " English-German dataset, ", " The Multi30K dataset. \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset, ", " English-German dataset. ", " English-German dataset, ", " English-German dataset. \n", " English-German dataset. \n", " English-German dataset \n", " English-German dataset, ", " English-German dataset, ", " English-German dataset, ", " English-German dataset \n"], "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": [" BIBREF20. \n", " Previous state-of-the-art models. \n", " BIBREF20, ", " The previous state-of-the-art models, ", " recent neural models, ", " Recent neural models, ", " Other LSTM models, ", " recent neural models, ", " Recent neural models, ", " SIGHAN Bakeoff 2005. ", " Recent models. \n", " unanswerable, ", " Models with various $n$-gram features. \n", " recent models \n", " Other recent models, ", " Recent models, ", " models from previous work, ", " Models from SIGHAN Bakeoff. \n", " Other recent models. ", " Strong baselines, \n", " BIBREF18, ", " Recent neural models, ", " BIBREF20, ", " Recent models, ", " Models with n-gram features, ", " BIBREF18, ", " Previous state-of-the-art models. \n", " Neural CWS models, ", " Our model, ", " BIBREF4, ", " BIBREF20, ", " Models using multiple n-gram features. \n", " Our model. \n", " Our proposed model, ", " Our model, ", " SIGHAN Bakeoff 2005 datasets. \n", " Strong baselines models, \n", " Other recent neural models, \n", " Other LSTM models, ", " BIBREF20 model, ", " Recent neural models, ", " Recent models, ", " Other models, ", " Our model, ", " Models using multiple n-gram features. \n", "  our model, ", " Other recent neural models. \n", " unanswerable, \n", " Our model, \n", " Recent neural models, \n", " Previous state-of-the-art models, ", " unanswerable, ", " The neural CWS models proposed in BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, and BIBREF13. \n", " Strong baselines model is compared to other neural models, ", " BIBREF9 and BIBREF11. \n", " Our model. \n", " BIBREF2, BIBREF4, BIBREF9, BIBREF10, BIBREF11, BIBREF17, BIBREF18, BIBREF20, BIBREF20, BIBREF33, BIBREF34, BIBREF35, BIBREF36, BIBREF37, BIBREF9, BIBREF11, BIBREF18, BIBREF20, BIBREF35, BIBREF36, BIBREF37, BIBREF21, BIBREF24, BIBREF25, BIBREF26,", " The neural CWS models. \n", " BIBREF20, ", " LSTM models, ", " Models that use various n-gram features, \n", " BIBREF18, ", " Yes, recent neural models are strong baselines compared to. \n", " SIGHAN Bakeoff models, ", " The previous models, ", " Previous state-of-the-art models, ", " The CWS models in BIBREF21, BIBREF18, BIBREF20, BIBREF9, BIBREF11, BIBREF34, BIBREF35, BIBREF36, and BIBREF37. \n", " Other neural models, ", " BIBREF18, \n", " unanswerable, ", " BIBREF18, BIBREF20, BIBREF2, BIBREF11, BIBREF4, BIBREF6, BIBREF7, BIBREF9, BIBREF17, BIBREF33, BIBREF34, BIBREF35, BIBREF23, BIBREF36, BIBREF37 \n", " \n", " Models with n-gram features, ", " Recent neural models, ", " BIBREF4 and BIBREF2. \n", " Yes, ", " Models with multiple n-gram features, ", " Models with diverse n-gram features, ", " Recent neural models, ", ", ", " recent models, ", " BIBREF2, BIBREF11, BIBREF20, BIBREF18, BIBREF33, BIBREF34, BIBREF35, BIBREF17, BIBREF9. \n", " Models with various n-gram features. \n", " The models that are only using unigram features and greedy decoding., ", " Our model, ", " Models such as BIBREF20, BIBREF18 and others. ", " Recent models, ", " Yes, recent neural models, ", " recent models, ", " Models that adopt various n-gram features. \n", " Other LSTM models, ", " BIBREF18 and the state-of-the-art model on MSR and AS, \n", " Recent models \n", " Strong neural baselines, ", " BIBREF18 and previous state-of-the-art models, ", " Unigram features. \n", " BIBREF18, \n", " BIBREF18, BIBREF20. \n", " Other neural CWS models, ", " Neural CWS models, "], "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": [" SQuAD dataset. ", " SQuAD, ", " SQuAD dataset. \n", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset. \n", " SQuAD, ", " SQuAD dataset. \n", " SQuAD dataset, ", " SQuAD. \n", " SQuAD dataset, ", " SQuAD dataset. ", " SQuAD dataset. \n", " SQuAD, ", " SQuAD dataset. \n", " SQuAD dataset, ", " SQuAD. \n", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset. \n", " SQuAD dataset. \n", " SQuAD, ", " SQuAD, ", " SQuAD dataset. ", " SQuAD, \n", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD, \n", " SQuAD dataset, ", " SQuAD. ", " SQuAD dataset, ", " SQuAD dataset. \n", " SQuAD, ", " SQuAD, ", " SQuAD, ", " SQuAD, \n", " SQuAD dataset, ", " SQuAD, ", " SQuAD, \n", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, \n", " SQuAD, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD. \n", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset. \n", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD, ", " SQuAD, ", " SQuAD, ", " SQuAD dataset, \n", " SQuAD, ", " SQuAD dataset. \n", " SQuAD dataset, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, ", " SQuAD, ", " SQuAD dataset, "], "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": [" CSAT, 20 newsgroups, and Fisher, ", " CSAT, 20 newsgroups, and Fisher, ", " 3 datasets: CSAT, 20 newsgroups and Fisher, \n", " CSAT, 20 newsgroups, Fisher, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, Fisher Phase 1 corpus. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. ", " CSAT, 20newsgroups, and Fisher. \n", " 3 datasets: CSAT, 20newsgroups and Fisher, \n", " CSAT, 20 newsgroups, and Fisher. ", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification, Fisher Phase 1 corpus for topic identification. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task. \n", " 3 different datasets. \n", " Three datasets: CSAT, 20newsgroups, and Fisher Phase 1 corpus. \n", " CSAT, 20 newsgroups, Fisher, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, \n", " 3 different datasets, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, ", " 3 datasets: CSAT, 20newsgroups and Fisher. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, Fisher Phase 1 corpus, \n", " 20 newsgroups, CSAT dataset for CSAT prediction, and Fisher Phase 1 corpus for topic identification, ", " CSAT, 20 newsgroups and Fisher. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task. ", " 3 datasets: CSAT, 20newsgroups, and Fisher, ", " CSAT, 20newsgroups, and Fisher, ", " CSAT, 20 newsgroups, and Fisher. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " 3 datasets, including CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. ", " CSAT, 20 newsgroups and Fisher, ", " Three datasets, \n", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, and Fisher. \n", " 3 datasets: CSAT, 20newsgroups and Fisher, ", " 3 datasets: CSAT, 20 newsgroups, and Fisher. \n", " CSAT dataset, 20 newsgroups dataset, and Fisher Phase 1 corpus. \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, Fisher Phase 1 corpus for topic identification task, \n", " CSAT, 20 newsgroups and Fisher, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus. \n\n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, Fisher Phase 1 corpus. \n", " Three datasets: CSAT, 20 newsgroups, and Fisher, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT, 20 newsgroups, and Fisher, ", " CSAT, 20newsgroups, and Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts., ", " CSAT, 20 newsgroups, Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus. \n", " CSAT, 20newsgroups, and Fisher. \n", " CSAT, 20newsgroups, and Fisher, ", " CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, ", " CSAT, 20 newsgroups, Fisher Phase 1 corpus. \n", " CSAT, 20 newsgroups, and Fisher, ", " CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus. \n", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus, ", " 20 newsgroups, CSAT, Fisher Phase 1 corpus. \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task. \n", "  CSAT, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus, \n", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification, and Fisher Phase 1 corpus for topic identification., ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus. \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification, and Fisher Phase 1 corpus for topic identification. \n", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups and Fisher Phase 1 corpus. ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus, ", " 3 different datasets: CSAT, 20 newsgroups, and Fisher Phase 1 corpus, ", " 3 datasets, CSAT, 20 newsgroups, and Fisher Phase 1 corpus, were used for evaluation. \n", " Three datasets, namely CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task. \n", " 3 datasets (CSAT, 20 newsgroups, and Fisher), ", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, Fisher Phase 1 corpus for topic identification task. \n", " CSAT, 20newsgroups, and Fisher, ", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. \n", " 3 datasets, ", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification, Fisher Phase 1 corpus for topic identification. \n", " CSAT dataset, 20 newsgroups dataset, and Fisher Phase 1 corpus. \n", " CSAT dataset for CSAT prediction, 20 newsgroups for topic identification task, and Fisher Phase 1 corpus for topic identification task. \n", " CSAT dataset, 20 newsgroups, and Fisher Phase 1 corpus. \n", " CSAT, 20 newsgroups, Fisher Phase 1 corpus. \n", " CSAT, 20 newsgroups, and Fisher Phase 1 corpus. \n"], "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": [" Racism, sexism, and personal attacks. ", " Personal attack, racism, sexism, and personal attack, sexism, and personal attack, racism, and sexism. ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attack, ", " Three topics of cyberbullying, \n", " racism, sexism, and personal attack, ", " racism, sexism, and personal attacks, ", " personal attack, racism, and sexism, ", " Racism, sexism, and personal attack, ", " Racism, sexism, and personal attack. ", " personal attack, racism, and sexism, ", " Three topics of cyberbullying (personal attack, racism, and sexism). \n", " Racism, sexism, personal attack, \n", " Three types of cyberbullying, namely personal attack, racism, and sexism. \n", " racism, sexism, and personal attack, ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attack. ", " Personal attack, racism, and sexism. \n", " Racism, sexism, and personal attack, ", " racism, sexism, personal attacks, \n", " racism, sexism, and personal attack, ", " personal attack, racism, and sexism, ", " Three topics: personal attack, racism, and sexism. \n", " Racism, sexism, and personal attack, ", " Racism, sexism, personal attack, and bullying in general. \n", " Racism, sexism, and personal attack, ", " Three types of cyberbullying. \n", " racism, sexism, and personal attack, ", " Racism, sexism, and personal attacks. \n", " Racism, sexism, and personal attack, ", " Racism, sexism, and personal attack, ", " Personal attack, racism, and sexism, ", " Racism, sexism, and personal attack, ", " Racism, sexism, and personal attack, ", " Racism, sexism, and personal attacks, \n", " Racism, sexism, and personal attacks. ", " Cyberbullying on racism, sexism, and personal attack. \n", " racism, sexism, and personal attacks, ", " Racism, sexism, and personal attack, ", " Three topics of cyberbullying: personal attack, racism, and sexism. \n", " Racism, sexism, personal attacks, \n", " _______________________, ", " Three topics of cyberbullying: personal attack, racism, and sexism. \n", " Racism, sexism, personal attack, and other topics. \n", " Personal attack, racism, and sexism. ", " racism, sexism, and personal attack, ", " Racism, sexism, and personal attack, ", " Racism, sexism, and personal attack, ", " racism, sexism, and personal attack, ", " Three topics of cyberbullying: personal attack, racism, and sexism. \n", " Cyberbullying topics of personal attack, racism, and sexism, \n", " \n", " Three topics of cyberbullying. ", " Racism, sexism, and personal attacks, ", " personal attack, racism, and sexism, ", " racism, sexism, and personal attack. \n", " Cyberbullying topics such as personal attack, racism, and sexism. \n", " Personal attack, racism, sexism. ", " Racism, sexism, and personal attacks. \n", " Personal attack, racism, and sexism. \n", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attack, ", " racism, sexism, and personal attacks, ", " Three types of cyberbullying topics, ", " Racism, sexism, and personal attacks. \n", " Racism, sexism, and personal attacks, ", " Three topics of cyberbullying: personal attack, racism, and sexism, ", " racism, sexism, and personal attack, ", " Cyberbullying topics of personal attack, racism, and sexism. \n", " Racism, sexism and personal attacks. \n", " Racism, sexism, and personal attacks, ", " Personal attacks, racism, and sexism, ", " racism, sexism, and personal attack, ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attacks. \n", " Racism, sexism, and personal attack. \n", " Racism, sexism, personal attack, and bullying. \n", " Cyberbullying on personal attack, racism, and sexism, as well as general cyberbullying in social media, across three SMPs. ", " Racism, sexism, and personal attacks. ", " Three types of cyberbullying topics. \n", " Racism, sexism, and personal attacks, ", " Personal attacks, racism, and sexism, ", " racism, sexism, and personal attack, ", " Racism, sexism, and personal attacks. \n", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attacks, \n", " Racism, sexism, and personal attacks. \n", " Three types of cyberbullying: personal attack, racism, and sexism. \n", " Personal attack, racism, and sexism, ", " Racism, sexism, and personal attack, ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attack. \n", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attacks, ", " Racism, sexism, and personal attack. \n", " Racism, sexism, and personal attack, ", " racism, sexism, personal attack, and all three topics on a teen-oriented social media platform called Formspring. \n", " Three topics of cyberbullying, personal attack, racism, and sexism. "], "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": ["  various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. ", " Various approaches have been proposed for modelling urban regions and for identifying points-of-interest and itineraries. \n", " Various approaches have been proposed for modelling urban regions and identifying points-of-interest and itineraries. \n", " Various approaches have been proposed. \n", " various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. ", " Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries.  Additionally, bag-of-words representations derived from Flickr tags have been found to give promising results for predicting different environmental phenomena. ", " various approaches have been proposed for modelling urban regions. ", " various approaches have been proposed for modelling urban regions and identifying points-of-interest and itineraries. \n", " [Your concise final answer here], ", " Various existing approaches have been proposed for modeling urban regions, identifying points-of-interest, and predicting itineraries. \n", " Various approaches have been proposed. ", " Several existing works combine word embedding models with geographic coordinates. ", " Various existing approaches have been proposed. ", " Many existing approaches. ", " Various, including modelling urban regions, identifying points-of-interest, and itineraries. ", " various existing approaches model urban environments and identify points-of-interest and itineraries,  but they do not integrate textual information with structured information in a natural way. \n", " unanswerable ", " Various approaches have been proposed. ", " Various approaches have been proposed for modelling urban regions. \n", "  various approaches. \n", " existing approaches are proposed for modelling urban regions, identifying points-of-interest, and itineraries. ", " various, including modelling urban regions, identifying points-of-interest and itineraries, and conducting manual evaluations of image content with little automated exploitation of the associated tags. ", " existing approaches include various methods for learning vector space representations from structured data, such as knowledge graphs, social networks, and taxonomies. \n", " Various approaches have been proposed. ", " The existing approaches for modeling urban regions, identifying points-of-interest, and itineraries, as well as predicting environmental phenomena, include using bag-of-words representations and manual evaluations of image content.  ", " Various existing approaches have been proposed for tasks such as modelling urban regions, identifying points-of-interest, and itineraries. \n", " various approaches have been proposed for modelling urban regions and identifying points-of-interest. \n", " various approaches have been proposed. \n", " various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. \n", " various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries.\n", " unanswerable. ", " Various approaches have been proposed for modelling urban regions, identifying points-of-interest and itineraries. ", " various approaches have been proposed for modelling urban regions and for identifying points-of-interest. \n", " unanswerable ", " Various approaches have been proposed for modelling urban regions and identifying points-of-interest. \n", " Various. ", " The use of Flickr for modelling urban environments has already received considerable attention with various approaches proposed for modelling urban regions, identifying points-of-interest, and itineraries. ", " they include various bag-of-words representations derived from Flickr tags. ", " Various. \n", " Various approaches have been proposed. ", " Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. \n", " Many recent studies have highlighted that Flickr tags capture valuable ecological information, which can be used as a complementary source to more traditional sources. Various approaches have been proposed for modelling urban regions, identifying points-of-interest and itineraries, and it has been found that bag-of-words representations derived from Flickr tags give promising results for predicting a range of different environmental phenomena. Additionally, the GloVe word embedding model has been used to capture linear regularities of word-word co-occurrence. \n\n", " There are various existing approaches for modelling urban regions, identifying points-of-interest and itineraries. ", " existing approaches include modelling urban regions, identifying points-of-interest, and itineraries. ", "  unanswerable, ", " various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. \n", " Various approaches have been proposed. ", " unanswerable \n", " various existing approaches have been proposed for modelling urban regions and identifying points-of-interest. ", " various, ", " unanswerable ", " They include various methods such as bag-of-words representations derived from Flickr tags, learning vector space representations from structured data, and combining word embedding models with structured information. \n", " Various existing approaches have been proposed. \n", " Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. ", " Various approaches have been proposed for modelling urban regions and for identifying points-of-interest. \n", " Various approaches have been proposed for modelling urban regions and identifying points-of-interest. ", " There are various existing approaches proposed for modeling urban regions, identifying points-of-interest, and itineraries, as well as for using social media to conduct manual evaluations of image content. ", " [Your concise final answer here], ", " various approaches have been proposed for modelling urban regions and identifying points-of-interest and itineraries. ", " Various approaches have been proposed for modelling urban regions and identifying points-of-interest. ", "  various approaches have been proposed for modelling urban regions, and for identifying points-of-interest and itineraries. \n", " There are various existing approaches. \n", " various approaches have been proposed for modelling urban regions and identifying points-of-interest. ", " various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. \n", " Various approaches have been proposed for modelling urban regions and identifying points-of-interest and itineraries. \n", " various existing approaches. ", " various approaches have been proposed.\n", " existing approaches include modelling urban regions, identifying points-of-interest and itineraries, and manually evaluating image content with little automated exploitation of associated tags. ", " various approaches have been proposed. \n", " existing approaches include modelling urban regions, identifying points-of-interest and itineraries, and predicting a range of different environmental phenomena. ", " They include modelling urban regions, identifying points-of-interest and itineraries. ", " They include modelling urban regions, identifying points-of-interest, and itineraries. \n", " several approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. Recent studies have also highlighted the value of Flickr tags in capturing ecological information. \n", " Various. ", " [Your concise final answer here], ", " various existing approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries, but the usefulness of Flickr for characterizing the natural environment is less well-understood. ", " existing approaches have already been proposed for modelling urban regions, identifying points-of-interest and itineraries. ", " Several approaches have been proposed. \n", " Several existing approaches have been proposed for modelling urban regions, identifying points-of-interest and itineraries. ", " Various. ", " Various existing approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. In addition, many studies have focused on analyzing Flickr tags to extract useful information in domains such as linguistics, geography, and ecology. \n", " they are bag-of-words representations derived from Flickr tags and structured scientific data. \n", " Existing approaches include various ways of modeling urban regions, identifying points-of-interest and itineraries, and using Flickr tags to model urban environments. ", " various approaches have been proposed for modelling urban regions, identifying points-of-interest and itineraries, and predicting climate features, land cover, species occurrence, and human assessments of scenicness. \n", " bag-of-words, skip-gram, continuous bag-of-words (CBOW), GloVe, and fastText. \n", " various approaches have been proposed for modelling urban regions and identifying points-of-interest. ", " There are various existing approaches for modeling urban regions, identifying points-of-interest and itineraries, and using social media to conduct manual evaluations of image content. Additionally, one recent exception is an approach that uses bag-of-words representations derived from Flickr tags to give promising results for predicting environmental phenomena. ", " Various. ", " There are various existing approaches. ", " Many existing approaches have been proposed for modelling urban regions, identifying points-of-interest and itineraries. ", " yes/no questions or phrases. ", " unanswerable ", " existing approaches include bag-of-words representations, points-of-interest, and itineraries. ", " existing approaches include vector space embeddings for representing objects, GloVe word embedding model, and vector space representations from structured data. ", " Various approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries. ", " There are various existing approaches proposed for modelling urban regions, identifying points-of-interest, and itineraries, but the usefulness of Flickr for characterizing the natural environment is less well-understood. Additionally, ecologists have mostly used social media to conduct manual evaluations of image content with little automated exploitation of the associated tags. \n", " Various existing approaches have been proposed for modelling urban regions and identifying points-of-interest. ", " various, ", " various approaches have been proposed for modelling urban regions. ", " existing approaches include modelling urban regions, identifying points-of-interest, and itineraries.  There are also other approaches to learn vector space representations from structured data such as knowledge graphs, social networks, taxonomies, and various lexicons.  "], "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": [" Deep neural networks, ", " Logistic Regression (LR) and Multilayer Perceptron (MLP). \n", " Machine learning models, specifically deep neural networks. ", "  Logistic Regression (LR) and Multilayer Perceptron (MLP),", " Discriminative models. \n", " deep neural network, \n", " Logistic Regression and Multilayer Perceptron. \n", " Neural network classifiers, \n", " Discriminative models. \n", " Discriminative models. \n", " Discriminative models (e.g., deep neural networks) and probabilistic latent variable models. ", " Discriminative models (e.g., deep neural networks). \n", " Discriminative and probabilistic classifiers. \n", " Discriminative model, \n", " \n", " Deep neural networks. \n", " Deep neural networks, ", " Discriminative classifiers, ", " Logistic Regression and Multilayer Perceptron. \n", " Logistic Regression (LR) and Multilayer Perceptron (MLP). \n", " Discriminative models. \n", " Discriminative models. \n", " deep neural networks \n", " Discriminative model, ", " Multilayer Perceptron and Logistic Regression. \n", " Deep neural networks. \n", " Neural networks. \n", " Discriminative models. \n", " Deep neural networks, \n", " Logistic Regression and Multilayer Perceptron. \n", " Deep neural networks, ", " [Your concise final answer here], ", " Discriminative classifiers. \n", " Discriminative models. ", " Logistic Regression and Multilayer Perceptron. ", " Discriminative models. \n", " Feedforward neural network and Multilayer Perceptron. ", " Logistic Regression and Multilayer Perceptron, \n", " Discriminative classifiers. \n", " Discriminative models. \n", " Discriminative, ", " Discriminative models. \n", " Discriminative classifiers, ", " Discriminative model. \n", " Deep neural networks and logistic regression. \n", " Discriminative classifiers, ", " Logistic Regression and Multilayer Perceptron. \n", " Discriminative, ", " Discriminative models, ", " Deep neural networks. ", " Deep neural networks. \n", " Discriminative models, ", " discriminative classifiers, ", " Discriminative models (e.g., a deep neural network), ", " Multilayer Perceptron (MLP) and Logistic Regression (LR). \n", " Feedforward neural networks. \n", " Discriminative models. ", " Discriminative model (e.g., a deep neural network) \n", " Discriminative model, ", " Logistic Regression and Multilayer Perceptron. \n", " discriminative and generative models, ", " Discriminative models, ", " Deep neural networks and discriminative model (e.g. a deep neural network) for classifying the relevance of microposts to an event. \n", " Deep neural networks, ", ", ", " Discriminative models. \n", " Discriminative models. \n", " Discriminative classifiers, ", " Discriminative models, \n", " Discriminative and probabilistic classifiers, \n", " Deep neural networks, ", " \n", " Logistical Regression (LR) and Multilayer Perceptron (MLP). ", " Discriminative classifiers. ", " discriminative models, ", " Feedforward networks. \n", " Discriminative and probabilistic. \n", " deep neural network, ", " [Your concise final answer here], ", " Logistic Regression and Multilayer Perceptron. ", " \n", " Discriminative model, ", " Discriminative model. \n", " Discriminative, ", " Discriminative models, \n", " Deep neural networks. \n", " Logistic Regression and Multilayer Perceptron, ", " Discriminative models, ", " Discriminative models. ", " Discriminative, ", " Logistic Regression (LR) and Multilayer Perceptron (MLP), ", " Discriminative classifiers. \n", " Logistic Regression and Multilayer Perceptron. \n", " Discriminative classifiers, ", " Logistic Regression and Multilayer Perceptron, \n", " Logistic Regression and Multilayer Perceptron (MLP). \n", " deep neural networks, ", " Discriminative models, ", " Logistic Regression and Multilayer Perceptron. \n", " Discriminative classifiers. \n"], "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": [" There are five baselines in total: 1) Vanilla ST baseline, 2) Pre-training baselines, 3) Multi-task baselines, 4) Many-to-many+pre-training baseline, 5) Triangle+pretrain baseline. ", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training baseline, Triangle+pre-train baseline. \n", " Vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, triangle+pretrain. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train. \n\n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pretrain. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train. \n", " The baselines include the vanilla ST baseline, three pre-training baselines, three multi-task baselines, the many-to-many+pre-training baseline, the triangle+pretrain baseline. \n", " Vanilla ST, encoder pre-training, decoder pre-training, encoder-decoder pre-training, many-to-one, many-to-many, many-to-many+pre-training, triangle+pre-train. \n", " Vanilla ST, pre-training baselines, multi-task baselines, many-to-many+pre-training, triangle+pretrain. ", " Vanilla ST, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many setting, many-to-one setting, many-to-many setting, many-to-many+pre-train, triangle+pre-train. \n", " Vanilla ST, Pre-training, Multi-task, Triangle and Many-to-many+pretrain, ", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Cascade models are the baselines. \n", " There are several baselines. \n", " The baselines are the vanilla ST model, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, triangle+pretrain baseline, and cascaded+re-seg. \n\n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training baseline, Triangle+pre-train baseline. \n", " The baselines include vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, triangle+pre-train. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train, and Cascaded. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training. \n", ", ", " Pre-training baselines, multi-task baselines, and many-to-many+pre-training baseline. \n", " The vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, triangle+pre-train, cascaded system. \n", " The baselines include the vanilla ST model, the pre-training baselines (encoder pre-training, decoder pre-training, encoder-decoder pre-training), the multi-task baselines (one-to-many setting, many-to-one setting, many-to-many setting), the many-to-many+pre-training model, and the triangle+pretrain model. \n\n", " The baselines are several existing speech-to-text translation methods, including Vanilla ST, Pre-training baselines, Multi-task baselines, and several related works. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training baseline, Triangle+pre-train baseline. ", " The baselines are the Vanilla ST, Pre-training, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train, and Cascaded system. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training baseline. \n", " They are Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train, and Cascaded System. \n", " Vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, Triangle+pre-train baseline. \n", " Vanilla ST, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many setting, many-to-one setting, many-to-many setting, many-to-many+pre-training, triangle+pretrain. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Triangle+pre-train. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train. ", " Vanilla ST baseline, pre-training baselines, multi-task baselines, and many-to-many+pre-train baseline, triangle+pre-train. ", " Vanilla ST baseline, pre-training baselines, multi-task baselines, and many-to-many+pre-training baseline. ", " The baselines are a set of existing models, including Vanilla ST, pre-training baselines (encoder pre-training, decoder pre-training, and encoder-decoder pre-training), multi-task baselines (one-to-many setting, many-to-one setting, and many-to-many setting), many-to-many+pre-training, Triangle+pre-train. \n", " Many multi-task baselines, pre-training baselines, Triangle model and a vanilla ST model. \n", " The baselines are: Vanilla ST baseline, pre-training baselines, multi-task baselines, and triangle+pre-training baseline. \n", " The baselines are vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, triangle+pretrain baseline, and other cascaded systems. \n", " Many-to-many, many-to-one, one-to-many, many-to-many+pre-train, triangle+pre-train, vanilla ST, decoder pre-training, encoder pre-training, encoder-decoder pre-training. \n", " Vanilla ST baseline, pre-training baselines, multi-task baselines, and cascaded system., ", " The baselines. ", " Baselines: vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, triangle+pre-train. \n", " Vanilla ST, pre-training baselines, multi-task baselines, many-to-many+pre-training, Triangle+pre-train. \n", " The baselines include the vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, triangle+pretrain baseline, and character experiments. \n", " Vanilla ST, Pre-training, Multi-task, many-to-many+pre-training, Triangle+pre-train. \n", " The baselines include vanilla ST baseline, pre-training baselines, multi-task baselines, and many-to-many+pre-training baselines, and triangle+pretrain. ", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pretrain. \n", " The baselines are Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, and Triangle+pre-train. ", " The baselines include the Vanilla ST baseline, three pre-training baseline experiments, three multi-task baseline experiments, and three additional baselines: Many-to-many+pre-training, Triangle+pre-train, and cascaded systems. \n", " Vanilla ST, Pre-training, Multi-task, and Triangle+pretrain. ", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training baseline, Triangle+pre-train baseline. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training baseline, Triangle+pre-train baseline. \n", " The baselines are as follows: vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, Triangle+pre-train. ", " Several baselines, including a vanilla ST baseline, pre-training baselines, multi-task baselines, and a triangle baseline, are used for comparison with the proposed TCEN model. \n\n", " Baselines include vanilla ST baseline, pre-training baselines (encoder pre-training, decoder pre-training, encoder-decoder pre-training), multi-task baselines (one-to-many setting, many-to-one setting, many-to-many setting), many-to-many+pre-training, and Triangle+pre-train. ", " Pre-training baselines and multi-task baselines. \n", " Vanilla ST, Pre-training, Multi-task, Many-to-many+pre-training, Triangle+pretrain. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training baseline. \n", " Vanilla ST, Pre-training baselines, Multi-task baselines, and Related Work. ", " Pre-training baselines and multi-task baselines. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train. \n", " Vanilla ST, pre-training baselines, multi-task baselines, many-to-many+pre-training, triangle+pretrain. \n", " Vanilla ST, encoder pre-training, decoder pre-training, encoder-decoder pre-training, one-to-many setting, many-to-one setting, and many-to-many setting. ", " The baselines are the vanilla ST model, pre-training baselines, multi-task baselines, and cascaded systems. \n", " Many-to-many multi-task model, many-to-one setting, many-to-one setting, one-to-many setting, one-to-many setting, one-to-many+pre-training, Triangle+pretrain, vanilla ST, and vanilla ST baseline. ", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pretrain, are the baselines. \n", " Pre-training baselines, multi-task baselines, many-to-many+pre-train baseline, triangle+pre-train baseline, and vanilla ST baseline. ", " The vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train. \n", " The vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training, Many-to-many, Triangle+pre-train baselines. \n", " The baselines are Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Cascaded systems. \n", ", ", " Vanilla ST, pre-training, multi-task, many-to-many, and triangle models. \n", " The baselines are Vanilla ST, Pre-training baselines (encoder pre-training, decoder pre-training, encoder-decoder pre-training), Multi-task baselines (one-to-many setting, many-to-one setting, many-to-many setting), Many-to-many+pre-training and Triangle+pre-train. ", " Pre-training baselines, multi-task baselines, and many-to-many+pretrain baseline. \n", " Pre-training baselines and multi-task baselines. \n", " Baselines include Vanilla ST, encoder pre-training, decoder pre-training, encoder-decoder pre-training, multi-task baselines, many-to-many+pre-training, Triangle+pre-train. ", " Many-to-many, one-to-many, and many-to-one settings; Vanilla ST; Pre-training baselines (encoder, decoder, encoder-decoder); Many-to-many+pre-training; Triangle+pretrain. \n\n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Triangle+pre-training, ", " The baselines include the Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training baseline, Triangle+pre-train baseline. \n", " The vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, triangle+pretrain baseline. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Many-to-many+pre-training. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and Triangle+pre-train, Many-to-many+pre-training. ", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and the'many-to-many+pre-train', 'triangle+pretrain' baseline. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, and the other four baselines including Triangle+pre-train and Many-to-many+pre-training. \n", " Baselines include Vanilla ST, encoder pre-training, decoder pre-training, encoder-decoder pre-training, many-to-one setting, one-to-many setting, many-to-many setting, many-to-many+pre-training, Triangle+pretrain. ", ", ", " The baselines include the vanilla ST baseline, the pre-training baselines, the multi-task baselines, and several other methods. \n", " Vanilla ST, pre-training, multi-task, Triangle+pre-train and many-to-many+pre-train. ", " Vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training baseline, triangle+pretrain baseline. \n", " Vanila ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training baseline, Triangle+pre-train baseline,  Cascaded system and Cascaded+re-seg baseline. \n", " Many-to-many+pre-training, Triangle+pre-train, Vanilla ST, Pre-training baselines, Multi-task baselines, One-to-many, Many-to-one, Cascaded systems. \n", " The baselines include Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pretrain. \n\n", " The baseline models used in the experiments are Vanilla ST baseline, Pre-training baselines (Encoder pre-training, Decoder pre-training, Encoder-Decoder pre-training), Multi-task baselines (one-to-many setting, many-to-one setting, many-to-many setting), Many-to-many+pre-training baseline and Triangle+pre-train baseline. \n", " Many-to-many+pre-training, Triangle+pretrain, one-to-many setting, many-to-one setting, and Vanilla ST baseline. ", " The baselines include vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, Triangle+pretrain. \n", " The baselines include the Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training baseline, Triangle+pretrain baseline. \n", " The baselines are \"vanilla ST baseline\", \"Pre-training baselines\", \"Multi-task baselines\", \"Many-to-many+pre-training\", and \"Triangle+pretrain\".", " Many baseline models. ", " Vanilla ST baseline, pre-training baselines, multi-task baselines, many-to-many+pre-training, Triangle+pre-train, and cascaded systems. \n", " Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train, Cascaded system and Cascaded+re-seg. "], "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": [" IMDb movie review dataset, ", " IMDb movie review dataset, ", " [Your final answer here], ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " The IMDb movie review dataset. ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset. \n", " IMDb movie review dataset. \n", " IMDb movie review dataset, ", ", ", " IMDb movie review dataset, ", "  ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset. \n", " IMDb movie review dataset, ", " The IMDb movie review dataset. \n", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset. ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset. \n", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " The IMDb movie review dataset. \n", " IMDb movie review dataset, ", " The IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", "  ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " The IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset. \n", " IMDb movie review dataset, ", " IMDb movie review dataset. \n", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " The IMDb movie review dataset. \n", " IMDb movie review dataset, ", ", ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset. \n", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, ", " IMDb movie review dataset, "], "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": [" yes, ", " yes, ", " yes, ", " yes, ", " Yes, some of the tasks were evaluated in previous work. ", " Yes, previous work, including BIBREF1, BIBREF2 and BIBREF3, examined the extent to which models capture syntax-sensitive phenomena., ", " Yes\n", " Yes, several tasks were evaluated in previous work. \n", " yes, ", " Yes, they were evaluated in BIBREF1, BIBREF2 and BIBREF3. ", " yes, they were. \n", " Yes, several of these tasks were evaluated in previous work. ", " yes, ", " yes, previously, ", " yes, ", " Yes, they were. ", " yes, previously evaluated, ", " yes, they were, in previous work using uni-directional language-model-like models. \n", " yes, ", " yes, ", " Yes, previously evaluated in BIBREF1, BIBREF2 and BIBREF3. ", " yes, previous work examined these tasks. \n", " yes, ", " yes, ", " yes, ", " Yes, several previous works had evaluated these tasks. \n", " Yes, previous work did evaluate some of these tasks. ", " Yes \n", " Yes, ", " Yes, previous work evaluated these tasks. \n", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, previous work did evaluate some of these tasks. ", " yes, ", " Yes, several previous works were referenced. ", " yes, ", " yes, ", " Yes, some of the tasks were evaluated in previous work. ", " yes, ", " Yes, they were evaluated in previous work. ", " Yes, several of them were. \n", " Yes, they were evaluated in previous work. \n", " Yes, they were. \n", " Yes, several tasks were evaluated in previous work. \n", " yes, ", " yes\n\n", " yes, ", " Yes, previous work evaluated these tasks, but not the bidirectional setting required by BERT. ", " Yes, several tasks were evaluated in previous work. ", " yes, ", " Yes, the tasks were evaluated in previous work by BIBREF1, BIBREF2, and BIBREF3. ", " yes, ", " Yes, the tasks were evaluated in previous work. ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, they were evaluated in previous work, ", " yes, ", " yes, ", " Yes, previous work evaluated some of these tasks. \n", " Yes, previous work evaluated some of these tasks. \n", " yes, ", " yes, ", " Yes, they were, according to the text. The author even mentions adapting the evaluation protocol and stimuli of BIBREF1, BIBREF2 and BIBREF3 to the bidirectional setting required by BERT. ", " yes, ", " Yes, previous work has evaluated subject-verb agreement patterns, reflexive anaphora, and negative polarity items using manually constructed stimuli. \n", " Yes, all three previous works evaluated the tasks. ", " yes, ", " Yes, some of them were, ", " Yes, all three previous works used different setups. \n", " yes, several tasks were evaluated in previous work. \n", " Yes, previous work examined the extent to which RNN-based models capture syntax-sensitive phenomena, including subject-verb agreement patterns. ", " Yes, the tasks were adapted from previous works and used as stimuli for the BERT models. \n", " yes \n", " Yes, previously. ", " Yes, several tasks were evaluated in previous work. \n", " yes, previous work was evaluated in RNN-based models, ", " yes, ", " Yes, previous work was done. \n", " yes, ", " yes, ", " Yes, they were.  The stimuli were adapted from previous works., ", " yes, ", " yes, ", " Yes, previous work on RNN-based models. ", " yes, ", " yes, previously evaluated. ", " Yes, previous work did evaluate these tasks. \n", " Yes, but with different evaluation protocol. ", " Yes, previous work evaluated them. ", " Yes, several tasks were evaluated in previous work. \n", " yes, ", " Yes, ", " yes, ", " Yes, several of these tasks were evaluated in previous work. \n"], "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": [" NLP toolkits including NLTK, Stanford CoreNLP, and TwitterNLP are used, as well as commercial NLP toolkits such as BIBREF17 and BIBREF18, in addition to TensiStrength, and CogComp-NLP. \n", " They use several NLP toolkits including NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, CogComp-NLP, and spaCy. \n", " \n", " NLTK, Stanford CoreNLP, TwitterNLP, and commercial toolkits including BIBREF17 and BIBREF18. \n\n", " They use NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, TensiStrength, spaCy, CogComp-NLP, and Stanford NLP. \n", " They use NLTK, Stanford CoreNLP, and TwitterNLP toolkits. \n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP have tokenization, PoS tagging, chunking, named entity recognition, and sentiment analysis capabilities. In addition, they use commercial NLP toolkits such as BIBREF17 and BIBREF18, and other toolkits like TensiStrength, CogComp-NLP, and spaCy for their experiments. \n", " NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF14, BIBREF15, BIBREF17 and BIBREF18 are used. ", " They use TwitterNLP, SentiStrength, BIBREF17, BIBREF18, BIBREF23, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, as well as CogComp-NLP and spaCy. \n", " NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, and commercial NLP toolkits like BIBREF17 and BIBREF18. \n", " They use a variety of toolkits including NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, CogComp-NLP, and spaCy, among others. \n", " BIBREF17, BIBREF18, and BIBREF23, among others. \n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP, as well as commercial APIs like Rosette Text Analytics and Google Cloud, are used. \n", " NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, Rosette Text Analytics, Google Cloud, BIBREF17 and BIBREF18 are used, along with others. \n", " [Your concise final answer here], ", " TensiStrength, TwitterNLP, BIBREF17, BIBREF18, CogComp-NLP, Stanford NLP NER, and spaCy. \n", " BIBREF2's crowdsourcing scheme with variable crowdworker numbers based on the difficulty of the annotation task. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP and commercial NLP toolkits like BIBREF17, BIBREF18. ", " Rosette Text Analytics, Google Cloud, TensiStrength, and BIBREF17 and BIBREF18, ", " They used NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, BIBREF17, BIBREF18, CogComp-NLP, and spaCy. \n", " They use NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, Rosette Text Analytics, and Google Cloud. \n", " They use NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF17, BIBREF18, CogComp-NLP, and spaCy. \n", " BIBREF23, BIBREF17, and BIBREF18 for entity-level sentiment analysis and BIBREF17, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, BIBREF18 for named-entity recognition, ", " NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, etc. \n", " The toolkits they use include NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, TensiStrength, and spaCy. \n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, and three commercial NLP toolkits: BIBREF17, BIBREF18, and BIBREF14. \n", " \n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, and BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, and CogComp-NLP, and spaCy. \n", " BIBREF2, NLP, NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF17, BIBREF18, BIBREF19, BIBREF23, BIBREF24, BIBREF25, BIBREF26, CogComp-NLP, spaCy, and Stanford NLP. \n", " NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, TensiStrength, spaCy, CogComp-NLP. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, and SentiStrength. ", " NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, and commercial APIs including BIBREF17 and BIBREF18 are used. ", " They use NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF17, BIBREF18, BIBREF23, BIBREF17, BIBREF19, BIBREF24, BIBREF25, BIBREF26, and BIBREF18. \n", " NLP toolkits NLTK, Stanford CoreNLP, TwitterNLP, and commercial NLP toolkits BIBREF17 and BIBREF18. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP and also commercial NLP toolkits such as BIBREF17 and BIBREF18, as well as other tools like TensiStrength, CogComp-NLP, and spaCy. \n\n", " NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLP toolkits typically have the following capabilities: NLTK, Stanford CoreNLP, and TwitterNLP, and in addition, the most popular tools for entity-level sentiment analysis are Rosette Text Analytics, Google Cloud, and TensiStrength. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, BIBREF17, BIBREF18, TensiStrength, CogComp-NLP, and spaCy, ", " NLP toolkits NLTK, Stanford CoreNLP, and TwitterNLP, and commercial NLP toolkits Rosette Text Analytics and Google Cloud, and TensiStrength. \n", " They use various toolkits, including BIBREF17, BIBREF18, and TwitterNLP. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF17, BIBREF18, CogComp-NLP, spaCy, and Stanford NLP NER. \n", " They use NLTK, Stanford CoreNLP, and TwitterNLP, among others. \n", ", ", " NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, CogComp-NLP, and commercial NLP toolkits such as BIBREF17 and BIBREF18, also including spaCy. \n", " NLP toolkits including NLTK, Stanford CoreNLP, TwitterNLP, and commercial toolkits like BIBREF17, BIBREF18, and others. \n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF23, BIBREF17, BIBREF18. \n", " NLP toolkits NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, and commercial NLP toolkits such as Rosette Text Analytics and Google Cloud are used. \n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP. ", " Various NLP toolkits. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, and commercial NLP toolkits such as BIBREF17 and BIBREF18, and also CogComp-NLP and spaCy. \n", " The NLP toolkits, Rosette Text Analytics, Google Cloud, TensiStrength, TwitterNLP, CogComp-NLP, and Stanford NLP NER. ", ", ", " [Your concise final answer here], ", " NLP toolkits such as NLTK, Stanford CoreNLP, and TwitterNLP, and commercial toolkits like Rosette Text Analytics and Google Cloud are used. \n", " Several toolkits. \n", " NLTK, Stanford CoreNLP, TwitterNLP, and other commercial and open-source toolkits. \n", " NLP toolkits with publicly-available APIs, including NLTK, Stanford CoreNLP, TwitterNLP, and commercial toolkits, such as BIBREF17 and BIBREF18. \n\n", " They use NLP toolkits such as NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, and spaCy. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, TensiStrength, CogComp-NLP, and spaCy. \n", " They use BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, and other publicly accessible commercial APIs. \n", " BIBREF23, BIBREF17, and BIBREF18 are used in the first set of experiments for entity-level sentiment analysis; BIBREF17, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, and BIBREF18 for named-entity recognition. Additionally, the following toolkits are used in the experiments: TwitterNLP, Stanford NER model, CogComp-NLP NER, and spaCy NER.\n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP; Sentiment detection tools like SentiStrength, TensiStrength, and commercial NLP toolkits like Rosette Text Analytics and Google Cloud; as well as automated systems like CogComp-NLP and spaCy. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF23, BIBREF17, BIBREF18, and also CogComp-NLP and spaCy. \n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP, as well as commercial NLP toolkits like IBM Watson, Google Cloud, and TensiStrength. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, BIBREF17, BIBREF18, TensiStrength, CogComp-NLP, and spaCy. \n\n", " NLP toolkits NLTK, Stanford CoreNLP, TwitterNLP, Rosette Text Analytics, Google Cloud, TensiStrength, spaCy, CogComp-NLP. \n", " NLP toolkits, including NLTK, Stanford CoreNLP, TwitterNLP, and others. \n", " They use NLP toolkits such as NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, and TwitterNLP, and commercial NLP toolkits such as BIBREF17 and BIBREF18, as well as TensiStrength, CogComp-NLP, and Stanford NLP NER. \n\n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, Rosette Text Analytics, Google Cloud, CogComp-NLP, and spaCy. \n", " The toolkits include NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, Rosette Text Analytics, Google Cloud, spaCy, CogComp-NLP, and commercial NLP toolkits like BIBREF17 and BIBREF18. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, and some other commercial toolkits and other systems including Rosette Text Analytics, Google Cloud, TensiStrength and CogComp-NLP. \n", " SentiStrength, TensiStrength, BIBREF17, BIBREF18, TwitterNLP, BIBREF19, CogComp-NLP, and Stanford NLP. \n", " TensiStrength, TwitterNLP, and two commercial NLP toolkits, BIBREF17 and BIBREF18, for experiments. ", " TensiStrength, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26, BIBREF23. \n", " BIBREF23, BIBREF17, and BIBREF18 for entity-level sentiment analysis; and BIBREF17, BIBREF19, BIBREF24, BIBREF25, and BIBREF26, BIBREF18 for named-entity recognition. \n\n", " They use BIBREF17, BIBREF18, BIBREF23, TwitterNLP, SentiStrength, TensiStrength, CogComp-NLP, Stanford NLP NER, spaCy. \n", " NLP toolkits such as NLTK, Stanford CoreNLP, and TwitterNLP. \n", " TensiStrength, TwitterNLP, BIBREF17, BIBREF18, spaCy, and CogComp-NLP. \n", " They use NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLP toolkits with publicly available APIs. \n", " NLP toolkits like NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, spaCy, and CogComp-NLP are used, in addition to commercial APIs from BIBREF17 and BIBREF18, for their experiments. \n", " NLTK, Stanford CoreNLP, TwitterNLP, BIBREF23, BIBREF17, BIBREF18. \n", " BIBREF17 and BIBREF18. \n", " Several toolkits, including NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, CogComp-NLP, spaCy, and Stanford NLP NER. \n", " NLP toolkits with publicly-available APIs, including NLTK, Stanford CoreNLP, and TwitterNLP, as well as commercial NLP toolkits like BIBREF17 and BIBREF18. \n\n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, Rosette Text Analytics, Google Cloud, CogComp-NLP, spaCy, and Stanford NLP. \n", " NLP toolkits. \n", " NLP toolkits like NLTK, Stanford CoreNLP, and TwitterNLP. \n", " NLP toolkits including NLTK, Stanford CoreNLP, and TwitterNLP, as well as commercial NLP toolkits such as Rosette Text Analytics, Google Cloud, and BIBREF17, and BIBREF18. Additionally, they also use TensiStrength, CogComp-NLP, and spaCy. \n", " NLP toolkits with publicly available APIs. \n", " NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, CogComp-NLP, spaCy, and Stanford NLP NER. \n", " NLP toolkits, such as NLTK, Stanford CoreNLP, and TwitterNLP. \n", " They use NLTK, Stanford CoreNLP, TwitterNLP, and various commercial NLP toolkits such as Rosette Text Analytics, Google Cloud, and BIBREF17, BIBREF18. \n\n", " NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, and the commercial toolkits BIBREF17 and BIBREF18. \n", " They use BIBREF2, BIBREF17, BIBREF18, BIBREF23, BIBREF24, BIBREF25, BIBREF26, and BIBREF27, and TensiStrength. \n", " \n"], "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": [", ", " The proposed qualitative annotation schema includes linguistic features, required reasoning and background knowledge, and factual correctness as high-level categories. It is enriched with metrics based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task. The full catalogue of features, their description, detailed annotation guideline as well as illustrating examples can be found in the appendix. \n", " The proposed qualitative annotation schema for machine reading comprehension (MRC) gold standards is described as a multi-label task, that involves annotating question, expected answer, and corresponding context, according to various linguistic, reasoning, and knowledge categories. \n", " A multi-label task with various dimensions of interest. ", " It is a multi-label task that includes various categories to characterise the machine reading comprehension gold standards, ", " The proposed qualitative annotation schema has multiple dimensions including linguistic complexity, required reasoning and background knowledge, and factual correctness. It categorises the type of knowledge required into factual and intuitive knowledge, and annotates features that introduce variance between the supporting facts and the question, such as redundant words and lexical entailment. It also includes categories like operational logic, arithmetic reasoning, and semantic altering grammatical modifiers, and is used to annotate question, expected answer, and the corresponding context., ", " It is based on four dimensions: linguistic complexity, required reasoning and background knowledge, factual correctness and complexity. \n", " It draws inspiration from annotation schema used by Wang2019, and adapts it around lexical semantics and syntax, with added features that introduce variance between the supporting facts and the question. It includes the annotation categories, detailed annotation guidelines, and examples. \n", " It consists of various categories including problem setting, dimensions of interest and framework for MRC gold standard analysis., ", " It is a multi-label task with dimensions of interest including linguistic complexity, required reasoning and background knowledge, and factual correctness, ", " It includes the dimensions of linguistic complexity, required reasoning and background knowledge, and their factual correctness.  \n", " It looks like a taxonomy, ", " It focuses on linguistic complexity, required reasoning, and factual correctness dimensions, ", " It includes dimensions of interest such as linguistic complexity, required reasoning, and factual correctness, with subcategories to annotate question, expected answer, and the corresponding context. ", " It is a taxonomy of features based on linguistic complexity, required reasoning and background knowledge, and their factual correctness., ", " It contains a full catalogue of features, their description, detailed annotation guideline as well as illustrating examples, ", " It categorises gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness, with measurable results to make observations about the challenges present in that gold standard data. \n", " It has various dimensions of interest, including linguistic complexity, required reasoning and background knowledge, and factual correctness, with corresponding categories and annotation guidelines., ", " The proposed qualitative annotation schema is a detailed and complex one that includes categories such as factual correctness, required reasoning, and linguistic complexity, with various subcategories and features within each category. ", " It looks like a taxonomy of features, ", " The proposed qualitative annotation schema includes linguistic features, required reasoning, and factual correctness dimensions., ", "  The proposed qualitative annotation schema looks like a comprehensive framework that categorises gold standards according to linguistic complexity, required reasoning, and factual correctness. It is a multi-label task that annotates question, expected answer, and the corresponding context. The framework includes features that introduce variance between the supporting facts and the question, as well as a metric based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task. \n", " It is based on linguistic complexity, required reasoning, and factual correctness. ", " It includes various features such as linguistic complexity, required reasoning and background knowledge, and factual correctness, with several categories and subcategories. \n", " It is based on dimensions of interest including factual correctness, required reasoning and background knowledge, and linguistic complexity, ", " It is a comprehensive categorisation framework for machine reading comprehension (MRC) gold standards, ", " It is detailed and richly annotated. ", " It includes various dimensions such as linguistic complexity, required reasoning, and factual correctness. \n", " It includes linguistic features, reasoning categories, and factual correctness evaluation, ", " It is not explicitly stated in the article. ", " It includes several dimensions of interest such as required reasoning, factual correctness, knowledge, and linguistic complexity. \n", " It includes various categories, ", " It can be found in Appendix, \n", " A comprehensive and detailed annotation guideline, ", " It includes linguistic complexity, required reasoning and background knowledge, and factual correctness, ", " The proposed qualitative annotation schema categorises question, expected answer and context according to linguistic complexity, required reasoning, and factual correctness., ", " The proposed qualitative annotation schema is a taxonomy that categorises gold standards according to linguistic complexity, required reasoning, and factual correctness, as well as introduces a metric based on lexical cues. \n\n", " It includes linguistic complexity, required reasoning and background knowledge, and factual correctness dimensions of interest, and also annotates features that introduce variance between supporting facts and the question, including redundant words, requirements on understanding of words' semantic fields, use of synonyms and paraphrases, and ambiguous syntactic features., ", " It includes linguistic complexity, required reasoning and background knowledge, and their factual correctness, ", " The proposed qualitative annotation schema includes a taxonomy of features that describe the complexity of the gold standards, ", " It draws inspiration from the annotation schema used by Wang2019, and adapts it around lexical semantics and syntax, including features that introduce variance between the supporting facts and the question. \n", " \n", " The proposed qualitative annotation schema is for categorising gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. \n", " The proposed qualitative annotation schema includes various dimensions of interest, such as linguistic complexity, required reasoning, knowledge, factual correctness, and more. \n", " The proposed qualitative annotation schema categorises gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness., ", " It is a taxonomy that includes various categories such as linguistic complexity, required reasoning and background knowledge, and factual correctness. \n", " It includes various categories, ", " It includes various features such as required reasoning, background knowledge, factual correctness, and linguistic complexity. \n", " It includes dimensions of interest such as linguistic complexity, required reasoning and background knowledge, and factual correctness, along with various categories. \n\n", " It is a taxonomy that categorises the linguistic complexity of textual data and the reasoning and potential external knowledge required to obtain the expected answer. \n", " The proposed qualitative annotation schema consists of a taxonomy that categorizes MRC gold standards across several dimensions, including required reasoning, background knowledge, and factual correctness. It also involves annotating features that introduce variance between supporting facts and questions, as well as metrics based on lexical cues., ", " The proposed qualitative annotation schema includes a taxonomy of categories that categorise gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. \n", " The proposed qualitative annotation schema is for categorising gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. \n", " It is a multi-label task with several dimensions of interest including linguistic complexity, required reasoning, and factual correctness. It also has several categories within these dimensions., ", " It has several dimensions of interest, ", " It looks like a taxonomy of linguistic complexity, required reasoning and background knowledge. \n", " The proposed qualitative annotation schema is a multi-label task that includes various linguistic features, required reasoning and background knowledge, and factual correctness. \n", " The proposed qualitative annotation schema includes dimensions such as required reasoning, knowledge, linguistic complexity, factual correctness, and complexity, ", " It categorises the linguistic complexity of the textual data and the reasoning and potential external knowledge required to obtain the expected answer, and includes factual correctness, required reasoning, and knowledge. \n", " It contains dimensions of interest, including problem setting, factual correctness, required reasoning, knowledge, and linguistic complexity, along with specific features within each dimension. \n", " It is a multi-label task with various categories including linguistic complexity, required reasoning and background knowledge, and factual correctness. \n", " It categorizes question, expected answer and the corresponding context according to linguistic complexity, required reasoning and background knowledge, and factual correctness. \n", " According to the proposed framework, the annotation schema includes dimensions such as factual correctness, required reasoning, and knowledge, which are further broken down into subcategories. \n", " It includes various categories such as linguistic complexity, required reasoning, and factual correctness., ", " The proposed qualitative annotation schema categorises gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. It has multiple categories such as Span, Paraphrasing, Unanswerable, and Generated. It also considers the type of knowledge required, linguistic phenomena, and reasoning capabilities. Additionally, it involves evaluating the factual correctness of answers, required reasoning, and the type of knowledge needed to answer the question. ", "  The proposed qualitative annotation schema includes multiple categories for linguistic complexity, required reasoning and background knowledge, and factual correctness. \n", " The proposed qualitative annotation schema categorizes gold standards into linguistic complexity, required reasoning and background knowledge, and their factual correctness. It includes features that introduce variance between the supporting facts and the question, such as redundant words, synonyms and paraphrases, and syntactic features, like ambiguous relative clauses and nominalization. It also includes required reasoning categories, such as Temporal succession of events, Spatial reasoning, Causal reasoning, and Operational logic. Additionally, it categorizes the type of knowledge required, such as factual knowledge, intuitive knowledge, and knowledge from external resources. Furthermore, it provides a metric based on lexical cues, such as", " It includes categories for linguistic complexity, required reasoning, factual correctness, and knowledge., ", " It includes dimensions of interest: Required ", " It includes dimensions of linguistic complexity, required reasoning and background knowledge, and their factual correctness., ", " The proposed qualitative annotation schema is a multi-label task that includes dimensions of interest such as linguistic complexity, required reasoning, and factual correctness, with categories such as redundancy, lexical entailment, syntactic variance, semantics-altering grammatical modifiers, operational and arithmetic reasoning, and required knowledge. \n\n", " It includes various dimensions of interest, ", " The proposed qualitative annotation schema categorises MRC gold standards according to linguistic complexity, required reasoning, and their factual correctness. \n\n", " The proposed qualitative annotation schema categorises question, expected answer and context by linguistic complexity, required reasoning and background knowledge, and factual correctness., ", " Not described in detail, ", " The proposed qualitative annotation schema includes dimensions of Interest, specifically linguistic complexity, required reasoning and background knowledge, and their factual correctness. \n", " It is a complex multi-label task. \n", " The proposed qualitative annotation schema consists of multiple categories including required answer type, factual correctness, required reasoning, and linguistic complexity. \n", " It is a comprehensive schema consisting of multiple categories. \n", " It has four dimensions of interest including linguistic complexity, required reasoning and background knowledge, and their factual correctness. It includes annotation categories such as retrieval, paraphrasing, unanswerable, and generated. \n", " It is a taxonomy with various categories. ", " It's based on linguistic complexity, required reasoning and background knowledge, and factual correctness dimensions. ", " It looks like the taxonomy shown in Figure FIGREF10, ", " The proposed qualitative annotation schema is described in the article, with five dimensions of interest, including linguistic complexity, required reasoning and background knowledge, and their factual correctness. ", " The proposed qualitative annotation schema has several dimensions of interest including linguistic complexity, required reasoning, and factual correctness, ", " It includes linguistic complexity, required reasoning and background knowledge, and factual correctness dimensions, ", " It includes four main categories, namely linguistic features, reasoning, knowledge, and factual correctness. \n", " It draws inspiration from the annotation schema used by Wang2019, and adapts it around lexical semantics and syntax. \n", "  Not explicitly stated in the article, ", " It categorises gold standards according to linguistic complexity, required reasoning, and their factual correctness, specifically focusing on dimensions such as required reasoning capabilities and factual knowledge. \n", " It includes dimensions of interest such as problem setting, factual correctness, required reasoning, knowledge, and linguistic complexity. ", " A proposed qualitative annotation schema is defined according to five high-level categories of a qualitative annotation schema for annotating question, expected answer and the corresponding context. \n\n", " The proposed qualitative annotation schema is a taxonomy that categorises gold standards according to linguistic complexity, required reasoning, factual correctness, and knowledge. It has multiple categories including linguistic features, reasoning categories, and knowledge categories, ", " It looks like a hierarchical structure with linguistic complexity, required reasoning and background knowledge, and factual correctness as high-level categories. \n", " The proposed qualitative annotation schema categorizes gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. \n", " Not provided in the article, ", " It includes various linguistic features and reasoning patterns. \n", " It looks like the proposed qualitative annotation schema is a comprehensive framework that encompasses various dimensions of interest including linguistic complexity, required reasoning and background knowledge, and factual correctness, as well as lexical cues that might simplify the reading required to arrive at the answer. \n", " It has multiple categories including dimensions of interest such as linguistic complexity, required reasoning and background knowledge, and factual correctness. \n", " It categorises gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness, using dimensions as high-level categories of a qualitative annotation schema. \n\n"], "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": [" English, ", " English, ", " unanswerable, \n", " None, ", " English, ", " None, ", " English. \n", " [Your concise final answer here], ", " English, ", " English, ", " English, ", " English, ", " English, ", " Answer and ", " English, ", " English, ", " English, ", "  unanswerable, ", " unanswerable, ", " English, ", " English, ", " English, ", " ", " \n", " English, ", " None, \n", " English, ", " Unanswerable, ", " English, ", " English, ", " [Your concise final answer here], ", " unanswerable, ", " unanswerable \n", " Unanswerable, ", " English, ", " unanswerable, ", " English, ", " English, ", " English, ", " unanswerable, ", " unanswerable, ", " English, ", " Unanswerable, ", " Not explicitly mentioned, ", " English. \n", " English, ", " \n", " unanswerable, \n", " English, ", " English, ", " unanswerable, ", " unanswerable, ", " [Your final answer here], ", " unanswerable, ", ", ", " English, ", "  ", " unanswerable, ", " English, ", " English, ", " unanswerable, ", " English, ", " English, ", " English, ", " Unanswerable, ", " English, ", " \n", " No specific natural language is studied in this paper. \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " English, ", " unanswerable, ", "  None, ", " English, ", " English, ", " None, ", " unanswerable, ", " unanswerable \n", " English, ", " English, ", " unanswerable, \n", " unanswerable, ", " Unanswerable, ", " English, ", " English, ", " English, ", " English, ", " English, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " English, ", " unanswerable \n", " English, ", " unanswerable, ", " English. \n", " Natural Language, \n", " Unanswerable \n"], "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": [" unanswerable \n", " unanswerable \n", " Yes, \n", " No, ", " unanswerable \n", " unanswerable \n", " unanswerable\n", " unanswerable ", " unanswerable \n", " no, ", " unanswerable \n", " yes \n", " unanswerable, ", " unanswerable \n", " unanswerable \n", " No, \n", " Yes, ", " unanswerable, ", " no, ", " Yes, according to the authors' plan in the future work. ", " unanswerable \n", " No, \n", " No, the dataset is not balanced. ", " no, ", " unanswerable \n", " No \n\n", " unanswerable, ", " unanswerable \n", " unanswerable \n", " unanswerable, ", " no, ", " unanswerable, ", " unanswerable \n", " unanswerable, ", " no \n", " no, \n", " unanswerable, ", " unanswerable, ", " unanswerable \n", " unanswerable \n", " no, \n", " unanswerable, ", " No, \n", " unanswerable, ", " unanswerable \n", " unanswerable \n", " unanswerable\n", " unanswerable \n", " No. \n", " unanswerable \n", " No, ", " No, \n", " No, ", " unanswerable, \n", " No, the dataset for sentiment analysis is unbalanced. ", " No, the lowest and highest correct classification rates pertained to different candidates. \n", " No, ", " unanswerable, ", " unanswerable \n", " No, \n", " unanswerable \n", " unanswerable \n", " unanswerable, ", " no, ", " no, ", " unanswerable \n", " no, ", " unanswerable \n", " No, \n", " unanswerable, ", " unanswerable, ", " no \n", " unanswerable \n", " No, \n", " unanswerable \n", " yes, ", " No \n", " unanswerable, ", " unanswerable, ", " yes, ", " unanswerable, ", " unanswerable, ", " no, \n", " unanswerable, \n", " No, ", " unanswerable \n", " unanswerable \n", " unanswerable ", " unanswerable, ", " No, ", " No, ", " No, ", " No, \n", " unanswerable \n", " unanswerable \n", " No, \n", " unanswerable \n", " unanswerable \n", " no, ", " unanswerable \n"], "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": [" 296,402 sentence pairs and 600K sentences. \n", " The sizes of WikiLarge and WikiSmall are 296,402 sentence pairs and 89,042 sentence pairs, respectively. \n", " The sizes of the datasets are 89,042 sentence pairs and 296,402 sentence pairs. \n\n", " 89,042 sentence pairs, 296,402 sentence pairs, 11.6M words, and 82K, ", " WikiLarge has 296,402 sentence pairs and WikiSmall has 89,042 sentence pairs. ", " 89,042, 296,402, 100, 2,000, 359, 600K, and 11.6M words, and 82K vocabulary size, ", " The sizes of both datasets are different: the training set of WikiSmall has 89,042 sentence pairs, the test set has 100 pairs, and the training set of WikiLarge has 296,402 sentence pairs. \n\n", " The sizes of both datasets are 89,042 and 296,402 sentence pairs respectively. The first dataset has 100 pairs in the test set, while the second dataset has 2,000 for development and 359 for testing. 600K sentences are also chosen as the simplified data with 11.6M words, and the size of vocabulary is 82K for the second dataset,  but not for the first dataset. Furthermore, the training set of the first dataset has 89,042 sentence pairs. \n\n", " 82K and 11.6M words, 296,402 and 600K sentences, respectively, for the vocabulary, simplified data, ordinary-simplified pairs and training data. \n", " The sizes of WikiSmall dataset is 89,042 pairs and 100 pairs, while the size of WikiLarge dataset is 296,402 pairs. \n", " The sizes of the two datasets are 296,402 and 600,000. \n", " 296,402 sentence pairs for WikiLarge and 89,042 sentence pairs for WikiSmall. 11.6M words and 82K vocabulary size for WikiLarge. \n", " 89,042 and 296,402 sentence pairs, ", " 296,402 and 89,042. \n", " WikiLarge has 296,402 sentence pairs and 2,359 sentences; WikiSmall has 89,042 sentence pairs and 100 pairs of test set. \n", " 89,042 sentence pairs and 296,402 sentence pairs, ", " 89,042 sentence pairs and 296,402 sentence pairs, ", " The sizes of both datasets are 89,042 sentence pairs for WikiSmall, and 296,402 sentence pairs for WikiLarge.  Also, there are 600K sentences in the simplified dataset, with 11.6M words, and the size of vocabulary is 82K. The training set of WikiLarge includes 8 (reference) simplifications for 2,359 sentences., ", " 600K sentences for the simplified dataset and 296,402 sentence pairs for WikiLarge. 89,042 sentence pairs for WikiSmall., ", " 89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. \n\n", " 296,402 sentence pairs and 600K sentences, \n", " The WikiLarge dataset has 296,402 sentence pairs and the WikiSmall dataset has 89,042 sentence pairs, with 100 pairs for testing, and 100K sentences are chosen for back-translation. \n", " 296,402 sentence pairs for WikiLarge and 89,042 sentence pairs for WikiSmall, ", " 89,042 sentence pairs (WikiSmall) and 296,402 sentence pairs (WikiLarge), with the WikiLarge dataset having 8 reference simplifications for 2,359 sentences. Additionally, the WikiLarge dataset has 2,000 sentences for development and 359 sentences for testing., ", " 296,402 and 600,000, ", " WikiSmall has 89,042 sentence pairs, WikiLarge has 296,402 sentence pairs, and has 2,359 sentences, \n", " The sizes of WikiSmall and WikiLarge are 89,042 and 296,402 sentence pairs respectively. \n\n", " the sizes of both datasets are 296402 and 10442, ", " The training set of WikiLarge contains 296,402 sentence pairs, and the test set contains 359 sentence pairs. The training set of WikiSmall has 89,042 sentence pairs, and the test set has 100 pairs. Additionally, the WikiLarge dataset includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing. Furthermore, the simplified data contains 600,000 sentences with 11.6M words. ", " 89,042 sentence pairs, and 296,402 sentence pairs, respectively. \n", " The sizes of the datasets are 89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge, and also 11.6M words for WikiLarge, and 82K vocabulary. \n", " The sizes of both datasets are 296,402 sentence pairs and 89,042 sentence pairs, and 2,000 for development and 359 for testing for the WikiLarge and WikiSmall dataset, respectively. \n", " the WikiLarge dataset has 296,402 sentence pairs and 2,359 sentences for testing, the WikiSmall dataset has 89,042 sentence pairs and 100 test pairs.  \n", " 89,042 sentence pairs and 296,402 sentence pairs. \n", " The WikiSmall dataset has 89,042 sentence pairs and 100 test pairs, the WikiLarge dataset has 296,402 sentence pairs. \n", " 296,402 and 600K, ", " 600K sentences and 82K vocabulary for the simplified dataset, 89,042 sentence pairs and 296,402 sentence pairs for the WikiSmall and WikiLarge datasets respectively. \n", " 296,402 and 89,042. \n", " Both datasets have 296,402 and 89,042 sentence pairs respectively, with WikiLarge has 8 reference simplifications for 2,359 sentences, and WikiSmall has 100 test set pairs. ", " The sizes of both datasets are not explicitly mentioned in the article. However, it is mentioned that WikiLarge includes 296,402 sentence pairs. The size of the other dataset is not specified, it is only mentioned as 'WikiSmall' and 'WikiLarge' where the latter is mentioned to have 296,402 sentence pairs. It also mentions that WikiSmall dataset has 89,042 sentence pairs in the training set, and 100 pairs in the test set. Therefore, based on the information available, it can be concluded that the size of WikiLarge is much larger than WikiSmall, ", " 89,042 and 296,402 sentence pairs, ", " The sizes of the datasets are 296,402 and 600K. ", " 296,402 and 600K sentences, \n", " The sizes of both datasets are 296,402 sentence pairs and 89,042 sentence pairs. The size of the vocabulary is 82K, the collected simplified corpora has 11.6M words, and the size of the collected simplified corpora is 600K sentences. \n", " The sizes of the datasets are 89,042 sentence pairs and 296,402 sentence pairs. ", " The sizes of both datasets are 89,042 and 296,402 sentence pairs. \n", " 600K and 296,402, ", " 89,042 and 296,402 sentence pairs, ", " WikiSmall has 89,042 sentence pairs, WikiLarge has 296,402 sentence pairs. Additionally, WikiSmall has 100 pairs in the test set., ", " The sizes of the two datasets are 296,402 sentence pairs for WikiLarge and 89,042 sentence pairs for WikiSmall. ", " 600,000 sentences and 11.6 million words, and 296,402 sentence pairs, \n", " The sizes of the two datasets are 89,042 sentence pairs and 296,402 sentence pairs, respectively. \n", " 296,402 sentence pairs and 600K sentences with 11.6M words, ", " The sizes of both datasets are 296,402 and 600,000 sentences. \n", " The sizes of both datasets are 296,402 and 89,042. \n", " 89,042 and 296,402, ", " 89,042, 11.6M\n", " The sizes of both datasets are 296,402 and 89,042 sentence pairs, as well as 600K and 11.6M words, respectively. \n", " The sizes of the two datasets are 296,402 sentence pairs for WikiLarge, and 100 sentence pairs for WikiSmall. Additionally, WikiLarge has 8 reference simplifications for 2,359 sentences, and WikiSmall has 89,042 sentence pairs for training, and 100 pairs for testing.  We also have 2,000 sentences for development and 359 for testing in WikiLarge, and 600K sentences as the simplified data with 11.6M words, and the size of vocabulary is 82K in WikiSmall.\n\n", " The sizes of both datasets are 89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge. ", " The sizes of WikiSmall dataset and WikiLarge dataset are 89,042 sentence pairs and 296,402 sentence pairs, respectively, with the test sets of 100 pairs and 2,359 sentences, respectively. \n\n", " 296,402 and 600,000. \n", " Both datasets are 296,402 sentence pairs and 89,042 sentence pairs, ", " The training set of WikiSmall has 89,042 sentence pairs, and the test set has 100 pairs; the training set of WikiLarge contains 296,402 sentence pairs. \n", " 600K and 11.6M. ", " 600K and 11.6M words for simplified data and 296,402 sentence pairs for WikiLarge, ", " The size of WikiSmall dataset is 89,042, while the size of WikiLarge dataset is 296,402. \n\n", " 296,402 and 600,000. \n", " 296,402 and 89,042 sentence pairs. \n", " the sizes of the datasets are 89,042 sentence pairs for WikiSmall and 296,402 sentence pairs for WikiLarge, as well as 100 sentence pairs for WikiSmall test set and 359 sentence pairs for WikiLarge test set.  \n", " 296,402 and 600,000, ", " The training set of WikiSmall has 89,042 sentence pairs, and the test set has 100 pairs, the training set of WikiLarge has 296,402 sentence pairs, and the test set contains 2,000 and 359 pairs respectively. \n", " The sizes of both datasets are 89,042 and 296,402 sentence pairs, and 600,000 sentences with 11.6M words. \n", " 296,402 and 600,000, ", " WikiLarge: 296,402 pairs, WikiSmall: 89,042 pairs, and 11.6M words. \n", " 600K and 296,402 respectively, ", " 296,402 sentence pairs and 600K sentences, ", " 89,042 and 296,402, ", " The WikiLarge dataset is 296,402 sentence pairs and the WikiSmall dataset is 89,042 sentence pairs plus a test set of 100 pairs. However, a further 2,000 are split into development set and 359 into testing set, respectively. Thus the sizes are 296,402, 2,000, and 359. On the other hand, for the WikiSmall dataset, we have 89,042 training pairs plus the test set of 100. Thus the sizes are 89,042 and 100. Therefore, we can say that WikiLarge has 296,402 training", " 296,402 and 600K, ", " 296,402 and 600K, ", " 600K sentences (WikiSmall) and 296,402 sentence pairs (WikiLarge) are used as the simplified data, along with 89,042 sentence pairs for training and 100 pairs for testing for WikiSmall, and 296,402 sentence pairs for training for WikiLarge. \n\n", " Both datasets have different sizes, with WikiSmall consisting of 89,042 sentence pairs and WikiLarge consisting of 296,402 sentence pairs. Additionally, WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing, and WikiSmall has a test set of 100 pairs., ", " The sizes of both datasets are 89,042 and 296,402. \n", " 296,402 and 89,042. \n", " 600K and 296,402, ", " The sizes of both datasets are 296,402 and 89,042 sentence pairs for WikiLarge and WikiSmall, respectively, ", " 296,402 and 89,042, ", " Both datasets consist of 296,402 and 89,042 sentence pairs. \n", " The sizes of both datasets are not explicitly given, but we know that WikiSmall has 89,042 sentence pairs in its training set, and WikiLarge contains 296,402 sentence pairs. For the test sets, WikiSmall has 100 pairs and WikiLarge has 2,000 for development and 359 for testing, respectively., ", " 296,402 sentence pairs for WikiLarge, and 89,042 sentence pairs for WikiSmall and 100 pairs for WikiSmall's test set. ", " The sizes of both datasets are 89,042 and 296,402, and 11.6M words, and 82K. \n", " 296,402, 89,042 and 600K sentences, 11.6M words, 82K vocabulary, respectively. \n", " Both datasets are quite large with WikiLarge having 296,402 sentence pairs and WikiSmall having 89,042 sentence pairs plus 100 test pairs. \n", " WikiLarge has 296,402 sentence pairs, while WikiSmall has 89,042 sentence pairs. ", " The sizes of the datasets are 296,402 sentence pairs for WikiLarge and 100 pairs for WikiSmall. ", " WikiSmall and WikiLarge datasets have 89,042 and 296,402 sentence pairs respectively, with WikiLarge containing 8 reference simplifications for 2,359 sentences split into 2,000 for development and 359 for testing. \n", " 296,402 sentence pairs and 600K sentences, \n", " The training set of WikiLarge has 296,402 sentence pairs and WikiSmall has 89,042 sentence pairs, while the test set of WikiSmall has 100 pairs. ", " 296,402 and 89,042, "], "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": [" The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The three datasets standardly used for the evaluation of emotion classification, \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset, ", " The Affective Text, the Fairy Tales, and the ISEAR datasets, ", " SemEval's 2007 Task 14, The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, ", " The three commonly used emotion datasets. \n", " the three datasets standardly used for the evaluation of emotion classification, \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, ", " They compared to existing benchmarks on the ISEAR, Affective Text, and Fairy Tales datasets. \n", " The Affective Text, Fairy Tales, and ISEAR datasets. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and ISEAR dataset, ", " Three datasets, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " SemEval 2007 Task 14 and three other datasets, ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " SemEval's 2007 Task 14, ", " The Affective Text, Fairy Tales, and ISEAR datasets. \n", " The three datasets: Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and ISEAR. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, \n", " SemEval's 2007 Task 14, the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The SemEval 2007 Task 14, Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset, ", " The SemEval 2007 Task 14, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The test set of Task 14, the Affective Text dataset development portion, the Affective Text dataset test, the Fairy Tales dataset, and the ISEAR dataset, ", " The test set of Task 14 and the other two datasets described in Section SECREF3, which are the Fairy Tales dataset and the ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, and also existing systems, for which results are reported in the respective papers. \n", " The SemEval 2007 Task 14, Affective Text, Fairy Tales dataset and the ISEAR dataset. \n", " The SemEval 2007 Task 14 Affective Text dataset. ", " The test set of Task 14 as well as the other two datasets described in Section SECREF3. ", " The three existing benchmarks used were the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text, Fairy Tales, and ISEAR datasets. \n", " Affective Text dataset, Fairy Tales dataset, and ISEAR dataset, ", " The three datasets standardly used for the evaluation of emotion classification. \n", " Three existing benchmarks. \n", " \n", " The three datasets standardly used for the evaluation of emotion classification. \n", " The three datasets standardly used for the evaluation of emotion classification. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, ", " The SemEval 2007 Task 14 and the other two datasets. \n\n", " Affective Text dataset, Fairy Tales dataset, and ISEAR dataset, ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The three datasets, the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " SemEval 2007 Task 14, the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, ", " The SemEval 2007 Task 14 Affective Text dataset. ", " The SemEval 2007 Task 14, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and ISEAR dataset. \n", " Three standard datasets for emotion classification, the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The test set of Task 14 as well as the other two datasets. \n", " The Affective Text, Fairy Tales, and ISEAR datasets. \n", " The three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems. \n", " The three datasets standardly used for the evaluation of emotion classification, which they have described in Section SECREF3. ", " The three datasets standardly used for the evaluation of emotion classification, which are the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems. \n", " The Affective Text, Fairy Tales, and ISEAR datasets. \n", " The three standardly used emotion classification datasets. \n", " The SemEval 2007 Task 14 Affective development dataset, The Fairy Tales dataset and the ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text dataset, Fairy Tales dataset, and ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The SemEval 2007 Task 14. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The test set of Task 14, the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The three datasets standardly used for the evaluation of emotion classification. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " the three datasets standardly used for the evaluation of emotion classification, \n", " Affective Text, Fairy Tales, ISEAR, and the test set of Task 14, ", " The three datasets used for evaluation are the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, which were all used as benchmarks for their evaluation. ", " The three datasets standardly used for the evaluation of emotion classification. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " They compared to three datasets standardly used for the evaluation of emotion classification. \n", " The test set of Task 14, the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text dataset, Fairy Tales dataset, and ISEAR dataset. \n", " The Affective Text, Fairy Tales, and ISEAR datasets, ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset., ", " The test set of Task 14, as well as the Affective Text dataset (development), the Fairy Tales dataset, and the ISEAR dataset. \n", " the three datasets standardly used for the evaluation of emotion classification. ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The SemEval 2007 Task 14 (Affective Text dataset) and the Fairy Tales dataset and the ISEAR dataset, ", " Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset. ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, ", " Affective Text dataset, Fairy Tales dataset, and ISEAR. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " Three datasets annotated with emotions. ", " Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset. \n", " The three datasets standardly used for the evaluation of emotion classification: the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " SemEval 2007 Task 14, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and ISEAR dataset, ", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset, as well as the development portion of the Affective Text dataset. \n", " The SemEval 2007 Task 14, Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " They compared to the Fairy Tales dataset, the ISEAR dataset, and the Affective Text dataset. \n", " The test set of Task 14 as well as the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and the ISEAR dataset, ", " The SemEval 2007 Task 14, Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. \n", " Affective Text dataset, Fairy Tales dataset, and ISEAR dataset. \n"], "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": [" BiLSTM, CNN, and SVM, ", " Bidirectional Long Short-Term-Memory (BiLSTM) and Convolutional Neural Network (CNN), ", " Bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN), and a linear SVM, ", " BiLSTM, CNN, SVM, \n\n", " CNN, BiLSTM and SVM. \n", " Bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN), and linear SVM. \n\n", " Bidirectional Long Short-Term-Memory (BiLSTM), Linear SVM, Convolutional Neural Network (CNN), and baseline models. \n", " BiLSTM, CNN and SVM, ", " BiLSTM, CNN, and a linear SVM. \n", " Linear SVM, Bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN). \n\n", " SVM, BiLSTM, and CNN. \n", " SVM, BiLSTM, and CNN, ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model, ", " BiLSTM, CNN and SVM models. \n", " SVM, BiLSTM, and CNN models, ", " SVM, BiLSTM, and CNN, ", " ", " SVM, BiLSTM, CNN, \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. ", " The models used are a linear SVM, a BiLSTM, and a Convolutional Neural Network (CNN) model, ", " BiLSTM, CNN, and a linear SVM, \n", " A Linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used, ", " Bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN), and Support Vector Machine (SVM) models are used. \n\n", " BiLSTM, CNN, and a linear SVM. \n", " BiLSTM, CNN, and linear SVM. \n", " BiLSTM and CNN models. \n", " CNN, BiLSTM and SVM, ", " Bidirectional Long Short-Term-Memory (BiLSTM), Linear SVM, and Convolutional Neural Network (CNN). \n", " The CNN, RNN, BiLSTM, and SVM are used in the experiment. \n", " BiLSTM, CNN, and SVM models are used in the experiment. \n", " SVM, BiLSTM, CNN. ", " BiLSTM, CNN, SVM, and the concatenation of LSTM's and average pool layer passed through a dense layer, ", " The models used in the experiment are SVM, BiLSTM, and CNN. \n", " BiLSTM, CNN, and linear SVM. ", " Linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), and Convolutional Neural Network (CNN), ", " BiLSTM, CNN, SVM, and FastText, ", " SVMs, BiLSTM, and CNN, ", " BiLSTM, CNN, and SVM, ", " SVM, BiLSTM, and CNN models, ", " Linear SVM, BiLSTM, CNN, and sentiment analysis system of sentimentSystem and rasooli2018cross. ", " The models used are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model, \n\n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used in the experiment, ", " The models used in the experiment are linear SVM, Bidirectional Long Short-Term-Memory (BiLSTM), and Convolutional Neural Network (CNN). ", " BiLSTM, CNN, and SVM, ", " Linear SVM, Bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN) models are used in the experiment. ", " SVM, BiLSTM, CNN, ", " SVM, BiLSTM, CNN, ", " CNN, BiLSTM, and SVM, ", " SVM, BiLSTM, and CNN, ", ", ", " BiLSTM, CNN, and SVM, ", " A Linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN) model are used in the experiment, ", " The models used in the experiment are Linear SVM, BiLSTM, and CNN. ", " The models used in the experiment are linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), and Convolutional Neural Network (CNN), ", " SVM, BiLSTM, CNN. \n", " BiLSTM, CNN, and linear SVM, ", " SVM, BiLSTM, and CNN models. \n", " SVM, BiLSTM, and CNN models are used in the experiment. \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. \n", " The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN) model. \n", " A CNN-based sentence classifier, a BiLSTM model, a linear SVM, a bidirectional Long Short-Term-Memory model, and a Convolutional Neural Network (CNN) model. \n", " BiLSTM, CNN, and SVM. ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used in the experiment. \n", " SVM, BiLSTM, CNN. \n", " SVM, BiLSTM, and CNN, ", " The models used in the experiment are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. \n", " The three models used are a linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model, ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used in the experiment, ", " BiLSTM, CNN, and SVM models are used. \n", " BiLSTM, CNN, and SVM models. \n", " BiLSTM, CNN, and SVM, ", " Neural models, including Bidirectional Long Short-Term-Memory (BiLSTM) and Convolutional Neural Network (CNN), ", " SVM, BiLSTM, and CNN, ", " BiLSTM, CNN, and SVM, ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN), ", " SVM, BiLSTM, and CNN, ", " SVM, BiLSTM, and CNN, ", " Bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN) and linear Support Vector Machine (SVM) with pre-trained FastText embeddings., ", " SVM, BiLSTM, and CNN, ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN), ", " BiLSTM, CNN, and linear SVM, ", " BiLSTM, CNN, and SVM, ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN) model. \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used in the experiment. \n", " Linear SVM, bidirectional Long Short-Term-Memory (BiLSTM) model, and Convolutional Neural Network (CNN) model. \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used, ", " SVM, BiLSTM, and CNN models, ", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN) model. \n\n", " SVM, BiLSTM, and CNN. \n", " A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM), and a Convolutional Neural Network (CNN) model. \n", " BiLSTM, CNN and SVM, ", " BiLSTM, CNN, and SVM. \n", " CNN, BiLSTM, and a linear SVM. \n", " SVM, BiLSTM, CNN, \n", " SVM, BiLSTM and CNN, ", " Neural models including CNN and BiLSTM, ", " The models used in the experiment are a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model. \n\n", " Bidirectional Long Short-Term-Memory (BiLSTM) model, Convolutional Neural Network (CNN), and Linear Support Vector Machine (SVM), "], "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": [" No, ", " unanswerable, ", " No, ", " unanswerable ", " No, ", " no, ", " No, \n", " unanswerable, ", " unanswerable, ", " No, ", " Unanswerable, ", " no, ", " no, ", " No, \n", " Unanswerable, ", " No, \n", " No, \n", " No \n", " unanswerable, ", " No, ", " No \n", " unanswerable, ", " Unanswerable. \n", " No, ", " No, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " No, \n", " unanswerable, ", " No, ", " No, ", " Unanswerable, ", " unanswerable, ", " No, ", " No, ", " No, ", " Unanswerable, ", " No, ", " unanswerable, ", " No, unanswerable \n", " unanswerable, ", " No, ", " unanswerable, ", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " unanswerable, ", " No, ", " unanswerable, ", " Unanswerable. \n", " No, ", " unanswerable, ", " unanswerable, ", " No, \n", " No, ", " No, ", " unanswerable, ", " No, ", " Unanswerable. \n", ", ", " unanswerable, ", " unanswerable, ", " no, ", " No, ", " No, ", " no, \n", " No, \n", " unanswerable, ", " No, ", " unanswerable, ", " No, ", " No, \n", " No, ", " No, ", " unanswerable, ", " No, \n", " unanswerable \n", " No, \n", " no, ", " unanswerable \n", " No, ", " No, \n", " unanswerable, ", " Unanswerable, ", " unanswerable ", " No, \n", " no, ", " unanswerable, ", " No, \n", " Unanswerable, ", " No, \n", " No, ", " No, ", " no, \n", " No, ", " No, the article does not discuss measuring the usefulness of the answers. It only discusses characteristics of the questions that tend to get answered or not. \n", " No, ", " Unanswerable, ", " unanswerable, "], "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": [" The invertibility condition is that the Jacobian determinant of the neural projector is nonzero and differentiable. ", " The invertibility condition is that the neural projector should be invertible, and the Jacobian matrix of the inverse neural projector should have a nonzero determinant. \n\n", " The invertibility condition is that the determinant of the Jacobian matrix of the neural projector should be equal to one (i.e. volume-preserving) and that the neural projector should have an inverse. \n\n", "  The invertibility condition consists of two requirements: (1) the function is invertible and (2) the Jacobian determinant is nonzero. \n\n", " The invertibility condition is that the neural projector's Jacobian determinant is equal to 1 and its inverse exists. \n", " Invertibility condition is that the neural projector's inverse exists and is differentiable. \n", " The invertibility condition is that a function has an inverse. \n\n", " The invertibility condition is that the neural projector satisfies INLINEFORM0 and INLINEFORM1 exists. \n", " The invertibility condition requires that the Jacobian determinant of the neural projector is nonzero and differentiable. \n", " Inline, Inline, and Inline, ", " The invertibility condition consists of two requirements: the Jacobian determinant of the transformation is nonzero and the transformation is differentiable. \n", " INLINEFORM0 and INLINEFORM1 exists, INLINEFORM0 and INLINEFORM1 are inverses of each other. \n", " INLINEFORM0 and INLINEFORM1 exists. \n", " The invertibility condition is that the neural projector is invertible and the Jacobian determinant is equal to one. \n", " The invertibility condition is that the neural projector's Jacobian determinant is equal to one (i.e., volume-preserving) and its inverse exists. \n", " The invertibility condition is that the neural projector is both invertible and volume-preserving. ", " exists,. ", " The invertibility condition is that the neural projector's Jacobian determinant is nonzero and differentiable. \n", " The invertibility condition is that the Jacobian matrix of function is triangular with all ones on the main diagonal. ", " INLINEFORM0 and INLINEFORM1 exists. ", " The invertibility condition is that the neural projector's Jacobian determinant is equal to one, indicating a volume-preserving transformation, and its inverse exists. \n", " The invertibility condition is that the Jacobian determinant is equal to one. \n", " The invertibility condition consists of two requirements: (1) the Jacobian of the transformation is full-rank, and (2) the transformation has an inverse. \n\n", " exists, and exists., ", " The invertibility condition is that the neural projector has a unit Jacobian determinant. ", " The invertibility condition is that the neural projector has a Jacobian determinant equal to 1 and its inverse exists. \n", " The invertibility condition is that the Jacobian determinant of the projection function is equal to one. \n", " The invertibility condition is that the Jacobian determinant of the neural projector is nonzero and differentiable. ", " INLINEFORM0 and INLINEFORM1 exists. ", " It is required that the determinant of the Jacobian matrix of the projection function is non-zero. \n\n", " The invertibility condition is a condition that constrains the neural projector to be invertible, and is satisfied when the Jacobian determinant of the projector is equal to one. \n", " It exists. ", " INLINEFORM0 and INLINEFORM1 exists. \n", " The invertibility condition is that the neural projector should have both an inverse and a Jacobian determinant equal to 1. \n", " The invertibility condition is that the determinant of the Jacobian matrix of the neural projector's inverse is nonzero. \n", " The invertibility condition is that the neural projector must be invertible and its Jacobian determinant must be equal to 1. ", " The invertibility condition consists of two requirements: the projection must be one-to-one, and its inverse exists. \n", " InlineForm0 and InlineForm1 exists. \n", " The invertibility condition is that the Jacobian determinant of the neural projector is nonzero and differentiable. ", " It is defined as the existence of a unique solution to the equation y = f(x) and the existence of a unique solution to the equation x = f^(−1)(y). \n\n\n", " The invertibility condition is that the projection INLINE2 is invertible, and INLINE3 exists. ", " INLINEFORM0 INLINEFORM1 exists, INLINEFORM2 exists. INLINEFORM3 = INLINEFORM4, \n", "  (INLINEFORM0 and INLINEFORM1 exists), ", " The invertibility condition is that the neural projector has a unit Jacobian determinant and its inverse exists. \n", " exists and exists. \n", " INLINEFORM5 exists. \n", " INLINEFORM0 and INLINEFORM1 exists, INLINEFORM3 is invertible. \n", " The invertibility condition is that a neural projector must have a parameterized inverse and satisfy two requirements: the input and output have the same dimension, and there exists a permutation matrix such that a right multiplication by it can convert the input into the output. ", " The invertibility condition is that the neural projector satisfies INLINEFORM0 and INLINEFORM1 exists. \n", " The invertibility condition is that the Jacobian determinant is equal to one. \n", " the Jacobian determinant of the projection function is nonzero. \n", " To be invertible, the Jacobian determinant of the neural projector must be nonzero and differentiable.  Specifically, the neural projector must be invertible, and its Jacobian determinant must be equal to one. \n", " The invertibility condition requires the neural projector to satisfy two requirements: the Jacobian determinant of the projector is nonzero, and its inverse exists. \n\n", " The invertibility condition is the Jacobian determinant of the neural projector is nonzero. ", " INLINEFORM0 exists. INLINEFORM1 exists. \n", " The invertibility condition requires that the neural projector satisfies two conditions: (1) the Jacobian determinant of the neural projector is non-zero, and (2) the neural projector has an inverse. \n", " The invertibility condition is the requirement that both the function f and its inverse f−1 exist. ", " INLINEFORM5 exists and INLINEFORM6=INLINEFORM7. \n", " The invertibility condition requires that the neural projector must satisfy two requirements: (1) the determinant of its Jacobian matrix is equal to 1, and (2) the Jacobian matrix must exist. \n", " The invertibility condition is that the neural projector is invertible, i.e., the transformation from the observed embedding space to the latent embedding space is one-to-one. Additionally, the transformation must preserve volume, meaning the Jacobian determinant of the transformation is equal to one. \n\n", " The invertibility condition is that the Jacobian determinant of the projection function should be nonzero and differentiable. \n", " INLINEFORM1 and INLINEFORM2 exists, \n", "  INLINEFORM5 exists, and INLINEFORM0 = INLINEFORM1. ", " The invertibility condition is that the neural projector INLINEFORM2 must satisfy two requirements: INLINEFORM3 and INLINEFORM4 exists. \n\n", " exists and INLINEFORM0 is nonzero. \n", " Inline-formula, Inline-formula, and Inline-formula exists, \n", " The invertibility condition is that the neural projector exists and its Jacobian determinant is nonzero and differentiable. ", " The invertibility condition is that the neural projector’s inverse exists, and its Jacobian determinant is nonzero and differentiable. \n", " The invertibility condition is that the Jacobian determinant of the neural projector is equal to one. \n", " The invertibility condition is that the Jacobian determinant of the projection is nonzero and differentiable. ", " The invertibility condition is that the neural projector’s inverse exists. \n", " The invertibility condition is that the neural projector has a Jacobian determinant that is equal to 1. \n\n", " The invertibility condition is (1) the determinant of the Jacobian matrix of the neural projector is nonzero, and (2) the determinant of the inverse of the Jacobian matrix exists. \n", " The invertibility condition is that a neural projector should have a non-zero and differentiable Jacobian determinant. \n", " INLINEFORM0 and INLINEFORM1 exists, and INLINEFORM0 is invertible. ", " The invertibility condition is that the neural projector INLINEFORM2 has a Jacobian matrix INLINEFORM1 that is triangular with all ones on the main diagonal. \n\n", " The invertibility condition is that the neural projector exists and the volume-preserving condition holds. \n", " The invertibility condition is that the neural projector satisfies INLINEFORM0 and INLINEFORM1 exists. \n", " The invertibility condition is that the neural projector must satisfy two requirements: (1) INLINEFORM0, and (2) INLINEFORM1 exists. \n\n", " The invertibility condition is that the determinant of the Jacobian matrix is nonzero and differentiable, and that the inverse of the projection function exists. \n", " The invertibility condition is that a neural projector with its inverse exists. \n", " The invertibility condition is that the neural projector should satisfy two requirements: (1) its Jacobian determinant should be 1, and (2) its inverse should exist. \n", " The invertibility condition is that the neural projector's Jacobian determinant is nonzero. \n", " INLINEFORM0 and INLINEFORM1 exists. ", " INLINEFORM0 and INLINEFORM1 exists. \n", " INLINEFORM5 exists, INLINEFORM0 = INLINEFORM1 INLINEFORM0. ", " It is that the neural projector satisfies two requirements: the Jacobian determinant is positive and the inverse exists. \n", " The invertibility condition is that the neural projector is an invertible function with a unit Jacobian determinant. \n", " The invertibility condition is that the neural projector is constrained with two requirements: (1) the determinant of the Jacobian matrix is non-zero, and (2) the inverse of the projection function exists. \n", " The invertibility condition is that the Jacobian determinant of the neural projector is nonzero. \n", " INLINEFORM5 exists, \n", " The invertibility condition is that the neural projector's Jacobian determinant is equal to 1. \n", " The invertibility condition is that the Jacobian determinant is nonzero and differentiable, which can be ensured by constraining the neural projector to be invertible and volume-preserving. ", " The invertibility condition is that the Jacobian determinant is nonzero and differentiable, and that the projection function and its inverse are both computable. \n\n", " InlineForm0 and InlineForm1 exists. ", " The invertibility condition is that the neural projector has to be invertible and that the determinant of its Jacobian matrix is nonzero. \n", " The invertibility condition is that the Jacobian determinant of the projection function is nonzero and differentiable. \n", " The invertibility condition is that the neural projector must be both invertible and volume-preserving. \n", " The invertibility condition is that the neural projector must be both invertible and volume-preserving. ", " The invertibility condition is that the Jacobian determinant is equal to one. \n"], "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": [" The researchers found that there were differences in the distributions of various metadata attributes between viral tweets containing fake news and viral tweets not containing fake news. \n", " Their distribution results showed significant differences, ", " The results detail the differences between viral tweets containing fake news and viral tweets not containing them for several variables.  The distributions were compared for each variable using the Kolmogorov-Smirnov test. ", " \n", " The researchers found that tweets containing fake news had different distributions for certain attributes compared to tweets that did not contain fake news. ", " The distributions of followers, the number of URLs on tweets, and the verification of the users showed significant differences. \n", " Not provided in the article, unanswerable.    \n", " The results showed that users generating viral tweets containing fake news had a very different distribution on the number of followers, and accounts spreading viral tweets with fake news appeared to have a larger proportion of friends/followers, and the ratio of friends/followers of the account producing a viral tweet could be useful to single-out fake news in Twitter. ", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users., ", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n\n", " There were differences around exposure, characterization and polarization between tweets containing fake news and those not containing them. ", " The distribution results show that there were significant differences between tweets containing fake news and tweets not containing them for exposure, characterization, and polarization.\n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The distributions of viral tweets containing fake news and viral tweets not containing fake news were significantly different in several attributes. \n", " They found significant differences in the distribution of followers, number of URLs on tweets, and verification of users., ", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " [Your concise final answer here], ", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " The distribution results show significant differences on the number of followers, the number of URLs on tweets, and the verification of the users. \n", " They were found to be statistically significant. \n", " Their distribution results showed significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " Significant differences were found between tweets containing fake news and those not containing them in terms of followers, URLs, favourites, friends, and media.\n", " Their distribution results were compared and found to have significant differences between tweets containing fake news and tweets not containing them. \n", " \n", " They showed significant differences, ", " Their distribution results showed differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The distributions of the variables of tweets containing fake news and tweets not containing fake news showed significant differences in the distribution of followers, the number of URLs, and the verification of the users. However, the distributions were not significantly different in terms of retweets, number of favorites, and number of hashtags, ", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users,", " \n", " Their distribution results indicated that tweets containing fake news have differences in several parameters. ", " The distributions were significantly different.\n", " Their distribution results showed that tweets containing fake news and tweets not containing them have different distributions for some attributes. \n", " The results showed significant differences in the distribution of several variables, including followers, number of URLs, and verification of users, between tweets containing fake news and tweets not containing them. \n", " The results showed significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " The results showed that tweets containing fake news were created more recently and had a higher number of URLs than other viral tweets. Also, accounts spreading fake news had a higher proportion of friends/followers, the number of followers was different, and the ratio of friends/followers was different. Additionally, tweets containing fake news mostly contained one mention, and viral tweets not containing fake news presented more media elements. However, the number of retweets and favorites did not show significant differences, ", " They found differences in the distribution of several variables. \n", " The results show significant differences in the distribution of followers, number of URLs on tweets, and verification of the users between tweets containing fake news and those not containing them. ", " The researchers found several significant differences in the distribution of certain variables between viral tweets containing fake news and those not containing fake news. \n", " They showed significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " Table TABREF23 reports the actual differences, ", " The distribution results showed that there were differences in the number of followers, number of URLs on tweets, and verification of users. \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The distributions of viral tweets containing fake news and viral tweets not containing fake news were found to be different. \n", " There were differences in the distribution results between tweets containing fake news and tweets not containing fake news, specifically in followers, URLs, and verification of users. ", " Their distribution results showed differences between tweets containing fake news and tweets not containing them, ", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users, and also found that the tweets containing fake news were created more recently, had fewer favourites, and included more URLs than those not containing fake news. \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users, ", " The results showed that there were significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " They reported significant differences in the distribution of various attributes, but the details vary by category, with some being statistically significant and others not. ", " The distribution results showed that the accounts spreading fake news have a larger proportion of friends/followers than those spreading viral content only, and that the viral tweets containing fake news have more URLs than those spreading viral content., ", " Their distribution results are presented in various figures in the article. \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The results were that there were significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The distribution results are detailed in Table 23 and shown in Figures 24-35. \n", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " The study found that the distribution of followers, the number of URLs on tweets, and the verification of users were all significant differences.  ", " They were statistically significant differences in the distributions of viral tweets containing fake news and viral tweets not containing fake news, \n", " They were not distributed equally. ", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users.  \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " Their distribution results showed that there are differences around exposure, characteristics of accounts spreading fake news and the tone of the content, with viral tweets containing fake news being shorter-lived, having a larger proportion of friends/followers, having a larger number of URLs, and being highly polarized. \n", " There were significant differences in the distribution of several factors. ", " yes, ", " The results show significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users between tweets containing fake news and those not containing them. ", " The distribution results showed that there were significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users, in comparison to viral tweets without fake news. \n\n", " They found differences in several distributions between viral tweets containing fake news and viral tweets not containing them. \n", " \n", " Their distribution results showed significant differences between viral tweets containing fake news and viral tweets not containing fake news in the followers count and the number of URLs on tweets.\n", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The distribution results showed statistically significant differences in several attributes between tweets containing fake news and tweets not containing fake news. ", " Their distribution results showed several differences between viral tweets containing fake news and viral tweets not containing fake news. ", " The results detail characteristics of tweets along previously mentioned dimensions.  ", " Their distribution results showed differences in exposure, characterization, and polarization. \n", " The results showed that there were significant differences in the distributions of followers, number of URLs on tweets, and the verification of the users. ", " Not explicitly stated, ", " They showed significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users, ", " The researchers found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of users between tweets containing fake news and those not containing them. ", " Their distribution results show differences in characteristics such as the number of followers, URLs, and verification of users.    \n", " Their results showed significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " The distribution results showed several significant differences between tweets containing fake news and those not containing them, including in the number of followers, the number of URLs on tweets, and the verification of the users. \n", " They found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The researchers found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users among viral tweets containing fake news and viral tweets not containing fake news. ", " They found differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " They were significant, ", " They found significant differences in several meta-data dimensions between tweets containing fake news and those not containing fake news. ", " \n", " Not directly stated, but it can be deduced that there were significant differences in distribution for various features. \n", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " Their results show significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " Significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users were found. ", " Their distribution results show differences in the features between viral tweets containing fake news and viral tweets not containing fake news., ", " The distributions of viral tweets containing fake news and viral tweets not containing fake news were found to differ on several meta-data attributes, including the number of followers, the number of URLs, the number of favourites, the time of exposure, and the characteristics of the accounts spreading fake news. \n", " Their distribution results showed significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users, among others. \n", " Their results showed statistically significant differences in the distributions of various attributes between viral tweets containing fake news and viral tweets not containing fake news. \n", " They found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " The authors found significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. \n", " Their distribution results showed significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users. ", " They showed significant differences in the distribution of followers, the number of URLs on tweets, and the verification of the users. "], "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": [" One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence, which is harmonic mean. \n", " One minus the absolute value of the difference between the irony score and the sentiment score. \n", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the irony reward which is the difference between the irony score of the input sentence and that of the output sentence, and their harmonic mean. \n\n", " The combination of rewards for reinforcement learning is the harmony mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n\n", " One minus the absolute value of the difference between the irony score and sentiment polarity score. \n", " One minus the absolute value of the difference between the irony score and the sentiment score. \n", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, plus the difference between the irony score of the input sentence and that of the output sentence, \n", " [Your concise final answer here], ", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, plus the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n\n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " The combination of rewards is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and that one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n\n", " The combination of rewards is one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the irony reward which is the difference between the irony score of the input sentence and that of the output sentence. \n\n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n", " The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " The combination of rewards for irony accuracy, sentiment preservation, and content preservation. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence, or their harmonic mean. \n\n", " The combination of rewards for irony accuracy and sentiment preservation. \n", " Irony accuracy and sentiment preservation. ", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence plus the difference between the irony score of the input sentence and that of the output sentence. \n\n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, ", " The combination of rewards for reinforcement learning is the harmonic mean of the irony reward and the sentiment reward. \n", " The combination of rewards is the harmonic mean of the irony reward and the sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n\n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the irony score and that of the output sentence, one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the harmonic mean of the two rewards. \n", " The combination of rewards is the harmonic mean of the irony reward and the sentiment reward. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence plus one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " [Your concise final answer here], ", "  Displaymathinline, ", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence, \n", " The combination of rewards is defined as the harmonic mean of the irony reward and the sentiment reward. \n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n\n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, plus one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n\n", " The harmonic mean of the irony reward and the sentiment reward, ", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n", " The combination of rewards for reinforcement learning is the harmonic mean of the irony reward and the sentiment reward. \n\n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " Irony reward and sentiment reward, ", " The combination of rewards is the harmonic mean of the irony reward and the sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, plus the irony reward which is the difference between the irony score of the input sentence and that of the output sentence. \n\n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " The combination of rewards is the harmonic mean of irony reward and sentiment reward. \n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. \n", " The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " one minus the absolute value of the difference between the irony score and sentiment score. \n", " [Your concise final answer here], ", " The combination of rewards for reinforcement learning is the harmonic mean of the irony reward and the sentiment reward. \n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " Irony reward, sentiment reward. ", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, plus the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n", " The harmonic mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, plus the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n\n", " The combination of rewards for reinforcement learning is the harmonic mean of the irony reward and the sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n\n", " one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence. \n\n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the harmonic mean of these two rewards. \n\n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the output sentence, and the harmonic mean of the above two. \n", " The combination of rewards for reinforcement learning is irony reward and sentiment reward. ", ", ", " Irony reward and sentiment reward. ", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, and the difference between the irony score of the input sentence and that of the output sentence. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " One minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence, and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n\n", " The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence and the difference between the irony score of the input sentence and that of the output sentence. \n", " The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward. \n", " One minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. \n", " one minus the absolute value of the difference between the irony score of the input sentence and that of the output sentence and one minus the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence, ", " The harmonic mean of irony reward and sentiment reward. \n"], "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": [" GloVe and Edinburgh embeddings. \n", " GloVe, Edinburgh embeddings, Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, and Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, and Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh, ", " GloVe, Edinburgh embeddings, and Emoji embeddings, ", " GloVe, Edinburgh, Emoji embeddings, ", " GloVe and Edinburgh, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh, ", " GloVe and Edinburgh embeddings, ", " Pretrained word embeddings used in the paper were GloVe, Edinburgh embeddings., ", " GloVe, Edinburgh, Emoji. \n", " GloVe, Edinburgh embeddings, and emoji embeddings. \n", " GloVe, Edinburgh embeddings, emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh, Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, Emoji embeddings, ", " GloVe, Edinburgh embeddings, and Emoji embeddings. \n", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, Emoji embeddings. \n", " GloVe and Edinburgh embeddings. \n", " GloVe, Edinburgh, and Emoji embeddings, ", " GloVe and Edinburgh, ", " GloVe, Edinburgh embeddings, Emoji embeddings. ", " GloVe, Edinburgh embeddings, and Emoji embeddings. ", " GloVe, Edinburgh embeddings, Emoji embeddings, ", " GloVe, Edinburgh embeddings and Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh, and Emoji, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh, Emoji embeddings, ", " GloVe, Edinburgh embeddings, and Emoji embeddings, ", " GloVe and Edinburgh, ", " GloVe and Edinburgh embeddings. \n", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh, ", " Pretrained word embeddings used were 200-dimensional GloVe embeddings and Edinburgh embeddings. \n", " GloVe, Edinburgh embeddings, emoji embeddings, ", " GloVe and Edinburgh embeddings. \n", " GloVe, Edinburgh, Emoji embeddings. \n", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, and emoji embeddings. \n", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings. \n", " GloVe, Edinburgh embeddings. ", " GloVe, Edinburgh embeddings, and Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings. \n", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, and emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh, and Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, and Emoji embeddings. \n", " GloVe, Edinburgh, Emoji, ", " GloVe and Edinburgh embeddings. \n", " GloVe, Edinburgh embeddings, Emoji embeddings. \n", " GloVe, Edinburgh embeddings, and emoji embeddings were used. ", " GloVe and Edinburgh embeddings. \n", " GloVe, Edinburgh, Emoji. \n", " GloVe, Edinburgh, and emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, Emoji embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh, ", " GloVe, Edinburgh embeddings, and Emoji embeddings. ", " GloVe and Edinburgh embeddings, ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings and emoji embeddings, ", " GloVe, Edinburgh embeddings, Emoji embeddings. \n", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, and emoji embeddings. ", " GloVe and Edinburgh embeddings. ", " GloVe and Edinburgh embeddings, ", " GloVe, Edinburgh embeddings, Emoji embeddings, "], "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": [" The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model may not work well with Shakespeare style transfer if the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The model's performance suffers when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " The model may not perform well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer in certain cases, ", " The model may not perform well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " A low average content score. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences, leading to low average content score. \n\n", " They can produce poor results when the style transfer dataset does not have similar words in the training set of sentences, ", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model does not work well with the Shakespeare style transfer when the generated English poem may not work well with the style transfer dataset. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " It may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer, ", " The model has a limitation that the generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The model may not perform well in certain scenarios where it is difficult to transfer the style of the poem into Shakespearean prose. \n", " The model may not work well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n\n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer in cases where the style transfer dataset does not have similar words in the training set of sentences, and the average content score can be low. \n", " The generated English poem may not work well with Shakespeare style transfer. \n\n", " The model may not work well with Shakespeare style transfer when the generated English poem may not work well with Shakespeare style transfer. \n\n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model may not work well with Shakespeare style transfer when the generated English poem does not have similar words in the style transfer dataset, leading to a low average content score. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n\n", " The model may not perform well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " They show that their model does not perform well with a low average content score when the generated English poem may not work well with Shakespeare style transfer. \n", " They have a low average content score for \"Starry Night\", ", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model may not perform well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer, specifically when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " They struggle with end-to-end dataset, which causes the generated English poem to not work well with Shakespeare style transfer in some cases. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The model's generated English poems may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " It may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " A low average content score, ", " The model may not work well with Shakespeare style transfer if the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n\n", " Limited vocabulary overlap between style transfer dataset and English poem data. \n\n", " The model has low average content score for the \"Starry Night\" painting, ", " A lack of end-to-end dataset, leading to potential misalignment between the generated English poem and Shakespearean style. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " The generated English poem may not work well with Shakespeare style transfer. \n", " They find that the generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " The generated English poem may not work well with Shakespeare style transfer in certain cases, \n", " The generated English poem may not work well with Shakespeare style transfer, ", " The generated English poem may not work well with Shakespeare style transfer in the absence of similar words in the training set of sentences. \n", " The model may not work well with Shakespeare style transfer when the generated English poem may not have similar words in the style transfer dataset. \n\n", " The model does not work well when the generated English poem may not work well with Shakespeare style transfer, \n", " The model may not work well with Shakespeare style transfer when the generated English poem may not be well represented in the style transfer dataset, ", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model's generated English poem may not work well with Shakespeare style transfer, ", " The model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n\n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " They have a limitation that the generated English poem may not work well with Shakespeare style transfer. \n", " The model may not work well with Shakespeare style transfer due to the style transfer dataset not having similar words in the training set of sentences. \n\n", " They have a low average content score when the generated English poem does not work well with Shakespeare style transfer. \n\n", " The model has limitations with the generated English poem may not work well with Shakespeare style transfer, \n", " They cannot handle style transfer when the source sentence lengths are long and the style transfer dataset does not have similar words in the training set of sentences. \n", " It may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model may not perform well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The model may not work well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer. \n", " The model may not work well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The generated English poem may not work well with Shakespeare style transfer, \n", " The generated English poem may not work well with Shakespeare style transfer, and the style transfer dataset does not have similar words in the training set of sentences. \n\n", " The model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " The model may not perform well when the generated English poem does not work well with Shakespeare style transfer, as shown in the case of \"Starry Night\". \n", " The model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " A solution to this limitation would be to expand the style transfer dataset for a better representation of the poem data. \n", " They show that the generated English poem may not work well with Shakespeare style transfer. \n\n", " The model's generated English poem may not work well with Shakespeare style transfer. \n", " A solution would be to expand the style transfer dataset, for a better representation of the poem data, ", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences, and this results in a low average content score. \n\n", " The generated English poem may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " They cannot work well with style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n", " They demonstrate that their model may not perform well when the style transfer dataset does not have similar words in the training set of sentences. \n", " The model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences. \n"], "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": [" The SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 datasets \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016, ", " SemEval-2016, ", " SemEval-2016, ", " SemEval-2016 dataset. \n", " SemEval-2016 dataset, ", " SemEval-2016 dataset. \n", " SemEval-2016 dataset \n", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” dataset. \n", " SemEval-2016, ", " The dataset used by the researchers is the SemEval-2016 \"Sentiment Analysis in Twitter\" task, which includes a fine-grained and a ternary sentiment classification task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 dataset, ", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016, ", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 dataset. \n", " The SemEval-2016 dataset. \n", " SemEval-2016. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. \n", " The SemEval-2016 dataset for ternary and fine-grained sentiment classification tasks. \n", " The dataset for fine-grained classification is split in training, development, development_test and test parts. ", " SemEval-2016 dataset for twitter sentiment classification. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " The dataset for fine-grained classification is split in training, development, development_test and test parts, specifically the SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 dataset. \n", " SemEval-2016 dataset. \n", " SemEval-2016, ", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " [Your concise final answer here], ", " SemEval-2016 dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\". \n", " SemEval-2016 dataset. ", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. \n", " SemEval-2016 datasets. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task, ", " SemEval-2016, ", " SemEval-2016, ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 dataset, ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset, ", " The SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 dataset, \n", " SemEval-2016 dataset \n", " SemEval-2016 “Sentiment Analysis in Twitter”, ", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n", " SemEval-2016 “Sentiment Analysis in Twitter”. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " The dataset used in the paper is from SemEval-2016 “Sentiment Analysis in Twitter” task. ", " SemEval-2016 \"Sentiment Analysis in Twitter\" dataset, ", " The SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 “Sentiment Analysis in Twitter”, ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset, ", " SemEval-2016 “Sentiment Analysis in Twitter” dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n", " SemEval-2016 dataset, ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task, ", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n\n", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016, ", " SemEval-2016 “Sentiment Analysis in Twitter” task. ", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016. \n", " SemEval-2016 “Sentiment Analysis in Twitter” dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n", " The SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. \n", " The dataset for fine-grained classification is split in training, development, development_test and test parts, but the exact name of the dataset is not mentioned in the article. However, it is part of the SemEval-2016 “Sentiment Analysis in Twitter” task. \n", " SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n", " SemEval-2016 dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\". ", " The SemEval-2016 “Sentiment Analysis in Twitter” task dataset. \n", " SemEval-2016 dataset, ", " SemEval-2016 “Sentiment Analysis in Twitter” dataset, ", " SemEval-2016 “Sentiment Analysis in Twitter” task, ", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task dataset. \n", " SemEval-2016 \"Sentiment Analysis in Twitter\" task. \n"], "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": [" The dataset is sourced from the Stanford Sentiment Analysis Dataset. ", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " \n", " From the Stanford Sentiment Analysis Dataset. ", " From the Stanford Sentiment Analysis Dataset. \n", " Expert curated dataset from the Stanford Sentiment Analysis Dataset, ", " From the Stanford Sentiment Analysis Dataset. ", " Expert curated dataset from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and the Twitter platform. \n", " The dataset includes 12,594 unique English hashtags from the Stanford dataset. \n", " The dataset is sourced from the Stanford Sentiment Analysis Dataset, ", " The new dataset of hashtags includes all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset is created from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and is comprised of 12,594 unique English hashtags from 1,268 randomly selected tweets. \n", " The dataset is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process, ", " It is sourced from the Stanford Sentiment Analysis Dataset and the Twitter platform. \n", " [Your concise final answer here], ", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " \n", " From the Stanford Sentiment Analysis Dataset. \n", " STAN dataset created by BansalBV15, and a new expert curated dataset including 12,594 unique English hashtags. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset, ", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " It includes all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis Dataset. ", " From the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset, ", " The dataset is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset is sourced from the Stanford Sentiment Analysis Dataset and includes 12,594 unique English hashtags. \n", " \n", " From a Stanford Sentiment Analysis Dataset, ", " From the Stanford Sentiment Analysis Dataset, ", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. ", " Expert curated. \n", " From the Stanford Sentiment Analysis Dataset. ", " The dataset includes all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis Dataset. \n\n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. \n", " It includes all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis Dataset, \n", " From the Stanford Sentiment Analysis Dataset. ", " Crowdsourced segmentations and the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. \n", " The dataset is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. ", " From a tweet dataset from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset, ", " From the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset and a curated dataset. \n", " The dataset is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " A new dataset is sourced from the Stanford Sentiment Analysis Dataset, and an existing dataset is used from BansalBV15. ", ", ", " \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset and Twitter. \n", " From the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. ", " From the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. ", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and is curated by experts. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. ", " From the Stanford Sentiment Analysis Dataset. \n", " The dataset includes all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. ", " It is sourced from a Twitter data. ", "  The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and includes 12,594 unique English hashtags. \n", " It is sourced from the Stanford Sentiment Analysis Dataset and tweets from 2010, \n", " From the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. ", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " From the Stanford Sentiment Analysis Dataset. \n", " It is sourced from the Stanford Sentiment Analysis Dataset. \n", " [Your final answer here], ", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and additional corrections were made by the authors. \n", " From the Stanford Sentiment Analysis Dataset. ", " Expert curated. \n", " It is created by crowdsourcing and also expert curated. \n", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset. \n", "  The Stanford Sentiment Analysis Dataset is used, which includes 1,268 randomly selected tweets. \n", " It is sourced from a combination of existing datasets and expert curation. ", " The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and the tweets from the year 2010. \n", " From the Stanford Sentiment Analysis Dataset. ", " Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets. \n", " \n"], "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": [" \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " Unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " unanswerable, \n", " \n", " \n", " unanswerable, \n", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " Unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable \n", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, \n"], "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": [" They showed that personalized models generated high-quality and specific recipes that aligned with historical user preferences. \n", " Their results show that personalized models outperform baselines in BPE perplexity, and they generate more diverse and acceptable recipes. \n", " They achieved average recipe-level coherence scores of 1.78-1.82 and higher scores for entailment of recipe steps, showing that personalized models outperformed baseline. \n\n", " They achieved average recipe-level coherence scores of 1.78-1.82. \n\n", " Our models outperformed the baselines in BPE perplexity, while generating more diverse recipes, but achieved lower scores in BLEU-4. \n", " They achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. ", " They achieved high-quality and specific recipes that align with historical user preferences, with their personalized models outperforming the baseline in BPE perplexity and Distinct-1/2, and the Prior Name model performing the best. \n", " They showed all personalized models outperform baseline in BPE perplexity. \n", " All personalized models outperformed the baseline in BPE perplexity, with the Prior Name model performing the best, as well as in user matching accuracy and Mean Reciprocal Rank, but performed worse in BLEU-1 due to generating more diverse and personalized recipes. ", " They outperformed the baseline models in BPE perplexity. \n", " All personalized models outperform baseline in BPE perplexity, \n", " All personalized models outperform baseline in BPE perplexity. \n", " All personalized models outperformed the baselines in BPE perplexity, with Prior Name performing the best. \n", " All personalized models outperformed baseline in BPE perplexity with Prior Name performing the best, while their Distinct-1/2 and BLEU-1 values also showed improvements over the baseline. However, their BLEU-4 values were worse than the baseline. \n", " All personalized models outperformed the baselines in BPE perplexity, with Prior Name performing the best. \n\n", " All personalized models outperformed the baseline in BPE perplexity, with Prior Name performing the best. \n\n", " They achieved average recipe-level coherence scores of 1.78-1.82 and a high entailment score, and also outperformed the baseline model on BLEU-1 and human evaluation. \n\n", " Their models outperformed the baselines in BPE perplexity, and Prior Name performed the best. \n\n", " All personalized models outperformed the baseline in BPE perplexity, with Prior Name performing the best. \n\n", " All personalized models outperformed baseline in BPE perplexity with Prior Name performing the best. \n", " They achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. \n", " All personalized models outperformed baseline in BPE perplexity. \n", " All personalized models outperform baseline in BPE perplexity with Prior Name performing the best. \n", " They showed all personalized models outperform baseline in BPE perplexity, with Prior Name performing the best. \n\n", " Their personalized models outperformed the baseline in BPE perplexity and generated more diverse recipes. \n", " They achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. \n\n", " All personalized models outperformed the baseline in BPE perplexity. \n", " All personalized models outperformed baseline in BPE perplexity. \n", " They found that all personalized models outperformed the baseline in BPE perplexity. \n\n", " They found that all personalized models outperformed baseline in BPE perplexity. \n", " All personalized models outperform baseline in BPE perplexity, with Prior Name performing the best. \n\n", " All personalized models outperformed baseline in BPE perplexity, with Prior Name performing the best, while exhibiting comparable performance to baseline in BLEU-1/4 and ROUGE-L. \n", " All personalized models outperform baseline in BPE perplexity, \n", " All personalized models outperformed baseline in BPE perplexity. \n", " They achieved comparable performance to baseline in BLEU-1/4 and ROUGE-L, but generated more diverse and acceptable recipes. \n", " They achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. \n", " They showed that personalized models generate high-quality and specific recipes that align with historical user preferences, and outperform baseline models in perplexity, user-ranking, and human evaluation. \n", " Their models outperformed the baselines in BPE perplexity, and also generated more diverse and acceptable recipes, but worse in BLEU-4 and ROUGE-L. \n", " They outperformed the baseline in BPE perplexity, BLEU-1, ROUGE-L, and had higher recipe-level coherence and recipe step entailment scores. \n", " They achieved better results than the baselines in all the evaluation metrics, with the Prior Name model performing the best. \n", " All personalized models outperform baseline in BPE perplexity. \n", " All personalized models outperform baseline in BPE perplexity, with Prior Name performing the best. They also generate more diverse recipes that are preferable to human evaluators. \n", " All personalized models outperformed baseline in BPE perplexity. \n", " All personalized models outperformed baselines in BPE perplexity, with Prior Name performing the best. \n", " All personalized models outperformed the baseline in BPE perplexity. \n", " All personalized models outperformed the baseline in BPE perplexity. \n", " Their models outperformed the baseline in BPE perplexity, BLEU-1, and BLEU-4. \n", " Their personalized models outperformed the baseline in BPE perplexity. \n", " They outperformed the baseline in BPE perplexity. \n", " All personalized models outperformed the baseline in BPE perplexity. \n", " They achieved better results on the new dataset with personalized models, ", " They achieved average recipe-level coherence scores of 1.78-1.82. \n", " They outperformed baseline in BPE perplexity, and Prior Name performed the best. \n", " Our models outperformed baseline in BPE perplexity, and all personalized models beat baselines in both user matching accuracy (UMA) and Mean Reciprocal Rank (MRR) by a large margin. \n\n", " They obtained better BPE perplexity on the new dataset. \n", " They obtained high-quality and specific recipes that align with historical user preferences. \n", " All personalized models outperform baseline in BPE perplexity. \n", " All personalized models outperformed the baseline in BPE perplexity. \n", " They outperformed baselines in BPE perplexity and achieved average recipe-level coherence scores of 1.78-1.82. \n", " Our models outperformed baselines in BPE perplexity and generated more diverse recipes, but performed worse in BLEU-4. ", " Their personalized models outperformed baseline in BPE perplexity, BLEU-1/4, and ROUGE-L, and generated more diverse and acceptable recipes. \n", " Their models outperformed the baselines in BPE perplexity. \n\n", " All personalized models outperformed baselines in BPE perplexity, \n", " They outperformed a baseline in BPE perplexity, with the Prior Name model performing the best. \n", " All personalized models outperformed the baseline in BPE perplexity. \n", " They outperformed baseline in BPE perplexity. \n", " All personalized models outperform baseline in BPE perplexity. \n", " Their personalized models outperformed baseline in BPE perplexity. \n", " They showed that personalized models generate high-quality and specific recipes that align with historical user preferences. \n", " They showed that all personalized models outperform baseline in BPE perplexity, with Prior Name performing the best. \n", " All personalized models outperformed baselines in BPE perplexity. \n", " The models exhibited comparable performance to baseline in BLEU-1/4 and ROUGE-L, but generated more diverse and acceptable recipes. ", " All personalized models outperformed the baselines in BPE perplexity and achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. \n\n", " Their personalized models outperformed the baseline model in terms of BLEU-1/4 and ROUGE-L, but performed better in terms of diversity metrics (Distinct-1/2) and recipe coherence. \n", " They achieved a new state-of-the-art result in generating plausible and personalized recipes from incomplete input specifications. \n", " They outperformed the baseline model. \n", " They outperformed their baselines in BPE perplexity. \n", " Our models outperform the baselines in BPE perplexity with Prior Name performing the best, but obtain comparable performance to baseline in BLEU-1/4 and ROUGE-L, while generating more diverse and acceptable recipes. \n", " They achieved comparable performance to the baseline in BLEU-1/4 and ROUGE-L, but better in perplexity. \n\n", " All personalized models outperformed the baseline in BPE perplexity, BLEU-1/4, and ROUGE-L metrics, and generated more diverse and acceptable recipes. \n", " All personalized models outperform baseline in BPE perplexity, with Prior Name performing the best. \n", " Our models exhibit comparable performance to baseline in BLEU-1/4 and ROUGE-L, but generate more diverse and acceptable recipes. \n", " They outperformed a non-personalized baseline in BPE perplexity with their personalized models. \n", " All personalized models outperformed baseline in BPE perplexity, and the Prior Name model performed the best. \n", " All personalized models outperform baseline in BPE perplexity, with Prior Name performing the best. \n", " They achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. \n", " They achieved average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77, and performed better in entailment score, with an average score of 0.92, exceeding the baseline score of 0.91. \n", " All personalized models outperformed the baseline in BPE perplexity, with Prior Name performing the best. \n", " Our models outperform baseline in BPE perplexity, ", " Our models outperform baseline in BPE perplexity and Distinct-1/2 with Prior Name performing the best, but perform worse in BLEU-4. \n", " Their personalized models outperformed baseline in BPE perplexity and recipe-level coherence scores. \n", " Their models outperformed the baseline in BPE perplexity with the Prior Name model performing the best. \n", " All personalized models outperformed the baselines in BPE perplexity. \n", " All personalized models outperformed baseline in BPE perplexity. \n", " They obtained average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77, with all personalized models achieving better results than the baseline in the metrics for recipe-level coherence. \n\n", " All personalized models outperformed the baselines in BPE perplexity. \n", " All personalized models outperform the baselines in BPE perplexity, and the Prior Name model performs the best. \n\n", " Their models outperformed the baseline in BPE perplexity. \n", " They showed that their personalized generative models can generate plausible, personalized, and coherent recipes preferred by human evaluators for consumption. \n", " They outperformed their baselines in BPE perplexity, BLEU-1 with more key entities present, but worse in BLEU-4, and their models performed better in ROUGE-L. \n"], "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": [" small, ", " Small, ", " small, ", " BERT$_\\mathrm {BASE}$, ", " They use small BERT. \n", " small BERT, ", " no, ", " Small, ", " small, ", " small BERT, ", " small, ", " Small BERT. \n", " Small BERT, ", " Small BERT, ", " small, ", " Small BERT, ", " Small BERT \n", " small, ", " BERT_BASE \n", " small BERT, ", " Small BERT, ", " small BERT, \n", " BERT_BASE, ", " small, ", " small, ", " small \n", " small, ", " small BERT, ", " small, ", " BERT BASE \n", " small, ", " Small BERT. \n", " small BERT, ", "  ", " small BERT, ", " small, ", " small BERT, ", " Small, ", " BERT$_\\mathrm {BASE}$, ", " small, ", " small BERT, ", " small, ", " small BERT, ", " small, ", " BERT BASE, ", " small, ", " small, ", " small, ", " Small BERT, ", " They use small BERT. \n", " small BERT, \n", " small, ", " small, ", " Small BERT, ", " small, ", " small BERT, ", " small BERT, ", " small, ", " small, ", " small, \n", " small, ", " no, ", " Small BERT, ", " small, ", " small, ", " Small BERT, ", " small, ", " small \n", " small BERT, ", " small, ", " They use a small BERT, \n", " small BERT, ", " BERT_BASE, ", " Small BERT, ", " Small BERT, ", " BERT$_\\mathrm {BASE}$, ", " small \n", " small, ", " small, ", " They use small BERT. \n", " They use small BERT, ", " small, ", " BERT$_\\mathrm {BASE}$, ", " small BERT, ", " BERT_BASE, ", " small BERT, ", " small, ", " small, ", " small, ", " They use small BERT. \n", " BERT$_\\mathrm {BASE}$, ", " Small, ", " small, ", " Small, ", " small \n", " They use a small BERT, ", " small, ", " small BERT, ", " Small, ", " small BERT, "], "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": [" unanswerable \n", " No, ", " No, unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " no, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Unanswerable, ", " unanswerable, \n", " unanswerable, ", " Unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable \n", " No, ", " Unanswerable \n", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " Unanswerable, \n", " No, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " No, ", " Unanswerable, ", " unanswerable, ", " unanswerable, ", " No, \n", " No, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " No, ", " unanswerable, \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " No, ", " No, ", " Yes, ", " unanswerable, ", " unanswerable, ", " No, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " unanswerable \n", " No, ", " unanswerable, ", " No, ", " unanswerable \n", " unanswerable \n", " unanswerable, "], "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": [" A compact and scalable representation of a set of word vectors. ", " A compact, scalable, and meaningful representation of a set of word vectors. \n", " A compact, scalable, and meaningful representation of a set of word vectors. ", " compact and meaningful representation of the whole set of words in a context. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact and meaningful representation of a whole set of words. \n", " a compact, scalable and meaningful representation of the whole set of word vectors. \n\n", " \n", " A compact, scalable and meaningful representation of the whole set of words. ", " compact and meaningful representation of the whole set of words. \n", " A compact and scalable representation of the whole set of words. \n", " A compact, scalable and meaningful representation of a set of words. ", " A compact and scalable representation of a set of word vectors. ", " A compact, scalable and meaningful representation of a set of word vectors, retaining most of the variability of features. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact, scalable, and meaningful representation of a whole set of word vectors. \n", " A compact, scalable, and meaningful representation of a set of word vectors. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " a compact, scalable and meaningful representation of the whole set of words. \n", " A compact, scalable, and meaningful representation of the whole set of words. \n", " A compact, scalable, and meaningful representation of a set of words. \n", " A compact, scalable, and meaningful representation of a whole set of word vectors. \n", " A compact, scalable and meaningful representation of the whole set of words. \n\n", " A compact, scalable and meaningful representation of the whole set of word vectors. ", " \n", " A compact and meaningful representation of the whole set of words. ", " Compact, scalable and meaningful representation of a set of word vectors. ", " A compact, scalable and meaningful representation of the whole set. ", " A compact, scalable, and meaningful representation of the whole set of word vectors. \n", " Compact, scalable, and meaningful representation of the whole set of words. \n\n", " A compact, scalable and meaningful representation of the whole set. \n", " A compact, scalable and meaningful representation of a set of word vectors, retaining most of the variability of features. \n", " A compact and scalable and meaningful representation of the whole set of words from the same context. ", " a compact, scalable, and meaningful representation of the whole set of word vectors. ", " A compact, scalable, and meaningful representation of the whole set. \n", " Compact, scalable and meaningful representation of the whole set. \n", " A compact, scalable, and meaningful representation of the whole set of word vectors. \n", " A compact, scalable, and meaningful representation of the whole set of word vectors. ", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact, scalable and meaningful representation of a set of words. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact, scalable and meaningful representation of the whole set. \n", " Most of the variability of the class. \n", " A compact, scalable and meaningful representation of the whole set of words. \n\n", " A compact, scalable, and meaningful representation of the whole set of words. \n", " A compact, scalable, and meaningful representation of the whole set of word vectors. \n", " a compact, scalable and meaningful representation of the whole set of words. \n", " Yes, it can retain most of the variability of the class and compactly represent the context of the corresponding text.\n\n", " A compact, scalable, and meaningful representation of a set of words. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n\n", " A compact, scalable, and meaningful representation of a set of word vectors. ", " A compact, scalable, and meaningful representation of a whole set. ", " A compact, scalable, and meaningful representation of the whole set of words. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact and scalable representation of the context of the corresponding text. \n", " A compact, scalable and meaningful representation of the whole set of words. \n\n", " A compact, scalable and meaningful representation of a set of word vectors. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n\n", " A compact, scalable, and meaningful representation of the whole set of word vectors. \n", " A compact and scalable representation of the whole set of word vectors. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n\n", " Most of the variability of the class. ", " Most of the variability of the class. \n", " A compact, scalable, and meaningful representation of the whole set of words. \n", " Compact, scalable and meaningful representation of the whole set. ", " a compact, scalable and meaningful representation of the whole set of word vectors. \n\n", " A compact, scalable and meaningful representation of a set of words. \n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact, scalable and meaningful representation of a whole set of words. ", " A compact, scalable and meaningful representation of the whole set. \n", " A compact, scalable, and meaningful representation of a set of word vectors. \n\n", " A compact, scalable and meaningful representation of the whole set of word vectors. ", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact, scalable, and meaningful representation of the whole set of words. \n", " A compact, scalable and meaningful representation of the whole set. \n", " Most of the variability of the class. ", " a compact, scalable and meaningful representation of the whole set of words from the same context. \n", " A compact, scalable and meaningful representation of the whole set of words. \n", " A compact, scalable and meaningful representation of a set of word vectors. \n", " A compact, scalable and meaningful representation of a set of word vectors, retaining most of the variability of features. ", " A compact, scalable and meaningful representation of a set of word vectors. \n", " A compact, scalable, and meaningful representation of a whole set of word vectors. ", " A compact, scalable and meaningful representation of a set of word vectors. \n", " A compact, scalable and meaningful representation of the whole set. ", " A compact and scalable representation of a set of word vectors. ", " The context of the corresponding text, retaining most of the variability of features.\n\n", " A compact, scalable and meaningful representation of the whole set of word vectors. \n", " A compact, scalable, and meaningful representation of a set of word vectors. \n", " A compact, scalable and meaningful representation of a whole set of word vectors. \n", " a compact, scalable and meaningful representation of the whole set of words. \n", " Compact and scalable representation of a set of word vectors. \n", " A compact, scalable, and meaningful representation of the whole set of word vectors. \n", " ", " A compact, scalable and meaningful representation of the whole set of words. \n", " A compact, scalable, and meaningful representation of the whole set of word vectors. \n", " a compact and scalable representation of a set of words. \n", " A compact, scalable, and meaningful representation of a set of word vectors. \n", " Text content. ", " Text context. ", " A compact, scalable and meaningful representation of the whole set of word vectors. \n"], "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": [" 110 hours, ", " unanswerable, \n", " 110 hours, \n", " unanswerable, ", " 110-hour, ", " unanswerable, ", " 110-hour, ", " unanswerable, ", " [Your concise final answer here], ", " \n", " unanswerable, \n", " unanswerable, ", " unanswerable, \n", " 110 hours, ", " 110-hour, \n", " 1, 234 hours, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Unanswerable, \n", " 110 hours, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " \n", " \n", " unanswerable, ", " 38 hours, ", " 960 hours, ", " 110 hours, ", " 110 hours, \n", " unanswerable, ", " Unanswerable, \n", " unanswerable, ", " unanswerable, ", " 110 hours, ", " 110 hours, ", " 110-hour, \n", " unanswerable, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " 960 hours, ", " unanswerable, \n", " 110 hours, ", " unanswerable \n", " unanswerable, ", " unanswerable, ", " unanswerable \n", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " unanswerable, \n", " 1200 hours, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " \n", " unanswerable, \n", " 110 hours, ", " unanswerable, \n", " unanswerable, \n", " unanswerable, ", " 1,200 hours, ", " \n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " 110 hours, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " 110 hours, \n", " unanswerable, ", " 110-hour German-English ST corpus, ", " 110 hours, \n", " 110 hours, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " 110-hour, ", " unanswerable, ", " 960 hours, ", " \n", " unanswerable, ", " unanswerable, ", " 110 hours, ", " unanswerable, ", " 110 hours, ", " 110-hour, \n", " unanswerable, \n", " 110 hours, ", " unanswerable, ", " unanswerable, ", " 110-hour, "], "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": [" InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and average BERT embeddings. ", " InferSent and Universal Sentence Encoder, ", " InferSent, Universal Sentence Encoder, and average GloVe embeddings, ", " InferSent, Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder, ", " InferSent, Universal Sentence Encoder, SkipThought. ", " InferSent, Universal Sentence Encoder, Skip-Thought, RoBERTa, Poly-encoders, BERT and GloVe. ", " InferSent and Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, Average GloVe embeddings, Polyencoders, Skip-Thought, BERT. \n\n", " InferSent and Universal Sentence Encoder, ", " [Your concise final answer here], ", " InferSent, Universal Sentence Encoder, Skip-Thought, and RoBERTa are evaluated. \n\n\n", " InferSent and Universal Sentence Encoder. ", " InferSent, Universal Sentence Encoder, and Average GloVe embeddings. \n\n", " InferSent and Universal Sentence Encoder, and also the average GloVe embeddings. ", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. \n", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. \n", " InferSent, Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, Skip-Thought, and poly-encoders, and average GloVe embeddings. \n", " InferSent and Universal Sentence Encoder, ", " InferSent, Universal Sentence Encoder, and BERT. \n", " InferSent, Universal Sentence Encoder, SkipThought, GloVe embeddings, SentEval. ", " InferSent, Universal Sentence Encoder, average GloVe embeddings. \n", " InferSent and Universal Sentence Encoder. \n", " Universal Sentence Encoder, InferSent, Skip-Thought, and average GloVe embeddings. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and polyencoders. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, SkipThought, bertsentenceembeddings1, bertsentenceembeddings2, bertsentenceembeddings3, poly-encoders, SentEval. ", " InferSent, Universal Sentence Encoder, average GloVe embeddings, BERT, RoBERTa. \n", " InferSent and Universal Sentence Encoder, ", " InferSent, Universal Sentence Encoder, Skip-Thought, poly-encoders, average GloVe embeddings. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, BERT cross-encoder, and GloVe embeddings. \n", " InferSent, Universal Sentence Encoder, Average GloVe embeddings. \n", " InferSent, Universal Sentence Encoder, Skip-Thought, BERT. ", " InferSent, Universal Sentence Encoder, and GloVe, ", " InferSent, Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, Skip-Thought, poly-encoders. \n\n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, BERT cross-encoder. ", " InferSent, Universal Sentence Encoder, GloVe, average GloVe, and poly-encoders. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, bertsentenceembeddings1, bertsentenceembeddings2, bertsentenceembeddings3. \n", " InferSent, Universal Sentence Encoder, BERT, RoBERTa. \n", " InferSent and Universal Sentence Encoder, ", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder, \n", " InferSent and Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder, ", " InferSent and Universal Sentence Encoder. \n", " Universal Sentence Encoder, InferSent. \n", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. \n", " [Your final answer here], ", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and average BERT embeddings/CLS-token output from BERT. \n", " InferSent, Universal Sentence Encoder, SkipThought, hill-etal-2016-learning, yang-2018-learning, polyencoders, RoBERTa, XLNet, bertsentenceembeddings1, bertsentenceembeddings2, bertsentenceembeddings3, and GloVe. \n", " InferSent, Universal Sentence Encoder, and average GloVe embeddings, \n", " InferSent, Universal Sentence Encoder, and others. \n", " InferSent and Universal Sentence Encoder, ", " InferSent and Universal Sentence Encoder, ", " Average GloVe embeddings, InferSent, Universal Sentence Encoder, BERT, and RoBERTa. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, and GloVe. \n", " InferSent and Universal Sentence Encoder, ", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, Average GloVe embeddings, BERT (using average embeddings or CLS-token output), Skip-Thought, Universal Sentence Encoder, and RoBERTa. ", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and average BERT embeddings, BERT CLS-token output. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. ", " InferSent, Universal Sentence Encoder, Average GloVe embeddings, and Poly-encoders. ", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and average BERT embeddings. \n\n", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder, ", " InferSent and Universal Sentence Encoder. ", " InferSent, Universal Sentence Encoder, and GloVe embeddings. ", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and RoBERTa. \n\n", " InferSent and Universal Sentence Encoder. ", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and average BERT embeddings / CLS-token output. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings. ", " Universal Sentence Encoder and InferSent, ", " InferSent, Universal Sentence Encoder, and average GloVe embeddings. \n", " Average GloVe embeddings, InferSent, Universal Sentence Encoder, BERT, and RoBERTa, \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, polyencoders. \n", " \n", " InferSent, Universal Sentence Encoder, Skip-Thought, bertsentenceembeddings1, bertsentenceembeddings2, bertsentenceembeddings3, polyencoders, average GloVe embeddings, RoBERTa, XLNet. \n", " InferSent, Universal Sentence Encoder, average GloVe embeddings, and the BERT CLS-token output. \n\n", " InferSent and Universal Sentence Encoder. \n", " InferSent, Universal Sentence Encoder, Skip-Thought, poly-encoders, RoBERTa, bertsentenceembeddings1, bertsentenceembeddings2, bertsentenceembeddings3. \n", " InferSent and Universal Sentence Encoder. \n", " InferSent and Universal Sentence Encoder, among others. \n"], "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": [" Yes, ", " Yes, ", " Yes, the authors implemented baselines and performed close data inspection to ensure probe quality. \n", " Yes, \n", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, the article mentions \"close data inspection to ensure probe quality\" and \"carefully constructed baselines\" to ensure the quality of the automatically constructed datasets. \n", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " No, \n", " Yes, ", " Yes, ", " yes, ", " No, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " No, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " No, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, the datasets are subject to careful manipulations and close data inspection. \n", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " No, ", " yes, ", " Yes, ", " yes, ", " No, unanswerable, yes. \n", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " No, ", " Yes, ", " yes, ", " Yes, ", " Yes, \n", " Yes, ", " yes, ", " Yes, ", " Yes, the authors describe several methods to ensure the quality of the automatically constructed datasets, including the use of baselines and close data inspection. \n\n", " Yes, \n", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, \n", " Yes, "], "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": [" yes, ", ", ", " yes \n", " no, \n", " No, ", " yes, ", " No, ", " Yes, ", " unanswerable, ", " yes, ", " yes, ", " no, ", " [Your concise final answer here], ", " no, ", " unanswerable, ", " Yes, \n", " yes, ", " Yes, \n", " unanswerable, ", " no, ", " yes, ", " No, ", " no, ", " yes, ", " yes, ", " no, ", " Yes, \n", " Yes, ", " yes, ", " No, \n", " Yes, ", " no, ", " Yes, ", " yes, ", " unanswerable, ", " unanswerable, ", " No, \n", " unanswerable, ", " unanswerable, ", " yes, ", " yes, ", " no, ", " no, ", " yes, ", " No, ", " unanswerable, ", ", ", " no, ", " Yes, ", " no, ", " Yes, ", " unanswerable, ", " yes, ", " Yes, ", " no, ", " no, \n", " No, ", " yes, ", " Yes, ", " No, ", " yes, ", " yes, ", " No, ", " yes, ", " unanswerable, ", " no, ", " No, ", " unanswerable, ", " yes, ", " unanswerable, ", " no, ", " yes, ", " no, ", " No, unanswerable \n", " no, ", " no, ", " no, ", " unanswerable, ", " no, ", " no, ", " no, ", " Yes, ", " Yes, ", " unanswerable, ", " no, ", " No, \n", " No, ", " yes, ", " yes, ", " Yes, ", " yes, ", " no, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " No, ", ", "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": [" unanswerable \n", " unanswerable, ", " No, ", " no, ", " No, \n", " no, ", " No, ", " unanswerable, ", " No, ", " no, ", " no \n", " no, ", " unanswerable, ", " No, ", " unanswerable, ", " No, \n", " no, ", " No, ", " No, ", " unanswerable, ", " no, ", " unanswerable, ", " No, ", " no, ", " No, \n", " unanswerable, ", " unanswerable, ", " No, ", " no, ", " no, ", " no, ", " no, ", " No \n", " unanswerable, ", " No, ", " no, \n", " No, \n", " no, ", " no, ", " No, ", " no, ", " ", " no, ", " no, ", " No, ", " unanswerable \n", " No, ", " no, ", " no, ", " no, ", " no, ", " no, ", " no \n", " unanswerable, ", " no, ", " No, \n", " unanswerable, ", " unanswerable, ", " No \n", " no, \n", " No, ", " no, ", " No, ", " No, \n", " no, ", " unanswerable, ", " No, ", " no, \n", " No, ", " No, ", " no, ", " no, ", " No, ", " no, \n", " no, ", " no, ", " no, ", " no, ", " No, ", " No, ", " No, ", " No, ", " No, ", " no, ", " no, ", " no \n", " No, ", " no, ", " No, ", " No, ", " No, ", " no, ", " No, ", " No, ", " unanswerable, ", " No, \n", " No, ", " No, \n", " No, ", " No, \n"], "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": [" Outperforms BERT-MRC by +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0, achieves F1 improvements by +0.97 and +2.36 for MSRA and OntoNotes4.0 respectively. \n", "  +0.29, +0.96, +0.97, +2.36, \n", " +0.29 and +0.96, +0.97 and +2.36. \n", " F1 improvements by +0.29 for English and +0.96 for OntoNotes5.0, +0.97 for MSRA and +2.36 for Chinese, \n", " The method outperforms the best baseline results by +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, and +2.36 for OntoNotes4.0 respectively. \n", " The proposed method achieves F1 improvements by +0.29 and +0.96 for English and Chinese datasets respectively. \n", " Outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets, and achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively. \n", " +0.29 and +0.96 for F1 improvements, respectively, on CoNLL2003 and OntoNotes5.0. Also, +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively. \n\n", " 93.33 (+0.29) and 92.07 (+0.96), \n", " 0.96, 2.36. \n", " +0.29 and +0.96 for English datasets, and +0.97 and +2.36 for Chinese datasets, respectively. \n\n", " +0.29, +0.96, +0.97, +2.36. \n", " +0.29, +0.96, +0.97, +2.36, \n", " +0.96 and +2.36. \n", " +0.96 for OntoNotes5.0 and +0.29 for CoNLL2003 in F1 score. \n", " +0.96, +0.29 for English and huge performance boosts on Chinese datasets. \n", " +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, and +2.36 for OntoNotes4.0, ", " 0.96, 2.36, \n", " +0.29, +0.96, +0.97, and +2.36, \n", " Outperforms BERT-MRC by +0.29 and +0.96, achieving F1 improvements by +0.97 and +2.36, respectively. \n", " 93.33, 92.07, 84.47, 96.72, \n", " Outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively for Chinese datasets. \n", " +0.29 and +0.96 for English datasets, and +0.97 and +2.36 for Chinese datasets, ", " +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0 in English datasets, and +0.97 for MSRA and +2.36 for OntoNotes4.0 in Chinese datasets. \n", " +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, and +2.36 for OntoNotes4.0. \n", " Outperforming BERT-MRC by +0.29 on CoNLL2003 and +0.96 on OntoNotes5.0 for English datasets, achieving F1 improvements by +0.97 on MSRA and +2.36 on OntoNotes4.0 for Chinese datasets. \n", " +0.29, +0.96, +0.97, and +2.36. \n", " F1 improvements by +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0 for English, F1 improvements by +0.97 for MSRA and +2.36 for OntoNotes4.0 for Chinese. \n\n", " F1 improvements of +0.29 and +0.96 on English datasets including CoNLL2003 and OntoNotes5.0 respectively, and +0.97 and +2.36 on Chinese datasets, i.e., MSRA and OntoNotes4.0, respectively. \n", " The proposed method achieves F1 improvements of +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0 for English datasets, and F1 improvements of +0.97 for MSRA and +2.36 for OntoNotes4.0 for Chinese datasets. \n", " +0.29 and +0.96 for English datasets and huge performance boosts for Chinese datasets with F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively. \n\n", " Outperforms BERT-MRC by +0.29 and +0.96 respectively on CoNLL2003 and OntoNotes5.0, +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively. \n", " Our method outperforms BERT-MRC by +0.29 and +0.96 in terms of F1 score on English datasets including CoNLL2003 and OntoNotes5.0, respectively; it achieves F1 improvements by +0.97 and +2.36 on Chinese datasets, MSRA and OntoNotes4.0, respectively. \n", " The proposed method outperforms BERT-MRC by +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, and +2.36 for OntoNotes4.0. \n", " +0.29, +0.96, +0.97, and +2.36. \n", " Our proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively on CoNLL2003 and OntoNotes5.0. Meanwhile, we achieve F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively. \n\n", " +0.29, +0.96, +0.97, +2.36. \n", " +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0, and +0.97 and +2.36 for Chinese datasets. \n", " +0.29 and +0.96 for English datasets, +0.97 and +2.36 for Chinese datasets. \n", " +0.29, +0.96, +0.97, and +2.36 \n", " \n", " Outperforms BERT-MRC by +0.29 and +0.96 respectively on English datasets, +0.97 and +2.36 on Chinese datasets. \n", " The method achieves improvements of +0.29 and +0.96 in F1 score for CoNLL2003 and OntoNotes5.0, and +0.97 and +2.36 for MSRA and OntoNotes4.0. \n", " +0.96 and +2.36, ", " +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, +2.36 for OntoNotes4.0, \n", " +0.29, +0.96, +0.97, +2.36, \n", " +0.29 and +0.96 for English and huge performance boosts on Chinese datasets. \n", " +0.29, +0.96 and +2.36, \n", " The proposed method achieves F1 improvements by +0.29 for English dataset CoNLL2003, and by +0.96 for English dataset OntoNotes5.0, while it achieves F1 improvements by +0.97 for Chinese dataset MSRA and by +2.36 for Chinese dataset OntoNotes4.0. \n\n", " +0.29 and +0.96 for English datasets, and F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. \n", " +0.29 and +0.96 for CoNLL2003 and OntoNotes5.0, +0.97 and +2.36 for MSRA and OntoNotes4.0, respectively. \n", " +0.29 and +0.96 for English, +0.97 and +2.36 for Chinese. \n", " +0.29, +0.96, +0.97, and +2.36, \n", " +0.96 and +0.29 respectively. \n", " F1 improvements of +0.29 and +0.96 for CoNLL2003 and OntoNotes5.0, +0.97 and +2.36 for MSRA and OntoNotes4.0, respectively. \n", " +0.29, +0.96, +0.97, and +2.36. \n", " The proposed method outperforms BERT-MRC by +0.29 and +0.96 on English datasets, achieves F1 improvements by +0.97 and +2.36 on Chinese datasets. \n", " +0.96 and +2.36, \n", " +0.96 on OntoNotes5.0, +0.29 on CoNLL2003, +2.36 on OntoNotes4.0, +0.97 on MSRA, \n", " +0.29, +0.96, +0.97, +2.36, \n", " +0.29 and +0.96 on CoNLL2003 and OntoNotes5.0 respectively, +0.97 and +2.36 on MSRA and OntoNotes4.0 respectively. \n", " +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, +2.36 for OntoNotes4.0. \n", " +0.29 and +0.96 for English, and +0.97 and +2.36 for Chinese, respectively. \n", " +0.29, +0.96, +0.97, +2.36. \n", " +0.29 and +0.96 for English datasets and +0.97 and +2.36 for Chinese datasets. \n", " +0.29 for CoNLL2003, +0.96 for OntoNotes5.0, +0.97 for MSRA, +2.36 for OntoNotes4.0. \n", " Achieves F1 improvements by +0.29, +0.96, +0.97, and +2.36 on CoNLL2003, OntoNotes5.0, MSRA and OntoNotes4.0, respectively. \n", " Achieve F1 improvements by +0.29, +0.96, +0.97, and +2.36 for the respective datasets, ", " +0.29, +0.96, +0.97, +2.36. \n", " +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0 for English datasets, +0.97 for MSRA and +2.36 for OntoNotes4.0 for Chinese datasets, respectively. \n", " Outperforms BERT-MRC by +0.29 on CoNLL2003 and +0.96 on OntoNotes5.0, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively.\n", " +0.96 for OntoNotes5.0, +0.29 for CoNLL2003, +0.97 for MSRA, +2.36 for OntoNotes4.0. \n", " +0.29, +0.96 and +0.97, +2.36 respectively. \n", " +0.29, +0.96, +0.97, and +2.36, ", " +0.29 and +0.96, +0.97 and +2.36. \n", " The method outperforms BERT-MRC by +0.29 and +0.96 in terms of F1 score on CoNLL2003 and OntoNotes5.0, respectively, and achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. \n\n", " +0.96 and +2.36 respectively, ", " +0.96 for CoNLL2003 and +2.36 for OntoNotes4.0. \n", " F1 improvements of +0.29 and +0.96 respectively for English and Chinese datasets. \n", " +0.29 and +0.96, +0.97 and +2.36 respectively. \n", " The proposed method achieves F1 improvements by +0.29 and +0.96 for English datasets (CoNLL2003 and OntoNotes5.0), and F1 improvements by +0.97 and +2.36 for Chinese datasets (MSRA and OntoNotes4.0). \n", " Our proposed method outperforms BERT-MRC by +0.29 and +0.96 in terms of F1-score on CoNLL2003 and OntoNotes5.0 respectively. Additionally, for Chinese datasets including MSRA and OntoNotes4.0, the proposed method achieves F1 improvements by +0.97 and +2.36 respectively., \n", " +0.29 and +0.96 for CoNLL2003 and OntoNotes5.0, and +0.97 and +2.36 for MSRA and OntoNotes4.0 respectively, ", "  +0.29 and +0.96, +0.97 and +2.36, \n", " +0.29, +0.96, +0.97, +2.36, \n", " +0.29 and +0.96 for English datasets, and +0.97 and +2.36 for Chinese datasets. \n", " 0.29, 0.96, 0.97, 2.36. \n", " +0.29 and +0.96 for English datasets including CoNLL2003 and OntoNotes5.0, and huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. \n\n", " Our proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively for English datasets including CoNLL2003 and OntoNotes5.0, and achieves F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0 for Chinese datasets. \n\n", " +0.29 and +0.96 for F1 for CoNLL2003 and OntoNotes5.0 respectively, and +0.97 and +2.36 for F1 for MSRA and OntoNotes4.0 respectively, ", " The method's improvements of F1 for NER task for English datasets are +0.29 and +0.96, while the improvements for Chinese datasets are +0.97 and +2.36, respectively. \n", " Our proposed method outperforms BERT-MRC by +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0 in terms of F1 score, and achieving F1 improvements by +0.97 on MSRA and +2.36 on OntoNotes4.0, respectively. \n", " The proposed method achieves F1 improvements by +0.29 and +0.96 for CoNLL2003 and OntoNotes5.0 respectively, +0.97 and +2.36 for MSRA and OntoNotes4.0 respectively. \n\n", " The proposed method improves F1 by +0.29 on CoNLL2003, +0.96 on OntoNotes5.0, +0.97 on MSRA, and +2.36 on OntoNotes4.0. \n", " \n", " +0.29 and +0.96 on F1 score for CoNLL2003 and OntoNotes5.0 respectively, +0.97 and +2.36 on F1 score for MSRA and OntoNotes4.0 respectively. \n", " +0.29 for English dataset CoNLL2003, +0.96 for English dataset OntoNotes5.0, +0.97 for Chinese dataset MSRA, and +2.36 for Chinese dataset OntoNotes4.0, ", " +0.29 for CoNLL2003 and +0.96 for OntoNotes5.0, +0.97 for MSRA and +2.36 for OntoNotes4.0, \n", " Outperforms BERT-MRC by +0.29 and +0.96 respectively. \n", " +0.96 for English OntoNotes5.0, +0.29 for English CoNLL2003, +0.97 for Chinese MSRA, and +2.36 for Chinese OntoNotes4.0. \n"], "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": [" B1, ", " B2, ", " Baseline B1. \n", " B2. \n", " Baseline B1. \n", " Baseline B1, ", " B1, ", " B1, ", " S2, ", " [Your final answer here], ", " [Your final answer here], ", " B1, B2, and S1, S2. However, baseline S1 and S2 is used for ASP task, B1 and B2 is used for AEP task. ", " Baseline B1. \n", " B1 and B2. \n", " \n", " B1. \n", " Random Forests (RF). \n", " Random Forests, ", " B2. \n", " B1, ", " B1, \n", " [Your concise final answer here], ", " Baseline B1, ", " B1, ", " Baseline model B1, ", " Baseline model B2. \n", " B2. \n", " Baseline B1. \n", " Baseline B1, ", " B1, ", " B1. ", " Baseline B1. \n", " Baseline B1 \n", " B1., ", " B2, ", " B1, ", " S2, ", " B2, ", " B1, ", " B2, ", " B1, ", " S2, ", " B2, \n", " The baseline model used is baseline B1. \n", " Baseline B1. \n", " B1, ", " B1, ", " \n", " Random Forests (RF). \n", " B1. \n", " Baseline 1. ", " B1. \n", " Baseline Model B1 is used. \n", " The baseline model is S1. ", " Baseline B1. \n", " Baseline 1, \n", " Baseline model used is B1, ", " Baseline B1, ", " Baseline Model B1, ", " B1. \n", " Baseline Model B1. \n", " B1. \n", " Baseline B1. \n", " Random Forests (RF). \n", " B1, ", " B1, B2, S1, S2. \n", " B1, ", " B1. \n", " B1, ", " Baseline 1. \n", " B1. \n", " B1, ", " Baseline B1 is used, which uses only the salience-based features by Dunietz and Gillick. ", " \n", " Random Forests (RF) BIBREF23, ", " Random Forests (RF), ", " Random Forests (RF) \n", " Random Forests (RF) and Support Vector Machines (SVM), ", " B2. \n", " B1 and B2, ", " The baseline models used are B1 and B2. \n", " S2, ", " B1. \n", " Baseline B1. \n", " Random Forests (RF) and Support Vector Machines (SVM) are used, ", " B1, ", " B1, ", " \n", " Baseline B1. \n", " B1, B2, S1, S2. \n", " The baseline model used is the first baseline (B1) which uses only the salience-based features by Dunietz and Gillick, and the second baseline (B2) which assigns the value relevant to a pair ( INLINEFORM0, INLINEFORM1 ) if and only if INLINEFORM1 appears in the title of INLINEFORM2. ", " Baseline model used is the random forest model and support vector machine. \n", " S2. \n", " B1, ", " B1, B2, S1, S2. ", " B1, ", " Yes, Random Forests (RF) and Support Vector Machines (SVM) are used as baseline models. \n", " Baseline B1. \n", " Random Forests (RF) BIBREF23 and Support Vector Machines (SVM) BIBREF24. \n", " \n"], "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": [" Their performance was competitive or even state-of-the-art for some of the emotion labels. \n\n", ", ", " Competitive or even state-of-the-art results for some of the emotion labels, ", " Competitive or even state-of-the-art results for some of the emotion labels. \n\n", " They achieved competitive results on emotion classification tasks. \n\n", " Their model's performance was competitive, ", " Our B-M model relies on subsets of Facebook pages for training, which were chosen according to their performance on the development set as well as on the observation of emotions distribution on different pages and in the different datasets, as described in Section SECREF4. The feature set we use is our best on the development set, namely all the features plus Google-based embeddings, but excluding the lexicon. This makes our approach completely independent of any manual annotation or handcrafted resource. Our model's performance is compared to the following systems, for which results are reported in the referred literature. Please note that no other existing model was re-implemented", " Our B-M model performs competitively on emotion detection. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels. \n", " Competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n\n", " Their performance was competitive or even state-of-the-art. \n", " Competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets, ", " Competitive or even state-of-the-art results for some emotion labels. \n\n", " They achieved competitive results, ", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing standard evaluation datasets. \n", " Their performance was competitive or even state-of-the-art results for some of the emotion labels. ", " Competitive or even state-of-the-art results for some of the emotion labels. \n", " State-of-the-art results for some emotion labels and competitive or even results for other emotion labels. \n\n", " Competitive or even state-of-the-art results, ", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing standard evaluation datasets. \n\n", " Their performance on emotion detection was competitive or even state-of-the-art for some emotion labels. \n", " The performance of their models was competitive or even state-of-the-art for some emotion labels on existing evaluation datasets. \n", " Competitive or even state-of-the-art results. \n", " Their model achieved competitive results without relying on any handcrafted resource. \n", " Their model's performance achieved competitive results, ", " They achieved competitive results without relying on any handcrafted resource, with an average f-score of 0.368 on the development set. \n\n", " Their best model achieved a micro-average f-score of 0.638 on the Affective development set, and a similar performance on the other two datasets, albeit lower, in terms of classification of emotion and valence. \n", " Their performance was competitive or even state-of-the-art for some emotion labels. \n", " Our B-M model achieved competitive results. \n", " Our model's performance is competitive. \n", " Their model's performance is competitive with existing models on standard benchmarks. \n", " Their model's performance was competitive or even state-of-the-art for some of the emotion labels. \n", " Their model's performance was competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets. \n", " Our B-M model achieved competitive results. \n", " Competitive or even state-of-the-art results. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n", " Their model's performance was competitive or even state-of-the-art for some of the emotion labels. \n", " State-of-the-art results for some of the emotion labels. \n\n", " They achieved competitive results. \n", " Their model's performance was competitive or even state-of-the-art for some emotion labels on existing, standard evaluation datasets. \n", " Their performance was competitive or even state-of-the-art for some emotion labels. \n", " [Your concise final answer here], ", " Competitive or even state-of-the-art results for some of the emotion labels. \n", " Competitive or even state-of-the-art results for some emotion labels. \n", " Their model's performance achieved competitive results. \n\n", " They achieved competitive or even state-of-the-art results for some of the emotion labels. \n", " \n", " They achieved competitive results. \n", " They achieved competitive results without relying on any handcrafted resource. \n", " They achieved competitive results. \n", " Competitive or even state-of-the-art results for some of the emotion labels. \n", " Competitive or even state-of-the-art results for some of the emotion labels. \n", " Competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n\n", " Competitive results. \n", " The models achieved competitive or even state-of-the-art results for some of the emotion labels. \n\n", " Their model's performance was competitive or even state-of-the-art on existing, standard evaluation datasets. \n", " Competitive or even state-of-the-art results for some of the emotion labels, ", " State-of-the-art results for some emotion labels on existing evaluation datasets, and competitive or even results for others. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing standard evaluation datasets. \n", " They achieved competitive results for some emotion labels. \n", " The model achieves competitive results, ", " Competitive results. \n", " They achieved competitive results. \n", " \n", " Competitive or even state-of-the-art results for some of the emotion labels. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels. \n", " Their model achieved competitive results. \n", " Competitive or even state-of-the-art results for some of the emotion labels. \n", " They achieved competitive results without relying on any handcrafted resource. \n", " Competitive or even state-of-the-art results for some of the emotion labels. \n\n", " Their model's performance was competitive or even state-of-the-art for some emotion labels on existing standard evaluation datasets. \n", " Competitive or state-of-the-art results for some emotion labels on existing, standard evaluation datasets. \n\n", " Competitive or even state-of-the-art results for some emotion labels, ", " The performance was competitive or even state-of-the-art for some emotion labels. \n", " Their model achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n", " They achieved competitive results without relying on any handcrafted resource, ", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n", " Competitive or even state-of-the-art results for some of the emotion labels. \n\n", " They achieved competitive results. \n", " They achieved competitive results without relying on any handcrafted resource. \n", " Their performance was competitive or even state-of-the-art for some emotion labels on existing, standard evaluation datasets. \n", " Their model achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n", " They achieved competitive results. \n\n", " Our B-M model achieved competitive results. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels. \n", " Their model achieved competitive results. \n", " Competitive or even state-of-the-art results for some emotion labels on existing evaluation datasets. \n", " They achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n", " Their best model (B-M) had a micro-average f-score of 0.368 on the development set. ", " Their model's performance is competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n\n", " They achieved competitive results. \n", " Competitive or state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n\n", " Their best model achieved a micro-average f-score of 0.7 on the ISEAR dataset. \n", " Their performance on emotion detection achieved competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n\n", " Their model achieved competitive or even state-of-the-art results for some of the emotion labels. \n", " Competitive or even state-of-the-art results for some emotion labels. \n", " Competitive or even state-of-the-art results. \n", " Their model's performance was competitive or even state-of-the-art results for some of the emotion labels. \n", " Competitive or even state-of-the-art results for some of the emotion labels on existing, standard evaluation datasets. \n"], "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": [" They define robustness of a model as the ability to perform well when the prior knowledge is biased. \n", " Making the model more insensitive to the prior knowledge. ", " Not clearly defined, \n", " It means that the model is less sensitive to bias in prior knowledge. \n", " As the model's insensitivity to prior knowledge. \n", " Through minimizing the impact of prior knowledge bias. \n", " A model is robust if it can handle the bias in prior knowledge and predict correctly even when the number of labeled features for a certain class is less than other classes. \n", " The model is more robust when it can handle biased prior knowledge and provide desirable performance. \n", " A model is more robust if it can handle the situation where we have more knowledge for some classes but not for others. \n", " A model is robust if it is not sensitive to prior knowledge. \n", " The model is more robust when it can make the output more consistent with the reference distribution. \n", " making the model more insensitive to the prior knowledge. \n", " They define robustness of a model as the ability to perform well when the prior knowledge is biased. \n", " Robustness of a model in this context means the model's ability to perform well when provided with biased prior knowledge. \n", " The ability to handle bias in prior knowledge. \n", " By using prior knowledge that is free of bias, \n", " \n", " They define robustness of a model as the model's ability to handle biases in prior knowledge. \n", " Making the model insensitive to the prior knowledge. \n", " They define robustness of a model as the model's ability to resist the effect of prior knowledge that is not appropriate for the task at hand, specifically bias in the prior knowledge. \n", " It is the ability of a model to handle situations where there is imbalance in the number of labeled features or in the class distribution. \n", " They define robustness of a model as a model that is not sensitive to the prior knowledge it is given. \n", " They define robustness as a model that cannot handle bias in prior knowledge. \n", " It is the model's ability to handle the bias in the prior knowledge. \n", " By measuring the model's ability to handle unbalanced labeled features and class distributions without being misled by prior knowledge. \n", " Robustness of a model is the ability to handle biased prior knowledge without heavily relying on it. \n", " They consider robustness of a model as being insensitive to the prior knowledge. \n", " The robustness of a model is measured by its ability to be less sensitive to biased prior knowledge. \n", " Robustness of a model is defined as being less sensitive to the prior knowledge. \n", " They define robustness of a model as the ability to make the model less sensitive to the prior knowledge. \n", " Robustness of a model is defined as its insensitivity to prior knowledge that may be biased. \n", " The model is robust when it is insensitive to biased prior knowledge, ", " They define robustness of a model as the ability of the model to resist bias in the prior knowledge that is supplied to it, which can come from various sources such as unbalanced labeled features or unbalanced dataset.\n", " \n", " Robustness of a model in this paper refers to the model's ability to be less sensitive to the prior knowledge supplied. \n", " The robustness of a model can be increased by making the model more insensitive to the prior knowledge supplied to the model. \n", " robustness of a model refers to its insensitivity to the prior knowledge. \n", " robustness of a model is the ability to handle biased prior knowledge, \n", " They define robustness as the ability of the model to be insensitive to prior knowledge. \n", " By the model's ability to not be misled by heavy bias to a particular class. \n", " Not fully defined in the article. However, a robust model should not be biased by prior knowledge that is unbalanced., ", " It means the model can handle bias in prior knowledge. \n", " Robustness of a model is defined as its ability to be insensitive to prior knowledge biases. \n", " \n", " robustness of a model is the ability of the model to handle the bias in the prior knowledge provided. \n", " Robustness to the prior knowledge that is supplied to the learning model, making the model less sensitive to biased prior knowledge. \n", " It refers to the ability of the model to perform well even when faced with biased prior knowledge. \n", " A model is robust if it can handle prior knowledge without bias.\n", " Making the model more robust involves reducing the sensibility of the prior knowledge and making the model more practical and effective. \n", " Robustness of a model against bias in prior knowledge. \n", " By a model's ability to perform well when given unbalanced prior knowledge. \n", " They define robustness of a model as a model being insensitive to the prior knowledge it receives. \n", " robustness to the bias in prior knowledge, \n", " Robustness is defined as the model's ability to handle situations with biased or limited prior knowledge. \n", " It is a model that can handle bias in prior knowledge and not misled by it, which is achieved by reducing its sensibility. \n", " Making the model more robust and practical. \n", " They define robustness of a model as being able to handle the bias in prior knowledge and making the model more insensitive to it. \n", " A model is robust if it is less sensitive to the prior knowledge. \n", " robustness of a model is defined as its ability to handle bias in prior knowledge. \n", " It is achieved by reducing the sensitivity of the prior knowledge, therefore making the model more robust and practical. \n", " The model is more robust when it can handle bias in the prior knowledge it receives. \n", " Robustness of a model refers to its ability to make accurate predictions without being misled by biased prior knowledge. \n", " To be robust, the model should not be sensitive to the prior knowledge. \n", " Robustness is related to how well the model handles bias in the prior knowledge. \n", " It can handle bias in prior knowledge, \n", " Based on the prior knowledge that it cannot handle with heavy bias. \n", " Through reducing the sensibility of the prior knowledge and making the model more robust and practical. \n", " Robustness of a model is its ability to resist the effect of unbalanced prior knowledge. \n", " Robustness of a model is defined as being insensitive to prior knowledge. \n", " Robustness of a model. ", " Robustness of a model refers to its ability to perform well even when it is given biased prior knowledge. \n", " Making the model more robust is about reducing the sensitivity of prior knowledge, \n", " It is about making the model more robust and practical, by reducing the sensibility of the prior knowledge, according to the study. \n", " They define robustness as the ability of a model to be less sensitive to prior knowledge and to make more accurate predictions despite imbalanced labeled features. \n", " To make the model more robust, they aim to make it less sensitive to the bias in the prior knowledge. \n\n", " Robustness of a model is being less sensitive to prior knowledge. \n", " They define robustness as making the model insensitive to prior knowledge bias. \n", " A model is robust when it is insensitive to the prior knowledge it is given, specifically when the knowledge is biased or incomplete. \n", "  Robustness to bias in prior knowledge. \n", " It is related to how the model handles biased prior knowledge. \n", " Robustness of a model to prior knowledge. \n", " The robustness of a model, in this context, refers to its ability to resist bias and be more stable in the face of unbalanced prior knowledge. \n", " They define robustness of a model as the ability to handle the bias in prior knowledge and not be misled by the biased information. \n", " A robust model can handle bias in prior knowledge, ", " Robustness of a model is about how sensitive the model is to the prior knowledge. \n", " They define robustness of a model as its insensitivity to the prior knowledge that it is supplied with. \n", " They define robustness of a model as the ability to handle bias in prior knowledge. \n", " Robustness of a model is its ability to handle heavy bias in prior knowledge. \n", " Based on the prior knowledge it leverages. \n", " \n", " It refers to the model's ability to handle the bias in the prior knowledge provided and to be less sensitive to the bias. \n", " Robustness is defined as the model's ability to perform well even when the prior knowledge is biased, which can occur when we provide more knowledge for some classes than others. \n", " They define robustness of a model in terms of its ability to handle biased prior knowledge. \n", " robustness is defined as the ability of a model to perform well even with biased prior knowledge, ", " Robustness of a model, \n", " By making the model less sensitive to prior knowledge, \n", " They define robustness of a model as the model's ability to handle the bias in prior knowledge supplied to it. \n", " Robustness of a model is defined as the model's ability to be less sensitive to biased prior knowledge. \n", " It means that the model should be insensitive to the prior knowledge. \n", " They define robustness as the ability to make the model more robust and practical, and not sensitive to prior knowledge with bias. \n"], "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": [" unanswerable, ", " The models they experimented with include the CAS-LSTM, the Bi-CAS-LSTM, the plain stacked LSTM, the model with different values of the vertical forget gate, the model without the vertical forget gate, and the model with peephole connections. \n\n", " Sentence Encoders and Top-layer Classifiers, ", " Several models, ", " The authors experimented with bidirectional CAS-LSTM (Bi-CAS-LSTM) models, and among them, the best performing model achieved the new state-of-the-art accuracy on SNLI and Quora Question Pairs datasets. \n", " Their models, and models with different forget gate initialization, and models without output gate, and models that integrate lower contexts via peephole connections.  ", " bidirectional CAS-LSTM network, \n", " Bi-CAS-LSTM, CAS-LSTM, SNLI, MultiNLI, Quora Question Pairs, SST, SST-2, SST-5, bidirectional CAS-LSTM, stacked LSTM, \n", " They experimented with several models including bidirectional CAS-LSTM network, plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections. \n\n", " Cell-aware Stacked LSTMs (CAS-LSTMs), bidirectional CAS-LSTMs (Bi-CAS-LSTMs), Multidimensional LSTMs (MDLSTM), Tree-structured LSTMs (Tree-LSTMs), grid LSTMs, and plain stacked LSTMs. \n\n", " Bidirectional CAS-LSTM, plain stacked LSTMs, models with different additional forget gates, models without additional forget gates, and models that integrate lower contexts via peephole connections. \n", " models with different vertical forget gates. ", " Various models, including a bidirectional CAS-LSTM network, \n", " Bidirectional Cell-aware Stacked LSTMs (Bi-CAS-LSTM), ", " bidirectional CAS-LSTM, ", " CAS-LSTM, stacked LSTM, peephole connections. ", " They experimented with the CAS-LSTM model, the stacked LSTM, and a variant with peephole connections. They also experimented with different cell state gate weights. They used a model that uses the same cell state weights for every layer and one that allows the cell state gate weights to be trained. They used both the 2D and 1D variants. They used the CAS-LSTM model, the Bi-CAS-LSTM model, the stacked LSTM model, and the model that integrates lower contexts via peephole connections. They also used the model with the cell-aware architecture. \n", " Bi-CAS-LSTM, \n", " The proposed architecture, CAS-LSTMs, and stacked LSTMs. ", " Bi-CAS-LSTM, ", " Multidimensional RNNs, bidirectional CAS-LSTM network, Top-layer Classifiers. \n", " Their models, the proposed architecture and variants, on natural language inference, paraphrase identification, and sentiment classification. ", " They experimented with CAS-LSTM, Bi-CAS-LSTM, plain stacked LSTMs, LSTMs with different peephole weights, LSTMs with peephole connections, and two-layer LSTMs with peephole connections. ", " The proposed CAS-LSTM architecture, stacked LSTMs, and multidimensional RNNs were experimented with., ", " They experimented with CAS-LSTMs and Bi-CAS-LSTMs, and also compared with the performance of plain stacked LSTMs and models with different variants. \n", " They experimented with several models including Cell-aware Stacked LSTMs (CAS-LSTMs), plain stacked LSTMs, and some variants of CAS-LSTMs. They also experimented with bidirectional CAS-LSTMs for the natural language inference and paraphrase identification tasks, and they used tree-structured LSTMs, multidimensional RNNs, and grid LSTMs for comparison. \n", " Unanswerable, ", " CAS-LSTMs, conventional stacked LSTMs, Tree-LSTMs, multidimensional RNNs, and bidirectional CAS-LSTMs. \n", " The proposed model CAS-LSTM and some variants of it. \n", " Various models, ", " They experimented with a bidirectional CAS-LSTM network, a 2-layer CAS-LSTM model, and a CAS-LSTM model, and also they experimented with tree-structured LSTMs, Tree-LSTMs, multidimensional RNNs, multidimensional LSTMs, and Grid LSTMs. \n\n", " Bidirectional Cell-aware Stacked LSTM (Bi-CAS-LSTM), ", " They experimented with several models including CAS-LSTM and bidirectional CAS-LSTM, ", " bidirectional CAS-LSTM, ", " various models, ", " They experimented with various CAS-LSTM models with different settings and architectures. \n", " They experimented with a CAS-LSTM, a Bi-CAS-LSTM, a stacked LSTM, and several variants of the CAS-LSTM model. \n", " They experimented with various models, including Bi-CAS-LSTM, bidirectional Cell-aware Stacked LSTMs, which is a two-layer bidirectional CAS-LSTM network, and other sentence encoder networks, as well as models that use plain stacked LSTMs, models with different forget gates, models without the additional forget gate, and models that integrate lower contexts via peephole connections. \n", " They experimented with stacked LSTMs, CAS-LSTMs, multidimensional RNNs, and Tree-LSTMs.\n", " They experimented with CAS-LSTM, plain stacked LSTMs, models with different vertical forget gates, models without vertical forget gates, and models that integrate lower contexts via peephole connections. \n", " Bi-CAS-LSTM and Child-Sum Tree-LSTMs and Weight-unshared Tree-LSTMs, ", " bidirectional CAS-LSTM and 3-layer CAS-LSTM, ", " Bi-CAS-LSTM and CAS-LSTM, ", " Their models. ", " Unanswerable, ", " Various models, ", " Cell-aware Stacked LSTMs and some variants, including plain stacked LSTMs and models with different and without, as well as models that integrate lower contexts via peephole connections. Also, they experimented with bidirectional CAS-LSTMs. \n", " Various models, ", " They experimented with CAS-LSTM and various CAS-LSTM variants, including models with plain stacked LSTMs, different cell state selectors, and models without the additional forget gate., ", " unanswerable, ", " various models including stacked LSTMs and Bi-CAS-LSTMs, and other models with variants such as without additional forget gate or using peephole connections, ", " Various models, ", " They experimented with CAS-LSTMs, plain stacked LSTMs, models with different vertical forget gates, models without output gates, and models that integrate lower contexts via peephole connections. \n", " Sentence encoders, Top-layer classifiers and other models. \n\n", " Various models were experimented with. \n", " They experimented with bidirectional CAS-LSTM and top-layer classifiers, specifically with a bidirectional architecture Bi-CAS-LSTM, a 1024D MLP, a 300D MLP, an MLP classifier, a fully-connected softmax layer, and the Stanford Sentiment Treebank. They also experimented with heuristic functions proposed by BIBREF36 and BIBREF37 in feature extraction, specifically a function to compute feature vectors for natural language inference and paraphrase identification experiments, and another function for sentiment classification. Additionally, they experimented with models that use plain stacked LSTMs, models with different forget gates, models without the additional", " They experimented with CAS-LSTM, plain stacked LSTM, variants of CAS-LSTM, and also Bi-CAS-LSTM. \n", " They experimented with CAS-LSTMs, bidirectional CAS-LSTMs, and stacked LSTMs. They also experimented with variants of the models, such as models without the forget gate or with peephole connections, and compared their performance to a baseline model, which was also an LSTM. However, the baseline model used a different type of LSTM, called \"plain stacked LSTMs.\" In addition, the authors experimented with different variants of the forget gate, including constant forget gates and learnable forget gates.  The authors also tested their proposed architecture, CAS-LSTM, on multiple benchmark tasks, such as natural language", " They experimented with CAS-LSTMs, Bi-CAS-LSTMs, stacked LSTMs, and Tree-LSTMs. \n", " Various models, including plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections. \n", " They experimented with CAS-LSTMs and various variants of models, including plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections. \n", " They experimented with Cell-aware Stacked LSTM (CAS-LSTM), Plain Stacked LSTM, bidirectional CAS-LSTM, and other model variants. \n\n", " Bi-CAS-LSTM, ", " Their models. \n", " Bi-CAS-LSTM and other models, ", " They experimented with their proposed CAS-LSTM model and the following variants: plain stacked LSTMs, models with different  or, models without, and models that integrate lower contexts via peephole connections. \n", " Sentence encoders, Top-layer classifiers, and Cell-aware Stacked LSTMs, ", " They experimented with plain stacked LSTMs, models with different vertical forget gates, models without vertical forget gates, and models that integrate lower contexts via peephole connections, ", " Their models, plain stacked LSTMs, models with different vertical forget gates, models without vertical forget gates, and models that integrate lower contexts via peephole connections. \n", " Sentence Encoders, Top-layer Classifiers, and bidirectional CAS-LSTM. ", " They experimented with several models including plain stacked LSTMs, models with different vertical forget gates, models without the output gate, and models that integrate lower contexts via peephole connections. \n", " they experimented with CAS-LSTMs and stacked LSTMs, \n", " Models that use plain stacked LSTMs, models with different weight initialization, models without peephole connections, and models that integrate lower contexts via peephole connections. \n", " Bidirectional Cell-aware Stacked LSTM (Bi-CAS-LSTM), ", " Various models. \n", " Bidirectional CAS-LSTM and plain stacked LSTMs, among others. ", " They experimented with models using plain stacked LSTMs, models with different vertical forget gates, models without the additional forget gate, and models that integrate lower contexts via peephole connections. \n", " Bidirectional CAS-LSTM network, ", " The CAS-LSTMs, the conventional stacked LSTMs, the model without INLINE0 being constant and trainable parameters, the model without INLINE1, the model that integrates lower contexts via peephole connections. ", " \n", " CAS-LSTM, plain stacked LSTMs, bidirectional CAS-LSTM, Tree-LSTMs, Multidimensional RNNs (MDRNN), Multidimensional LSTMs (MDLSTM), Grid LSTMs, and models with peephole connections. \n", " Bidirectional Cell-aware Stacked LSTM (Bi-CAS-LSTM) and standard stacked LSTM., \n", " The proposed CAS-LSTM and conventional stacked LSTMs, as well as bidirectional CAS-LSTMs, along with variants including plain stacked LSTMs, models with different vertical forget gates, models without the cell state gate, and models that integrate lower contexts via peephole connections. \n\n", " CAS-LSTM, Bidirectional CAS-LSTM, Plain Stacked LSTM, LSTM without forget gate, and LSTM with peephole connections, ", " The proposed architecture CAS-LSTM, and several variants of it and plain stacked LSTMs., ", " They experimented with bidirectional CAS-LSTM and conventional stacked LSTMs, and also variants of models that use plain stacked LSTMs, models with different weights for the forget gate, models without the forget gate, and models that integrate lower contexts via peephole connections. \n", " Their models, models with different forget gates, models without output gates, and models with peephole connections.  ", " Various models, ", " Various, ", " Bi-CAS-LSTM, ", " Cell-aware Stacked LSTM (CAS-LSTM), plain stacked LSTMs, models with different forget gates, models without output gates, and models that integrate lower contexts via peephole connections., ", " The authors experimented with various models including stacked LSTMs, bidirectional CAS-LSTMs, and multidimensional RNNs. However, the main focus was on the CAS-LSTM architecture and its variants., ", " Sentence Encoders, Top-layer Classifiers, Cell-aware Stacked LSTMs, Bi-CAS-LSTM, and Bidirectional CAS-LSTMs,", " Several models including plain stacked LSTMs, CAS-LSTMs, Bi-CAS-LSTMs, peephole connections, tree-structured RNNs, multidimensional RNNs, and others were experimented with. However, they mainly focused on comparing the performance of CAS-LSTMs and plain stacked LSTMs. ", " Sentence Encoders, Top-layer Classifiers and their variants, ", " Bidirectional CAS-LSTM and conventional stacked LSTMs, ", " Bidirectional Cell-aware Stacked LSTMs and regular stacked LSTMs, ", "  ", " They experimented with various models including models that use plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections.\n\n\n", "  CAS-LSTMs, stacked LSTMs, bidirectional CAS-LSTMs, plain stacked LSTMs, and models that integrate lower contexts via peephole connections. \n"], "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": [" Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask. \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, \n", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, ", " Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask, "], "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": [" syntactic tree-based models and other neural models, ", " The baselines they compared against include syntactic tree-based models, latent tree models, and other neural models. \n\n", " various other models, ", " Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " The syntactic tree-based models and the latent tree models. \n", " Latent tree-based model, non-tree models, syntactic tree-based models. \n", " \n", " They compared against latent tree-based model, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders, as well as BiLSTM with generalized pooling. \n", " Syntactic tree-based models, latent tree models, non-tree models. \n", " syntactic tree-based models and latent tree models. ", " syractic tree-based models and latent tree models, ", " yes, ", " The latent tree-based model, syntactic tree-based models, and other neural models. \n", " The latent tree-based model and other neural models, \n", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. ", " the previous syntactic tree-based models and other neural models. \n", " Their model, syntactic tree-based models, and latent tree models. \n", " yes, ", " Syntactic tree-based models and latent tree models, ", " Latent tree-based model, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " Other neural models, syntactic tree-based models, and latent tree models. \n", " BIBREF10, BIBREF11, BIBREF35, BIBREF4, BIBREF36, BIBREF37, BIBREF38, ", " Other neural models, syntactic tree-based models, and latent tree models. \n", " BIBREF16, BIBREF9, BIBREF18, BIBREF10, BIBREF11, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " Their models, various other neural models. \n", " SST-2, SST-5, MR, SUBJ, TREC, latent tree-based model, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " Latent tree-based model, syntactic tree-based models, ELMo, non-tree models. \n", " They compared against other neural models. \n", " Latent tree-based model, neural models built on both syntactic trees and latent trees, non-tree models, Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " The recent latent tree-based model and neural models built on both syntactic trees and latent trees. ", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. ", " syntactic tree-based models and latent tree models. ", " sybolic tree-based models and latent tree-based models. \n", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders, ", " various other models on syntactic trees and latent trees, as well as non-tree models. \n", " Syntactic tree-based models, latent tree models, and other neural models. \n", " None, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. ", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n", " syntactic tree-based models, latent tree models, neural models built on both syntactic trees and latent trees, and non-tree models. ", " Various syntactic tree-based models as well as other neural models. ", " Tree-based CNN, Latent Syntax Tree-LSTM, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling, \n", " Several sentence-level models, including latent tree-based models and non-tree models. \n", " Other neural models, syntactic tree-based models, and latent tree models. \n\n", " Tree-based CNN, Latent Syntax Tree-LSTM, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n\n", " Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n", " syntactic tree-based models, latent tree models, and neural models. \n", " Syntactic tree-based models, latent tree-based model, and non-tree models. \n", " syntactic tree-based models and latent tree models, ", " syntactic tree-based models, latent tree models, and other neural models. ", " Tree-based CNN, Latent Syntax Tree-LSTM, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders and BiLSTM with generalized pooling. \n", " Tree-based CNN, Latent Syntax Tree-LSTM, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders. \n", " Syntactic tree-based models and other neural models, including latent tree models. \n", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " various other models on both syntactic trees and latent trees, as well as the non-tree models. \n", " BIBREF10, BIBREF11, BIBREF35, BIBREF4, BIBREF36, BIBREF37, BIBREF38, \n", ", ", " Latent tree-based model, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. ", " None (unanswerable), ", " Other neural models. \n", " Previous syntactic tree-based models as well as other neural models. \n", " Syntactic tree-based models and latent tree models, as well as other neural models. \n", " The latent tree models, syntactic tree-based models, and other neural models. \n", " Other neural models built on both syntactic trees and latent trees, as well as the non-tree models. \n", " The recent latent tree-based model, syntactic tree-based models, and other neural models. \n", " latent tree-based model and several other neural models. \n", " Tree-based CNN, Latent Syntax Tree-LSTM, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " Neural models built on both syntactic trees and latent trees, as well as the non-tree models. \n", " sytactic tree-based models and latent tree models. \n", " Their model, latent tree models, syntactic tree-based models, and other neural models. \n", " None, syntactic tree-based models, latent tree models, latent syntax tree-LSTM, tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling, ELMo. \n", " Other neural models and latent tree models. \n", " syntactic tree-based models, latent tree models, and non-tree models, ", " Their SATA Tree-LSTM model, syntactic tree-based models, and latent tree models. \n", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. ", " Their model and other tree-structured models, as well as latent tree models, and some neural models. ", " Baselines included syntactic tree-based models and latent tree models, and neural models including Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders. ", " Various other models, including latent tree models and non-tree models. \n", " Tree-based CNN, Latent Syntax Tree-LSTM, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " None of the above options were mentioned as the baselines in the article, ", " Latent tree models, syntactic tree-based models, other neural models. \n", " the previous syntactic tree-based models as well as other neural models, \n", " Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n\n", " Syntactic tree-based models and latent tree models, ", " Latent tree-based model, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, BiLSTM with generalized pooling. \n", " None, ", " the previous syntactic tree-based models as well as other neural models, latent tree-based model, and structurally pre-trained models such as ELMo. \n", " syntactic tree-based models and latent tree models, ", " The latent tree-based model, syntactic tree-based models, and other neural models. \n", " The baselines included latent tree-based model, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders. \n", " Tree-based CNN, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n", " syntactic tree-based models, latent tree models, and other neural models. ", " Other neural models. \n", " The recent latent tree-based model, syntactic tree-based models, and other neural models. \n", " syntactic tree-based models and latent tree models, ", " \n", " Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. ", " Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n\n", " Several other neural models. \n", " Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, Residual stacked encoders, and BiLSTM with generalized pooling. \n", " Other neural models, including latent tree-based models, syntactic tree-based models, and non-tree models. \n"], "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": [" Manual annotation and using Flickr30K Entities to create a coreference graph and apply Louvain clustering. \n", " Manual detection of biases and unwarranted inferences using methods such as manual categorization of images, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities by creating a coreference graph and applying Louvain clustering. \n", " A browser-based annotation tool, tagging all descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities. \n", " Using a browser-based annotation tool that shows both the images and their associated descriptions. \n", " To leverage the structure of Flickr30K Entities and Louvain clustering. \n", " Using a browser-based annotation tool and part-of-speech information. \n", " Several methods, including manually detecting patterns by looking at a collection of images, and leveraging the structure of Flickr30K Entities, which enriches Flickr30K by adding coreference annotations. \n\n", " Several methods. \n", " Two methods: a browser-based annotation tool to find interesting patterns and a tool to tag all descriptions with part-of-speech information. \n", " Manually detect stereotypes, biases, and odd phrases.\n\n\n", " One method is to leverage the structure of Flickr30K Entities and another is to manually categorize images. \n", " Various methods are considered, including looking at all images where annotators have used a marker, manually categorizing each image, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities. \n", " Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities. ", " Several methods are considered to find examples of biases and unwarranted inferences, including using a browser-based annotation tool, part-of-speech tagging, and leveraging the structure of Flickr30K Entities. \n", " Several methods including browsing the images and their associated descriptions, using part-of-speech information, and leveraging the structure of Flickr30K Entities can be used. ", " Several methods are considered to find examples of biases and unwarranted inferences, including using a browser-based annotation tool and leveraging the structure of Flickr30K Entities. \n\n", " Several methods. \n", " A browser-based annotation tool and tagging all descriptions with part-of-speech information. ", " Looking at the structure of Flickr30K Entities or a browser-based annotation tool that shows both the images and their associated descriptions. \n", " Looking at all the images where the annotators have used a marker (in this case: adjectives like black, white, asian), and for those images count how many descriptions (out of five) contain a marker, as well as manually categorizing each of the baby images. ", " several methods. \n", " several methods including a browser-based annotation tool and leveraging Flickr30K Entities. ", " Manual methods and an automated method. ", " Tagging all descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities, then applying Louvain clustering to the coreference graph. \n", " Manual annotation, tagging, and coreference graph clustering. \n", " Looking at the images and their associated descriptions, tagging all descriptions with part-of-speech information, and using a coreference graph created by Flickr30K Entities. \n", " One method readers may find particularly useful is to leverage the structure of Flickr30K Entities to tag all descriptions with part-of-speech information, and to look at the coreference graph and clusters of expressions that refer to similar entities, following which Louvain clustering can be applied to the coreference graph.\n\n", " manual methods are considered to find examples of biases and unwarranted inferences.\n", " Several methods are considered. \n", " Other methods. \n", " Several methods. \n", " Another method is to tag all descriptions with part-of-speech information, or to leverage the structure of Flickr30K Entities and coreference graph, or to use manual inspection of the dataset to find examples of biases and unwarranted inferences. \n", " Several methods including a browser-based annotation tool, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities by creating a coreference graph and applying Louvain clustering are considered to find examples of biases and unwarranted inferences. \n\n", " Manual methods and part-of-speech tagging, coreference graph analysis using Flickr30K Entities. \n", " Several methods including browser-based annotation tool, tagging descriptions with part-of-speech information, and using Flickr30K Entities dataset. \n", " There are several methods to manually detect stereotypes, biases, and odd phrases. \n", " Several methods. \n", " Several methods including manual inspection of images and descriptions, annotating descriptions with part-of-speech information, and using Flickr30K Entities are considered to find examples of biases and unwarranted inferences. ", " Using a browser-based annotation tool and leveraging the structure of Flickr30K Entities by adding coreference annotations and applying Louvain clustering. \n\n\n\n", " several methods, including a browser-based annotation tool and manual categorization, as well as the structure of Flickr30K Entities. \n", " Some methods include manual detection, using a browser-based annotation tool, tagging all descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities. \n", " The methods to find examples of biases and unwarranted inferences include manually annotating the data, looking at patterns in the data, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities to create a coreference graph and apply Louvain clustering. \n", "  Various methods including manual annotation, analyzing distribution of terms used to describe entities, counting how many descriptions contain a marker, and leveraging the structure of Flickr30K Entities. \n", " several methods such as using a browser-based annotation tool, tagging descriptions with part-of-speech information, leveraging the structure of Flickr30K Entities and Louvain clustering to create a coreference graph.\n\n\n\n", " There are several methods that can be used to detect biases and unwarranted inferences. \n\n", " Manually detecting stereotypes, biases, and odd phrases by various methods. \n", " Several methods. \n", " Manual and automatic methods. \n", " several methods, including manually detecting and analyzing the data, tagging descriptions with part-of-speech information, and using the Flickr30K Entities dataset to create a coreference graph and applying Louvain clustering. \n", " To manually detect stereotypes, biases, and odd phrases, several methods are considered, including manually categorizing and visualizing the data, using the Flickr30K Entities dataset to enrich the data, and applying Louvain clustering to the coreference graph. ", " Manual methods including browser-based annotation tools and leveraging the structure of Flickr30K Entities. \n", " Several methods are considered, including manual categorization, tagging descriptions with part-of-speech information, leveraging the structure of Flickr30K Entities, and applying Louvain clustering to the coreference graph. \n\n", " They include manual categorization, leveraging Flickr30K Entities, part-of-speech information, and Louvain clustering. \n", " Manual detection methods are considered to find examples of biases and unwarranted inferences. \n", " Browser-based annotation tool and part-of-speech information, along with Louvain clustering on the Flickr30K Entities dataset. \n", " Three methods are considered to find examples of biases and unwarranted inferences: manual detection, tag all descriptions with part-of-speech information, and leverage the structure of Flickr30K Entities. \n", " Tagging descriptions with part-of-speech information and analyzing Flickr30K Entities dataset enriched with coreference annotations.\n", " Three methods are considered to find examples of biases and unwarranted inferences: manual detection, part-of-speech tagging, and leveraging Flickr30K Entities. ", " Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. Additionally, one can leverage the structure of Flickr30K Entities by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image, and then create a coreference graph and apply Louvain clustering to the coreference graph. Finally, one can manually categorize images or descriptions and create tables or graphs to see the patterns of biases or unwarranted inferences. One can also create a browser-based", " There are several methods to detect stereotype-driven descriptions, including manual detection and tagging all descriptions with part-of-speech information. ", " Browser-based annotation tool, manual categorization, tag descriptions with part-of-speech information, leveraging the structure of Flickr30K Entities. \n", " Various methods are considered, including manual annotation, tagging descriptions with part-of-speech information, leveraging Flickr30K Entities, and applying Louvain clustering. \n\n", " Manual detection using a browser-based annotation tool. \n", " Using the Flickr30K data with browser-based annotation tool. ", " Several methods including manually detecting biases, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities. \n", " Several methods. \n", " Several methods including a browser-based annotation tool, manual categorization, tagging all descriptions with part-of-speech information, leveraging the structure of Flickr30K Entities. \n", " Several methods \n", " Looking at a browser-based annotation tool and tagging all descriptions with part-of-speech information, as well as leveraging the structure of Flickr30K Entities.\n", " Several methods, including tagging descriptions with part-of-speech information, leveraging the structure of Flickr30K Entities, and creating a coreference graph by linking all phrases that refer to the same entity and applying Louvain clustering, are considered. \n", " manual categorization and coreference graph. \n", " Using a browser-based annotation tool, tagging all descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities. \n\n", " Methods include manually annotating images, using part-of-speech information, and leveraging the structure of Flickr30K Entities to create a coreference graph. \n", " Three methods. \n", " several methods are considered to find examples of biases and unwarranted inferences. ", " Tagging descriptions with part-of-speech information and leveraging the structure of Flickr30K Entities are two methods considered to find examples of biases and unwarranted inferences. ", " Manual detection and part-of-speech tagging. \n", " manual methods. ", " Manual methods, such as tagging all descriptions with part-of-speech information and leveraging the structure of Flickr30K Entities. ", " Looking at all the images where annotators have used a marker, and then manually categorizing each of the images. \n", " Several methods. \n", " Several methods are considered, including a browser-based annotation tool, tagging descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities to create a coreference graph and apply Louvain clustering. \n", " Two methods are considered to find examples of biases and unwarranted inferences: (1) making a browser-based annotation tool to show both images and their associated descriptions, and (2) using the Flickr30K Entities to create a coreference graph by linking all phrases that refer to the same entity.\n\n", "  browser-based annotation tool, part-of-speech information, Louvain clustering, and Flickr30K Entities. \n", " browser-based annotation tool, tagging all descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities. \n", " Tagging descriptions with part-of-speech information, and leveraging Flickr30K Entities to create a coreference graph and applying Louvain clustering. ", " Several methods are considered, including using a browser-based annotation tool and leveraging the structure of Flickr30K Entities. \n", " Another method is to tag all descriptions with part-of-speech information, or leveraging the structure of Flickr30K Entities by creating a coreference graph and applying Louvain clustering. \n\n\n", " Several methods are considered to find examples of biases and unwarranted inferences, including a browser-based annotation tool that shows both the images and their associated descriptions, tagging all descriptions with part-of-speech information, and leveraging the structure of Flickr30K Entities by creating a coreference graph and applying Louvain clustering to it.\n\n\n", " Manual and automated methods, including leveraging the structure of Flickr30K Entities. \n", " Manual categorization, part-of-speech tagging, and Louvain clustering. ", " Several methods are considered to find examples of biases and unwarranted inferences. ", " Browser-based annotation tool and using Flickr30K Entities. \n", " To get an idea of the kinds of stereotype-driven descriptions that are in the Flickr30K dataset, a browser-based annotation tool and leveraging the structure of Flickr30K Entities can be used.\n", " To get an idea of the kinds of stereotype-driven descriptions that are in the Flickr30K dataset, I made a browser-based annotation tool that shows both the images and their associated descriptions, and also leverage the structure of Flickr30K Entities, which enriches Flickr30K by adding coreference annotations, and applied Louvain clustering to the coreference graph, resulting in clusters of expressions that refer to similar entities.\n\n", " Two methods to detect stereotype-driven descriptions are suggested. \n", " Several methods including manually categorizing baby images, tag all descriptions with part-of-speech information, and leveraging Flickr30K Entities dataset can be used. ", " several methods, including looking at adjectives used and leveraging the structure of Flickr30K Entities. ", " Several methods including a browser-based annotation tool, manual categorization, part-of-speech information, Flickr30K Entities, and Louvain clustering are considered to find examples of biases and unwarranted inferences. \n", " Manual annotation and automated analysis.\n\n"], "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": [" No, ", " Yes, ", " yes, ", " No, ", " Yes, ", " yes, ", " no, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " No, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " no, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " no, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " No, ", " yes, ", " No, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " No, ", " yes, ", " No, ", " No, ", " Yes, ", " Yes, ", " no, ", " yes, ", " yes, ", " No, ", " yes, ", " no, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " No, \n", " Yes, ", " yes, ", " no, ", " No, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " no, ", " No, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " No, ", " yes, ", " yes, ", " No, ", " No, ", " Yes, ", " yes, "], "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": [" The two-tag and three-tag schemes are employed. \n", " A novel tagging scheme consisting of three tags, namely {0, 1, 2} and another scheme consisting of two tags {0, 1}.  ", " Three tags { INLINEFORM0 }, INLINEFORM1, and INLINEFORM2.\n", " InlineForm0, InlineForm1, and InlineForm2 tags. ", " The tagging scheme employed is a three-tag scheme, {IN, NONE, OUT}, which is able to capture the structural constraint that each context contains a maximum of one pun. ", " A three-tag scheme consisting of T, P, and O. \n", " { ONLINEFORM0 }, where ONLINEFORM0 tag means the current word is not a pun, ONLINEFORM0 tag means the current word is a pun. \n", " The INLINEFORM0 tagging scheme with three tags: {INLINEFORM0, INLINEFORM1, INLINEFORM2}, is employed. \n", " The INLINEFORM0 tagging scheme is employed. ", " Three tags, namely {P, O, O'}, where P indicates the current word is a pun, O indicates the current word appears before the pun, and O' indicates the current word appears after the pun. \n", " The novel tagging scheme to capture the structural constraint that each context contains a maximum of one pun is employed, which consists of three tags: INLINEFORM0, INLINEFORM1, and INLINEFORM2. \n\n\n", " The INLINEFORM0 tagging scheme. \n", " { O, P }, ", " The tagging scheme employed is {O, B, I, E, S, T} or {O, B, I}. \n", " The INLINEFORM0 tagging scheme consisting of two tags { INLINEFORM1 } is employed, which is then modified to the INLINEFORM2 tagging scheme consisting of three tags { INLINEFORM3 }. \n", " A three-tagging scheme, namely {INLINEFORM0}, {INLINEFORM1}, and {INLINEFORM2}. ", " A tagging scheme with three tags, namely {M, S, B}, is employed. ", " INLINEFORM0, INLINEFORM1, INLINEFORM2, INLINEFORM3, INLINEFORM4, INLINEFORM5, INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, INLINEFORM11, INLINEFORM12, INLINEFORM13, INLINEFORM14, INLINEFORM15, INLINEFORM16., \n", " The novel tagging scheme consisting of three tags {M, P, E} is employed. \n", " The tagging scheme employed is the INLINEFORM0 scheme. \n", " {0, 1}, {0, 1, 2}, or {<, =, >}. ", " Three tagging schemes are employed. ", " The tagging scheme employed is a combination of three tags {INLINEFORM0}, which indicates that the current word appears before the pun in the given context, the current word is a pun, and the current word appears after the pun. ", " [Your concise final answer here], ", " The three tags scheme. \n", " A novel tagging scheme consisting of three tags, namely {<PUN>,<UNPUN>,<BEFORE>,<AFTER>}, which is also referred to as the <PUN> scheme. \n", " It uses two tagging schemes, namely INLINEFORM0 and INLINEFORM1 for INLINEFORM2 and three tags, namely {INLINEFORM3} for INLINEFORM4. \n\n", " {O, I, B}, ", " A 3-tag tagging scheme. ", " The tagging scheme employed is a three-tag scheme. \n\n", " Two tagging schemes are employed. ", " INLINEFORM0, INLINEFORM1, INLINEFORM2 scheme. \n", " Three tags. ", " The tagging scheme employed is the {0, 1} scheme for the first simple tagging scheme and the {0, 1, 2} scheme for the new tagging scheme. ", " InlineForm0 tagging scheme and InlineForm1 tagging scheme \n", " Three tags { INLINEFORM0 }, { INLINEFORM1 }, and { INLINEFORM2 }. ", " Three tags { O, B, I }. \n", " The tagging scheme employed includes {POS or not, PUN or not}, {BEFORE, PUN, AFTER}, and {B, P, A}. \n", " The tagging scheme is the INLINEFORM0 scheme which guarantees that there is a maximum of one word that will be tagged as a pun during the testing phase. However, the INLINEFORM0 scheme was replaced with the INLINEFORM1 scheme to capture the structural constraint that there exists only one pun if the given sentence is detected as the one containing a pun. \n", " The tagging scheme employed is {INLINEFORM0, INLINEFORM1, INLINEFORM2}. However, two other schemes are also introduced which are {INLINEFORM3, INLINEFORM4, INLINEFORM5} and {INLINEFORM6, INLINEFORM7, INLINEFORM8}. \n", " INLINEFORM0 scheme, INLINEFORM2 scheme, INLINEFORM3 scheme. ", " Three tags { INLINEFORM0 }, INLINEFORM1, and INLINEFORM2, ", " The novel tagging scheme employed is {InlineForm0}, {InlineForm1} and {InlineForm2}, {InlineForm3}, and {InlineForm4}. ", " A tagging scheme with three tags, namely {P, O, B}, is employed. ", " The tagging scheme is a combination of three tags { 1, 2, 3 }. ", " The three-tagging scheme { INLINEFORM0 }, which includes { INLINEFORM1 }, { INLINEFORM2 } and { INLINEFORM3 } is employed. \n", " The three-tagging scheme { I, M, B } is employed. ", " The novel tagging scheme, which captures the structural property that there is a maximum of one pun residing in the text. ", " A tagging scheme consisting of three tags: <T1>, <T2>, <T3>. \n", " {P, N}, {B, I, O} \n", " INLINEFORM0, INLINEFORM1, INLINEFORM2, INLINEFORM3, INLINEFORM4, INLINEFORM5, INLINEFORM6, INLINEFORM7, INLINEFORM8, INLINEFORM9, INLINEFORM10, INLINEFORM11, INLINEFORM12, INLINEFORM13, INLINEFORM14, INLINEFORM15, INLINEFORM16, INLINEFORM17, INLINEFORM18, INLINEFORM19, INLINEFORM20, INLINEFORM21, INLINEFORM22, INLINEFORM23, INLINEFORM24, INLINEFORM25, INLINEFORM26, INLINEFORM27, INLINEFORM28, INLINEFORM29, INLINEFORM30, INLINE", " InlineForm0 scheme and InlineForm1 scheme. ", " Three tags {INLINEFORM0}, INLINEFORM1 and INLINEFORM2. \n\n", " The novel tagging scheme employed is the { INLINEFORM0 }, which indicates that the current word appears before the pun, INLINEFORM1 which highlights the current word is a pun, and INLINEFORM2 which indicates that the current word appears after the pun. \n", " {M, P, T}, ", " { INLINEFORM0, INLINEFORM1, INLINEFORM2 }, ", " Three-tag scheme, namely {T, S, B}. ", " { P, B, E, O}, ", " The three-tag scheme {INLINEFORM0, INLINEFORM1, INLINEFORM2} or the three-tag scheme {INLINEFORM0, INLINEFORM3, INLINEFORM4} is employed. However, it's not clear which one is correct in the answer field. ", " The novel tagging scheme consists of three tags: INLINEFORM0, INLINEFORM1, and INLINEFORM2. \n\n", " The tagging scheme employed is called the INLINEFORM2 scheme. ", " The new tagging scheme proposed by the authors is a novel scheme consisting of three tags, namely { INLINEFORM0 }, INLINEFORM1, and INLINEFORM2. \n", " The novel tagging scheme consisting of three tags {INLINEFORM0}. ", " A novel tagging scheme consisting of three tags: T1, T2, and T3. ", " The tagging scheme employed is the {INLINEFORM0, INLINEFORM1} and {INLINEFORM0, INLINEFORM1, INLINEFORM2} schemes. ", " The tagging scheme employed is a combination of three tags: INLINEFORM0, INLINEFORM1, and INLINEFORM2. \n", " The three tags { <P1>, <P2>, <P3> },", " The INLINEFORM0 tagging scheme employed. ", " [Your concise final answer here], ", " The INLINEFORM0, INLINEFORM1, and INLINEFORM2 tagging schemes are employed.  ", " {L0, L1}, {L0, L1, L2} or the INLINEFORM0, INLINEFORM1, INLINEFORM2 tagging scheme. ", " The tagging scheme employed is a 3-tag scheme and a 2-tag scheme, ", " The tagging scheme employed is {N, P}, but later changed to a novel tagging scheme {B, I, O}. ", " The new tagging scheme consisting of three tags: {I, B, O}, ", " {INLINEFORM0, INLINEFORM1}, ", " The tagging scheme is the one proposed by the authors that consists of three tags {INLINEFORM0}. ", " The proposed tagging scheme consists of three tags, namely {0,1,2}. \n", " {0, 1}, ", " INLINEFORM0 scheme with three tags. ", " Three tags, namely {INLINEFORM0}, {INLINEFORM1}, and {INLINEFORM2}, are employed. ", " { INLINEFORM0, INLINEFORM1 }, ", " The new tagging scheme consisting of three tags, namely {P}, B and E. ", " The INLINEFORM0 tagging scheme, INLINEFORM1 tagging scheme and INLINEFORM2 tagging scheme are employed. INLINEFORM0 tag indicates that the current word appears before the pun in the given context, INLINEFORM1 tag highlights the current word is a pun and INLINEFORM2 tag indicates that the current word appears after the pun. \n\n", " The two tagging schemes, namely {0}, are employed. \n", " The model uses two tagging schemes, namely { INLINEFORM0 } and { INLINEFORM0 }, and later another scheme { INLINEFORM0 } is proposed. ", " The tagging scheme employed is {T, P}, ", " The proposed tagging scheme is { O, T }. ", " {<u>,<b>}, ", " INLINEFORM0 {INLINEFORM1 INLINEFORM2 INLINEFORM3}. ", " The tagging scheme employed is a novel tagging scheme consisting of three tags, namely {INLINEFORM0, INLINEFORM1, INLINEFORM2}. INLINEFORM0 tag indicates that the current word appears before the pun in the given context, INLINEFORM1 tag highlights the current word is a pun, and INLINEFORM2 tag indicates that the current word appears after the pun. ", " The INLINEFORM0 scheme, which guarantees the context property that there exists a maximum of one pun residing in the text. ", " {T, P}, ", " The tagging scheme employed is the three-tag INLINEFORM0 scheme. \n", " The INLINEFORM2 tagging scheme is employed. ", " The tagging scheme employed is a three-tag scheme consisting of { INLINEFORM0, INLINEFORM1, INLINEFORM2 }. ", " It includes two tags, { INLINEFORM0 }, or three tags, { INLINEFORM0 }. ", " A tagging scheme consisting of three tags, namely INLINEFORM0, INLINEFORM1, INLINEFORM2, \n", " The novel tagging scheme. \n", " Two tagging schemes. \n", " The INLINEFORM0 tagging scheme and the INLINEFORM1 tagging scheme. \n"], "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": [" Relation detection. \n", " Relation detection. ", " KB relation detection. \n", " Relation detection \n", " KB relation detection. \n", " Two-step relation detection, ", " Relation detection. \n", " KB relation detection model, \n", " Relation detection, ", " Improved KB relation detection model, ", " relation detection. ", " relation detection \n", " Relation detection. \n", " KB relation detection, ", " Relation detection. \n", " Relation detection, ", " Improved KB relation detection. \n", " KB relation detection. \n", " KB relation detection. \n", " relation detection \n", " Relation detection. \n", " Relation detection. \n", " Hierarchical Residual BiLSTM (HR-BiLSTM). \n", " Relation detection \n", " The core component for KBQA is relation detection. \n", " Relation detection. \n", " KB relation detection. \n", " KB relation detection, ", " Relation detection. \n", " KB relation detection. \n", " Relation detection. \n", " Relation detection. ", " KB relation detection. \n", " Relation detection, ", " Hierarchical Residual BiLSTM. \n", " Two-step relation detection, ", " KB relation detection. ", " Relation detection \n", " Relation detection. \n", " The core component for KBQA is the relation detection subtask. \n", " Relation detection, ", " Two-step relation detection \n", " Relation detection. \n", " Relation detection, ", " Relation detection, ", " Relation detection. ", " Relation detection, ", " Relation detection. \n", " Relation detection. \n", " Relation detection \n", " KB relation detection. ", " KB relation detection model \n", " KB relation detection, ", " Relation detection. \n", " KB relation detection. \n", " Entity Linking and Relation Detection, ", " The relation detection model. \n", " Relation detection model, \n", " KB relation detection, ", " Entity linker \n", " Relation detection. \n", " Relation detection \n", " Relation detection, ", " KB relation detection. \n", " KB relation detection, ", " Relation detection, ", " KB relation detection. \n", " Relation detection. \n", " Relation detection. \n", " Hierarchical Residual BiLSTM (HR-BiLSTM) model, \n", " KB relation detection, ", " Relation detection. \n", " Relation detection. \n", " The relation detection subtask. \n", " KB relation detection. ", " Relatio detection model. \n", " Entity Re-Ranking \n", " Relation detection \n", " relation detection model, ", " Relatioin detection. \n", " Relation detection. ", " KB relation detection, \n", " KB relation detection. ", " The core component for KBQA is the relation detection model. \n", " Relation detection, ", " Relation detection. \n", " Relation detection \n", " Improved relation detection model. \n", " KB relation detection. \n", " Relation detection. ", " Relation detection. ", " Relation detection \n", " Relation detection, ", " KB relation detection. \n", " Hierarchical Residual BiLSTM. ", " Relation detection. \n", " relation detection. \n", " KB relation detection. \n", " The core component for KBQA. ", " Hierarchical Residual BiLSTM (HR-BiLSTM). \n"], "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": [" unanswerable, ", " Romance languages, ", " unanswerable, ", " They explore French. ", " French. \n", " French, ", " unanswerable ", " Romance languages and Semitic languages. \n", " They explore Romance languages, as well as Semitic languages. \n", " Romance languages, ", " French. ", " Romance languages and Semitic languages. \n", " Semitic languages. \n", " The article gives a concrete example of a Winograd schema that can be used to test a translation program. This example involves two sentences, one in English and one in the other language (French in this case). The challenge for a machine translation program is to correctly identify the referent for the ambiguous pronoun in each sentence. For instance, if the two sentences are\nJane and Susan sang a song and they danced.\nFred and George sang a song and they danced.\nThen a human can easily determine that in the first sentence, \"they\" refers to Jane and Susan, and in the second sentence, \"they\" refers to Fred and", " French, Italian, Spanish, and Arabic, ", " Romance languages, ", " Romance languages, ", " Semitic languages, ", " Semitic languages, ", " Romance languages, ", " [Your final answer here], ", " Romance languages, ", " English and other languages. \n", " French and Spanish, ", " Romance languages, ", " Romance languages and Semitic languages. \n", " French, ", " French, ", " Semitic languages. \n", " Romance languages, ", " unanswerable. ", " Romance languages, ", ", ", " Romance languages, ", " Romance languages. \n", " Romance languages, ", " They explore French, Italian, Spanish, Hebrew, and Arabic. \n", " Romance languages such as French, Italian, and Spanish. ", " Romance languages. \n", " unanswerable, ", " English, ", " Romance languages and Semitic languages. ", " Romance languages, ", " Romance languages and Semitic languages, ", " French, ", " Romance languages, ", " They explore French and Spanish, among other languages. \n", " French, ", " [Your final answer here], ", " French. \n", " Romance languages, ", " Romance languages and Semitic languages. \n", " Romance languages. ", " They explore several languages, ", " French, Italian, and Spanish, and Semitic languages such as Hebrew and Arabic. \n", " They explore many languages. \n", "  Semitic languages, ", " French. \n", " English, ", " French and English, ", " Romance languages, ", " Romance languages and other languages. ", " Semitic languages, ", " They explore French. ", " Romance languages, ", " Semitic languages. \n", " French, ", " Romance languages and Semitic languages. \n", " Romance languages, ", " Romance languages. ", " Romance languages, ", " French, ", " They explore French. ", " Romance languages, ", " Romance languages and Semitic languages. \n", " Yes. ", " Romance languages and Semitic languages, ", " in ", " Romance languages, ", " Romance languages. \n", " Romance languages and Semitic languages, ", " Romance languages. ", " Romance languages, ", " yes, ", " Semitic languages. ", " French and English. \n", " Romance languages and Semitic languages, ", " [Your concise final answer here], ", " Romance languages, ", " Romance languages. ", " French, ", " unanswerable ", " Romance languages, ", " French, ", " Romance languages and Semitic languages, ", " French, ", " Romance languages, ", " French, Italian, Spanish, Hebrew, Arabic. \n", " Romance languages, ", " Romance languages and Semitic languages, "], "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": [" significant, ", " significant, ", " A significant improvement. \n", " A significant improvement is observed. ", " A significant boost. \n", " ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics compared to baselines. \n\n", " Significant \n", " A significant improvement, ", " ALOHA demonstrates a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " A significant improvement. \n\n", " A significant boost in performance, \n", " A significant improvement. \n", " ALOHA achieves a significant improvement. \n", " Significant improvement, ", " Significant improvement, ", " ALOHA achieves a significant boost in Hits@n/N accuracy, \n", " A significant boost in Hits@n/N accuracy. \n", " A significant boost, ", " The proposed model outperforms the baselines significantly. \n", " ALOHA outperforms baselines, ", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " A significant boost. \n", " A significant improvement, ", " A significant improvement. \n", " significant, ", " A significant improvement. \n", " Significant, ", " ALOHA achieves a significant improvement over the baseline open-domain chatbot models. \n", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " A significant improvement. \n", " A significant improvement. \n", " Significant, ", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " ALOHA outperforms baselines. ", " A significant improvement. \n", " Significant improvement, ", " A significant improvement. ", " The proposed model outperforms the baselines. ", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n\n", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n\n", " Significant. \n", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " Significant. ", " ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics compared to baselines. \n", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " ALOHA demonstrates a significant improvement over the baselines. \n", " Significant. \n", " The proposed model has a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " A significant improvement, ", " A significant improvement. \n", " A significant boost in Hits@n/N accuracy. \n", " A significant improvement, ", " Significant, ", " The proposed model ALOHA has a significant boost in Hits@n/N accuracy and other metrics compared to the baseline open-domain chatbot models. \n", " Significant. \n", " ALOHA achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. ", " ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics compared to the baseline open-domain chatbot models. \n", " A significant boost in performance. \n", " Noticeable improvement, \n", " A significant improvement. ", " A significant boost, ", " Significant. ", " Significant improvement, ", " \n", " A significant improvement, ", " A significant improvement, ", " The proposed model significantly outperforms the baselines. \n", " Significant, ", " A significant improvement, ", " A significant improvement. ", " A significant improvement. \n", " Significant, ", " ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities. \n", " A significant improvement, ", " A significant boost. \n", " A significant improvement. \n", " A significant improvement. ", " A significant improvement. \n", " A significant improvement, ", " Noticeable improvement, ", " A significant improvement, ", " A significant boost. \n", " A significant improvement, ", " Significant improvement, ", " A significant boost in performance \n", " ALOHA demonstrates a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. \n", " ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities compared to the baselines. \n", " ALOHA achieves a significant improvement. \n", " Significant. \n", " significant \n", " The proposed model outperforms baselines significantly. \n", " Significant, ", " Significant, ", " A significant improvement, ", " A significant improvement is observed. \n", " ALOHA outperforms baselines, ", " Significant, ", " A significant improvement \n", " A significant improvement, "], "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": [" Sumy algorithms. \n", " Sumy package, ", " Sumy algorithms. \n", " Sumy algorithms, ", " Sumy package, ", " Sumy package, ", " Sumy package and ILP-based approach, ", " Sumy package, ", " Sumy algorithms. \n", " Sumy package, ", " Sumy algorithms, ", " Sumy package, ", " Sumy package, ", " Sumy package algorithms, ", " unanswerable, ", " Sumy package, ", " The Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy package. ", " Sumy package. \n", " Sumy package algorithms, ", " Sumy package, ", " Sumy algorithms, ", " Sumy algorithms, \n", " The authors experimented with a few summarization algorithms provided by the Sumy package. \n", " Sumy, ", " Sumy package, ", " Sumy package. \n", " Sumy package and ILP-based approach, ", " Sumy package, ", " Sumy package, ", " Sumy algorithms, \n", " Sumy package, ", " Sumy package, ", " Sumy algorithms, ", " Sumy package and ILP-based, ", " Sumy package, ", " Sumy algorithms. \n", " Sumy algorithms. \n", " Sumy package, ", " Sumy package, ", " Sumy package algorithms and ILP-based summarization, ", " ILP-based summarization and a few algorithms provided by the Sumy package, ", " Sumy package, ", " Sumy package algorithms, ", " The Sumy package. \n", " Sumy package algorithms, ", " Sumy package, ", " Sumy package, ", " The authors experimented with several algorithms including ILP-based summarization and those provided by the Sumy package. \n", " Sumy package and ILP-based approach, ", " Sumy package algorithms, ", " Sumy package, ", " The Sumy package and ILP-based summarization, ", " Sumy package. ", " Sumy package, ", " Sumy algorithms, ", " Sumy package, ", " The Sumy package, ", " Sumy algorithms, ", " The Sumy package, ", " Sumy algorithms, ", " Sumy package, ", " Sumy algorithms, ", " ILP-based summarization and Sumy package algorithms, ", " Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy package and ILP-based approach, ", " Sumy package algorithms, ", " Sumy package, ", " Sumy package, ", " Sumy package and ILP-based approach, ", " Sumy package, ", " Sumy algorithms, ", " Sumy package, ", " The authors experimented with ILP-based summarization, Sumy package, but not other algorithms. However, they also compared with the performance of Sumy algorithms. \n\n", " Sumy package, ", " Sumy package and ILP-based summarization, ", " Sumy algorithms, ", " Sumy package, ", " Sumy package, ", " Sumy package, ", " Sumy algorithms, ", " Sumy package and the ILP-based summarization. ", " Sumy algorithms and their ILP-based summarization, ", " Sumy package, ", " Sumy algorithms, ", " Sumy package and ILP-based approach, ", " Sumy algorithms, ", " Sumy algorithms. \n", " Sumy package, ", " Sumy package and ILP-based, ", " Sumy package, ", " Sumy package. \n"], "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": [" Proposed by BIBREF0 \n", " Baseline models by BIBREF0 over a larger corpus, ", " Baseline models by BIBREF0 \n", " Unanswerable, ", " Proposed probabilistic graphical models. \n", " Proposed by BIBREF0 and evaluated by BIBREF7. ", " A probabilistic graphical model proposed by BIBREF0. ", " Proposed by BIBREF0. ", " BIBREF0's model \n", " Models with discourse features, and probabilistic graphical models. \n", " Proposed by BIBREF0, \n", " Probabilistic graphical models with vocabulary dependent latent post categories. \n", " Proposed by BIBREF0. ", " Models with discourse features. \n", " Probabilistic graphical models with vocabulary dependent latent post categories. \n", " A work by BIBREF0. \n", " BIBREF7. \n", " Proposed by BIBREF0. \n", " Proposed by BIBREF0, later BIBREF7 evaluated baseline models over a larger corpus and found the results to vary widely across MOOCs. \n", " Probabilistic graphical models. \n", " Previous state of the art was proposed by BIBREF0 and BIBREF1. \n\n", " Proposed by BIBREF0. ", " Baseline models by BIBREF0. \n", " proposed systems to aid instructors to selectively intervene on student discussions where they are needed the most. \n\n", " BIBREF7 evaluated baseline models by BIBREF0 over a larger corpus and found the results to vary widely across MOOCs. \n", " Proposed by BIBREF0. \n", " Proposed by BIBREF0. ", " Probabilistic graphical models. ", " Proposed by BIBREF0. \n", " The previous state-of-the-art for instructor intervention in MOOC forums was proposed by BIBREF0. \n", " Proposed by BIBREF0, BIBREF1. \n", " Unanswerable, ", " Probabilistic graphical models that modelled structure and sequence. \n", " Proposed models by BIBREF0. \n", " The previous state of the art was the model proposed by BIBREF0. ", " BIBREF0. \n", " BIBREF0 \n", " Models with discourse features. \n", " Proposed by BIBREF0, \n", " BIBREF7. \n", " Predicting instructor intervention with probabilistic graphical models. \n", " Models with discourse features. \n", " BIBREF0 proposed probabilistic graphical models to model structure and sequence. \n", " State-of-the-art for instructor intervention in MOOC forums before this paper was proposed by BIBREF0. ", " BIBREF0, \n", " The previous state of the art was proposed by BIBREF0, but its model requires a hyperparameter for the number of latent states. \n", " Unanswerable, ", " Models with discourse features to enable better prediction over unseen MOOCs. \n", " Proposed probabilistic graphical models to model structure and sequence. \n", " Baseline models proposed by BIBREF0. \n", " Proposed models by BIBREF0. \n", " Proposed models by BIBREF0. \n", " The state-of-the-art for instructor intervention in MOOC forums before this work was proposed by BIBREF0, which required a hyperparameter for the number of latent states, and was later improved by BIBREF1 and BIBREF8. ", " BIBREF0. ", " BIBREF0. \n", " BIBREF0 and BIBREF1. \n", " BIBREF7 \n", " Proposed by BIBREF0. ", " Probabilistic graphical models, ", " Proposed by BIBREF0. ", " Proposed by BIBREF0, \n", " BIBREF0. \n", " Probabilistic graphical models. \n", " Proposed by BIBREF0, which used probabilistic graphical models. \n", " BIBREF0. \n", " Unanswerable, ", " Proposed by BIBREF0 \n", "  BIBREF7. \n", " A model proposed by BIBREF0 that modelled the thread structure and the sequence of posts explicitly improved the performance, however, this model required a hyperparameter for the number of latent states which likely would not generalise due to weak evaluation BIBREF7. \n", " Proposed by BIBREF0. \n", " Proposed by BIBREF0 \n", " Proposed by BIBREF0, then BIBREF7 evaluated baseline models. \n\n", " Probabilistic graphical models proposed by BIBREF0. \n", " The previous state of the art was proposed by BIBREF7. \n", " The previous state of the art for this task was proposed by BIBREF7. \n", " Proposed by BIBREF0. \n", " BIBREF0 \n", " The previous state of the art for this task was proposed by BIBREF7. \n", " Proposed by BIBREF0. ", " Models proposed by BIBREF0 and BIBREF7. \n", " Unanswerable. \n", " Proposed by BIBREF0, probabilistic graphical models to model structure and sequence. \n", " Prior works. \n", " Predicting instructor intervention in MOOC forums. ", " \n", " Logistic regression classifier with features inclusive of bag-of-words representation of the unigrams and thread length. \n", " BIBREF0, BIBREF1, BIBREF7, BIBREF8. \n", " The previous state of the art was proposed by BIBREF0. \n", " BIBREF0 \n", " Prior work had proposed systems to aid instructors to selectively intervene on student discussions where they are needed the most, but earlier studies on MOOC forum intervention either modelled the entire context or required the context size to be specified explicitly. \n", " proposed systems \n", " Proposed by BIBREF0. \n", " Probabilistic graphical models. \n", " Proposed by BIBREF0. \n", " Probabilistic graphical models \n", " BIBREF0 proposed probabilistic graphical models to model structure and sequence, however, their model required a hyperparameter for the number of latent states. \n\n", " Probabilistic graphical models to model structure and sequence proposed by BIBREF0. \n", "  BIBREF0 proposed probabilistic graphical models to model structure and sequence. ", " Probabilistic graphical models. \n", " Proposed by BIBREF0. \n"], "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": [" Kannada, Hindi, Telugu, Malayalam, Bengali, English, and the seventh language is not explicitly mentioned, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, and others. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, and Tamil is missing from the list. ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English and the 7th is unmentioned. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English.  ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, and they don't explicitly mention Tamil, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, and others mentioned in the table below Table 1. ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, and 7th is not specified. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, and Telugu. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, and Tamil is not present, but 7 Indian languages is 7, not 8, so it must be that Tamil is not present in 7 Indian languages listed in table below, Answer is 7 languages, the 7th is not clearly stated in the table but  according to the Table bellow it is  Malayalam (this is the conclusion I am forced to make). \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " _______________________, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, and also Kannada was included in the 7 Indian languages. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English., ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English., ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, and another Indian language, \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, and no 7th language is specified. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", ", ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, English, and 7th language is missing, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, and also another Indian language. \n", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, ", " [Your concise final answer here], ", " [Your concise final answer here], ", " Kannada, Hindi, Telugu, Malayalam, Bengali, and English, "], "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": [" Enc-Dec, NN. \n", " An Encoder-Decoder baseline with ingredient attention (Enc-Dec), a name-based Nearest-Neighbor model (NN)., ", " Enc-Dec (Encoder-Decoder) and NN (Nearest-Neighbor). \n", " The Enc-Dec model, ", " The baseline models are the Non-personalized Encoder-Decoder (Enc-Dec) model and the Name-based Nearest-Neighbor (NN) model. \n", " Enc-Dec, ", " A name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec, \n", " Enc-Dec and NN. \n", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are the name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The Encoder-Decoder baseline with ingredient attention and the Neural Checklist Model adapted. \n", " The baseline models include a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are a Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are a Nearest-Neighbor model and a simple Encoder-Decoder model with ingredient attention. \n", " Encoder-Decoder baseline with ingredient attention and a name-based Nearest-Neighbor model. \n", " The baseline models are the Nearest-Neighbor model (NN) and the Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Encoder-Decoder with ingredient attention (Enc-Dec) and Name-based Nearest-Neighbor model (NN). \n", " A name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The two baseline models. ", " The two baselines are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " The baselines are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " The two baseline models are the Encoder-Decoder and a name-based Nearest-Neighbor model (NN), however, the name-based Nearest-Neighbor model (NN) is adapted from the Neural Checklist Model of BIBREF0. \n\n", " Enc-Dec (Encoder-Decoder baseline with ingredient attention). ", " Enc-Dec model and Nearest-Neighbor model (NN) \n", " Baseline models include the name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " A name-based Nearest-Neighbor model (NN) and an Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Baseline models include a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder model with ingredient attention (Enc-Dec). \n\n", " A simple Encoder-Decoder baseline with ingredient attention (Enc-Dec) and a name-based Nearest-Neighbor model (NN). \n", " The baseline models include a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " Enc-Dec model and the Neural Checklist Model. \n", " The baselines are the Name-Based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Encoder-Decoder and Nearest-Neighbor. \n", " The baselines are NN (Nearest-Neighbor) model and Enc-Dec (Encoder-Decoder) model. \n", " Enc-Dec, NN. \n", " The Encoder-Decoder baseline with ingredient attention (Enc-Dec), the name-based Nearest-Neighbor model (NN), and the adapted Neural Checklist Model of BIBREF0. \n\n", " The baseline models are an Encoder-Decoder model and a Nearest-Neighbor model. \n", " Encoder-Decoder baseline with ingredient attention and Nearest-Neighbor model (NN). \n", " The baseline models are a Nearest-Neighbor model and a simple Encoder-Decoder model with ingredient attention. \n", " Enc-Dec and NN, ", " The baseline models are the Name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " Enc-Dec, ", " Enc-Dec and NN, ", " Encoder-Decoder and Nearest-Neighbor, ", " The baselines are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec. \n", " Baseline models are an Encoder-Decoder model with ingredient attention and a name-based Nearest-Neighbor model. \n\n", " The baselines are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " A name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " A name-based Nearest-Neighbor model and a simple Encoder-Decoder model with ingredient attention. \n", " Encoder-Decoder with ingredient attention (Enc-Dec) and Name-based Nearest-Neighbor (NN). ", " Enc-Dec and NN. \n", " A simple Encoder-Decoder baseline with ingredient attention (Enc-Dec) and a name-based Nearest-Neighbor model (NN). \n", " Enc-Dec and NN. \n", " Enc-Dec (Encoder-Decoder), ", " Baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are the name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec and NN. \n", " Encoder-Decoder baseline with ingredient attention and name-based Nearest-Neighbor model. \n", " Enc-Dec, ", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " An Encoder-Decoder baseline with ingredient attention and a name-based Nearest-Neighbor model. ", " Enc-Dec, \n", " Enc-Dec, NN. ", " A simple Encoder-Decoder baseline with ingredient attention and a name-based Nearest-Neighbor model. \n", " The baseline models are the Encoder-Decoder model and the Nearest-Neighbor model. \n", " The baseline models are the Encoder-Decoder and the Nearest-Neighbor model. \n", " The baseline models are a name-based Nearest-Neighbor model (NN) and an Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " Baseline models include a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baselines are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baselines are a Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec and NN. ", " The baseline models are the Nearest-Neighbor model (NN) and the simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n\n", " Encoder-Decoder baseline with ingredient attention (Enc-Dec) and a name-based Nearest-Neighbor model (NN). \n", " Enc-Dec (Encoder-Decoder), ", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models include a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec (Encoder-Decoder baseline with ingredient attention), ", " Enc-Dec and NN. \n", " The Encoder-Decoder baseline with ingredient attention (Enc-Dec) and a name-based Nearest-Neighbor model (NN). \n", " Enc-Dec, ", " Encoder-Decoder baseline with ingredient attention (Enc-Dec) and a name-based Nearest-Neighbor model (NN). \n", " The baseline models are a Nearest-Neighbor model (NN) and an Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec, NN, \n", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n\n", " Enc-Dec, \n", " Enc-Dec (Encoder-Decoder baseline with ingredient attention). ", " The baseline models are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec. \n", " The baseline models are the name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec. \n", " Enc-Dec and NN. \n", " Enc-Dec and NN. \n", " The baselines are a name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder model with ingredient attention (Enc-Dec). \n", " A name-based Nearest-Neighbor model (NN) and an Encoder-Decoder baseline with ingredient attention (Enc-Dec). \n", " Enc-Dec and NN, ", " Encoder-Decoder baseline with ingredient attention (Enc-Dec) and a name-based Nearest-Neighbor model (NN), "], "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": [" ARAML outperforms several state-of-the-art GAN baselines with lower training variance. \n", " Significant improvement. \n", " ARAML performs better than several state-of-the-art GAN baselines with lower training variance. \n", " 4-6 BLEU points. \n", " Compared to other GAN baselines, ARAML achieves better performance with lower training variance on three text generation tasks. \n\n", " Significantly better. \n", " Better performance than several state-of-the-art GAN baselines. \n", " ARAML gains a moderate improvement in the performance metrics of the three text generation tasks.\n\n\n", " Outperforms several state-of-the-art GAN baselines in all metrics, yet producing better performance on three text generation tasks. \n", " ARAML outperforms state-of-the-art GAN baselines with lower training variance. \n\n\n", " ARAML outperforms other baselines in terms of Self-BLEU, and performs significantly better in both grammaticality and relevance. \n", " Our model performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.\n\n", " Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training, \n", "  ARAML performs better than several state-of-the-art GAN baselines with lower training variance, \n", " Significant improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML).\n\n\n", " ARAML performs better than several state-of-the-art GAN baselines, producing better performance on three text generation tasks. \n", " \n", " The results show that ARAML significantly outperforms other baselines, with a notable improvement in both fluency and diversity. \n", " ARAML achieves better performance than several state-of-the-art GAN baselines. \n", " significant, ", " ARAML gains significant improvements from Adversarial Reward Augmented Maximum Likelihood, reaching the best reverse perplexity on COCO dataset and performs significantly better than other baselines in dialogue generation on WeiboDial.\n\n\n", " ARAML achieves lower training variance and outperforms state-of-the-art GAN baselines on three text generation tasks, with better performance in fluency, diversity, and relevance.\n\n\n", " 8%, ", " ARAML performs significantly better than other baselines in all the cases.\n", " ARAML gains significant improvement over state-of-the-art GAN baselines with lower training variance. \n", " The ARAML outperforms several state-of-the-art GAN baselines, especially in stability and diversity of generation results, and produces better performance on three text generation tasks, but no specific improvement rate is provided in the article.\n\n\n", " The best reverse perplexity is achieved with the temperature of 0.1. \n", " Better performance than several state-of-the-art GAN baselines. \n", " ARAML gains improvement by outperforming baselines on perplexity, Self-BLEU and manual evaluation.\n", " better performance\n\n\n", " ARAML performs better than several state-of-the-art GAN baselines. \n", " Our model performs better than several state-of-the-art GAN baselines with lower training variance.\n", " Significant improvement, ", " ARAML performs significantly better than other baselines in both grammaticality and relevance metrics in the dialogue generation task on the WeiboDial dataset, which demonstrates that it gains significant improvement over other models. \n", " ARAML outperforms several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.\n", " ARAML gains significant improvement in terms of grammaticality, relevance and fluency over other baselines in dialogue generation and text generation tasks. \n", " Unanswerable \n", " ARAML outperforms state-of-the-art GAN baselines with lower training variance and better performance. \n", " Lower training variance. \n", " Better performance on three text generation tasks.\n", " Lower training variance and better performance. \n", " ARAML outperforms policy gradient in the stability of adversarial training. \n", " Significant improvement\n", " ARAML outperforms state-of-the-art GAN baselines with lower training variance. \n", " 1-8.4% improvement over previous methods. \n", " ARAML outperforms policy gradient in the stability of adversarial training. \n", " Our model performs better than several state-of-the-art GAN baselines with lower training variance. \n\n\n", " better performance, ", " Significant improvement. \n", " ARAML outperforms other baselines in both metrics of forward and reverse perplexity on COCO. \n", " A significant improvement is gained from ARAML, surpassing many state-of-the-art GAN baselines and achieving a lower training variance with better performance on three text generation tasks. \n", " ARAML outperforms the baselines in both forward/reverse perplexity and Self-BLEU. \n\n", " The results show that ARAML outperforms other GAN baselines in both forward/reverse perplexity and Self-BLEU, and has the ability to generate grammatical and coherent results.\n\n\n", " A significant improvement. \n", " Our model performs better than several state-of-the-art GAN baselines. \n", " It outperforms policy gradient in the stability of adversarial training and achieves the best performance on three text generation tasks.\n\n", " better performance, ", " ARAML gains more stable and better performance. \n", " Our model performs better than several state-of-the-art GAN baselines with lower training variance, yet producing better performance on three text generation tasks.\n", " Our model outperforms policy gradient in the stability of adversarial training. The forward/reverse perplexity and Self-BLEU on COCO, and grammaticality and relevance on WeiboDial are improved significantly, ", " Best results. \n", " Our model outperforms state-of-the-art GAN baselines with lower training variance, producing better performance on three text generation tasks.\n", " Much, ", " ARAML gains more stable training and improvement in performance. \n", " better performance, ", " ARAML gains the best reverse perplexity and a smaller standard deviation in the training process. \n", " ARAML gains a notable improvement over GAN baselines in both fluency and diversity, ", " ARAML outperforms state-of-the-art GAN baselines in terms of both automatic metrics and human evaluation. \n\n", " best reverse perplexity, ", " Significant improvement. \n", " ARAML gains a significant improvement over other GAN baselines. \n", " It outperforms policy gradient in the stability of adversarial training and achieves the best performance in all the cases in dialogue generation on WeiboDial. \n", " Our model outperforms other baselines in terms of Self-BLEU. \n", " ARAML performs better than several state-of-the-art GAN baselines with lower training variance. \n", " 33.14%, ", " ARAML gains significant improvement over other baselines in terms of stability, fluency, and relevance. \n", " ARAML performs better than several state-of-the-art GAN baselines with lower training variance. \n", " ARAML gains significant improvement over several state-of-the-art GAN baselines. \n", " Significant improvement. \n", " Significant improvement. \n", " It gains a noticeable improvement, achieving best results in all metrics, with smaller standard deviation in training process, compared to other baselines. \n", " better, ", " \n", " Better performance, ", " Significant improvement, ", " ARAML gains improvement over the baselines. \n", " ARAML achieves better performance on three text generation tasks with lower training variance. \n", " better performance on three text generation tasks\n", " The improvement gained from ARAML is not explicitly stated. \n", " Better performance and lower training variance are gained.\n", " ARAML gains an improvement of 4.32 BLEU points over the baseline on WeiboDial, and achieves the best performance on the dataset. \n", "  improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)., ", " 2-6%, ", " ARAML gains significant improvement compared to other baselines in both metrics. ", " \n", " ARAML achieves the best results compared to the baselines. \n", " ARAML performs better than other baselines with lower training variance, yet producing better performance on three text generation tasks. \n", " ARAML gains 10% improvement in terms of Self-BLEU. \n", " The ARAML model achieves the best reverse perplexity among all the baselines on the EMNLP2017 WMT dataset and shows significant improvements in both grammaticality and relevance on the WeiboDial dataset. \n", " best results\n"], "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": [" competitive performance, ", " \n", " Reasonable performance can be obtained. \n", " The model achieves competitive performance compared with QANet trained on Chinese, ", " [Your concise final answer here], ", " Competitive performance compared with QANet trained on Chinese, ", " Reasonable performance. \n", " Competitive performance \n", " Reasonable performance. \n", " Reasonable performance can be obtained. \n", " Reasonable performance. \n\n", " It achieves competitive performance compared with QANet trained on Chinese. \n", " The model performance on target language reading comprehension is reasonable and competitive. \n", " Unanswerable. ", " Competitive performance. \n", " It is competitive to human-level performance on SQuAD and relatively lower EM compared with the model with comparable F1 scores. ", " It achieves competitive performance. ", " It achieves competitive performance compared with other models. \n", " competitive performance, ", " Reasonable performance. \n", " Reasonable performance. \n", " Reasonable performance can be obtained. \n", " competitive performance compared with QANet trained on Chinese, ", " Reasonable performance. ", " Reasonable, ", " The model can achieve competitive performance compared with a model trained on target language. \n", " The model performance on target language reading comprehension is competitive compared to the model trained on the target language. \n", " Competitive performance, ", " Reasonable performance can be obtained. \n", " The model performance on target language reading comprehension can be competitive to a model that is trained on the target language. \n", " Reasonable performance can be obtained. \n", " The model performance on target language reading comprehension can be competitive. \n", " Reasonable performance can be obtained. \n", " Reasonable performance can be obtained. \n", " Competitive performance. \n", " \n", " Reasonable performance. \n", " Reasonable performance can be obtained. \n\n", " competitive performance \n", " Reasonable performance can be obtained. \n\n", " Competitive performance compared with QANet. \n", " Reasonable performance. \n\n", " Reasonable performance can be obtained. ", " Competitive performance compared with QANet trained on Chinese, ", " The model performance is competitive. \n\n", " Reasonable performance is obtained. \n", " The model achieves reasonable performance on the target language reading comprehension task. \n", " Reasonable performance. \n", " Zero-shot cross-lingual transfer learning is feasible and achieves reasonable performance. \n\n", " \n", " Reasonable performance. \n", " [Your final answer here], ", " Reasonable performance. ", " The model achieves competitive performance on the target language reading comprehension compared with QANet. \n", " Competitive performance is obtained on target language reading comprehension. \n", " Competitive performance compared with QANet trained on Chinese. \n", " Reasonable performance can be obtained. \n", " competitive performance \n", " The model achieves competitive performance. ", " Zero-shot learning is feasible. \n", " Reasonable performance. \n", " Reasonable performance can be obtained, ", " The model shows reasonable performance on target language reading comprehension. \n", " Competitive performance compared with QANet, ", " Competitive performance was obtained on target language reading comprehension. \n", " Competitive performance \n", " It is reasonable. \n", " Reasonable performance can be obtained. \n", " Reasonable performance can be obtained. \n", " The model performance on target language reading comprehension varies depending on several factors, including the size and quality of the training data, the similarity of the source and target languages, and whether the training data is translated or not. \n", " \n", " The model achieves reasonable performance. \n", " Reasonable performance can be obtained on the target language reading comprehension. ", " competitive performance. \n", " Competitive performance. \n", " It achieves competitive performance compared with a model trained on target language data. \n", " Reasonable performance can be obtained. \n", " Reasonable performance can be obtained. \n", " Reasonable performance can be obtained. ", " Reasonable performance can be obtained. \n", " It achieves competitive performance compared with QANet. \n", " \n", " Reasonable performance can be obtained. \n", " Reasonable performance. \n", " competitive performance \n\n", " It has competitive performance on the target language. \n", " Competitive performance compared with QANet trained on target language. \n", " Reasonable performance can be obtained on target language reading comprehension. \n", " Unanswerable \n", " Reasonable performance. \n", " The model achieves reasonable performance on target language reading comprehension, especially between Chinese and Korean. \n", " The model achieves competitive performance. \n", " Competitive performance compared with QANet trained on Chinese. \n", " Reasonable performance. \n", " Reasonable performance can be obtained. ", " Competitive performance compared with QANet trained on Chinese. \n", " It can obtain reasonable performance, although with lower EM compared with the model with comparable F1 scores. \n", " Reasonable performance can be obtained. \n", " Reasonable performance. ", " Competitive performance. "], "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": [" sum, ", " No master node, \n", " Neighbors-only, ", " Renormalization, ", " Sum, ", " No, ", " The sum function. \n", " Neighbors-only. \n", " No component, ", " The identity function (Neighbors-only), ", " no master node. \n", " No master node, ", " The CNN, ", " Sum, ", " The sum in Renormalization, ", " The sum component, ", " No component, but the experiment without renormalization shows the lowest performance. \n", " No component, ", " Renormalization trick. \n", " The GRU combine function is the least impactful, ", " Undirected edges, ", " No master node, \n", " No specific component is mentioned as the least impactful in the article. However, the use of 1-layer perceptrons for graph classification is considered inferior to the use of multi-layer perceptrons, as stated in the article. One possible interpretation is that the neural network architecture used for message passing is more important than other components, but this is not explicitly stated in the article. Therefore, the question is unanswerable based on the information provided. \n", " Unweighted word co-occurrence networks. \n", " Renormalization, ", " Yes, the no renormalization experiment, ", " The GRU combine function is not the least impactful. \n", " No clear answer given in the text. \n", " Sum, ", " The master node. \n", " Renormalization trick, ", " Renormalization, ", " No master node, ", " The sum in the message passing aggregation. \n", " no component is unimpactful \n", " No, ", " neighbors-only, ", " No master node. \n", " No component is explicitly labeled as \"least impactful\", however, the use of GAT-like attention in early experiments did not result in better results. \n", " no master node. \n", " no master node, ", " No, ", " Undirected edges, ", " The structure of the graph, \n", " Yes/no question, so provide a yes/no answer.\n", " Identity function, ", " No master node. \n", " No component is significantly less impactful than others based on ablation studies. \n", " GRU, ", " yes/no, unanswerable, or can be identified. \n\n", " Renormalization, \n", " No master node, ", " No master node. \n", " The structural information. \n", " no, ", " no skip connection, ", " The component that is the least impactful is the \"1-layer perceptron\". \n", " GRU. \n", " Yes \n", " No. \n", " The sum component. \n", " No, \n", " no-renormalization, ", " neighbors-only, \n", " Renormalization trick, ", " Neighbors-only. \n", " The skip connection, ", " Renormalization, ", " No, \n", " no renormalization \n", " No of neighbors, ", " No master node, \n", " Yes, the master node is the least impactful. \n", " no master node, ", " Undirected edges. \n", " No, ", " No, ", " Undirected edges. \n", " No master node, \n", " No renormalization, \n", " Yes, no, or unanswerable.\n", " Undirected edges, ", " Undirected edges. \n", " The update gate. \n", " No component is explicitly stated as the least impactful in the article. \n", " No component, all components are important. \n", " Yes, the \"master node\" is the least impactful component in the MPAD model when removed. ", " unanswerable \n", " sum, ", " No component can be considered as the least impactful. \n", " unanswerable \n", " No master node. \n", " renormalization, ", " yes, ", " no-renormalization \n", " GRU, ", " unanswerable \n", " No. \n", " Identity function, ", " neighbors-only. \n"], "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": [" Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+ARL-SEN, and test set. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN, ", " Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Pos-FT. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and text style transfer baseline, ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN, ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos. ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos-RL, and Pointer-Gen+ARL-SEN, \n", " Pointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM13, Pointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0.5, Pointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.1, Pointer-Gen+Pos-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.5, Pointer-Gen+RL-", " Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, Pointer-Gen-ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Pos. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, Test set. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN, and test set. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+ARL-ROUGE. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN, \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+ARL-SEN, and test set. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and Test set headlines, ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Test Set. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, and test set headlines, ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, and Test set. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen is used for baseline evaluation. ", " Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same, Pointer-Gen+RL-ROUGE, Pointer-Gen, and Pointer-Gen+RL-SEN. \n", " Pointer-Gen+Pos-FT, Pointer-Gen+Same-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+Pos-FT, Pointer-Gen+ARL-SEN, Pointer-Gen+, Test set. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, and test set. ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN, and also test set headlines. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN, ", " Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN are not used for evaluation in the ROUGE metrics. The other baselines used for evaluation in the ROUGE metrics are Pointer-Gen, Pointer-Gen+RL-ROUGE and Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN are the baselines used. \n", " Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN, and Test set. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos. \n\n", " Pointer-Gen and text style transfer baseline, ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, and test set. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-ROUGE. (However, the last one is not a baseline, but the real model), ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN with minimum sensationalism score as 0.1, Pointer-Gen+ARL-SEN with minimum sensationalism score as 0.1, Pointer-Gen+Pos. ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN, ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and test set. \n\n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Text style transfer baseline, and the test set. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN are the baselines., ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, Test set headlines, Text style transfer baseline, Pointer-Gen+ARL-ROUGE, \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and test set, ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN, \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Text style transfer baseline. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and test set headlines. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT. \n", " Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, Pointer-Gen-ARL-SEN, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+RL-ROUGE, Pointer-Gen+ARL-SEN. ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and Test set, ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set. \n\n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$.", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, Test set. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen and the text style transfer baseline, ", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n\n", " Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, Test set, and Text style transfer baseline. \n", " Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN are not baselines but our models; Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN are our models with baselines; Pointer-Gen+RL-ROUGE is the baseline model trained by optimizing $L_{\\text{RL-ROUGE}}$ in Equation (17). Pointer-Gen is the baseline model trained by optimizing", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos-RL-SEN, Pointer-Gen+Pos-ARL-SEN, Pointer-Gen+Same-RL-SEN, Pointer-Gen+Same-ARL-SEN, Text style transfer baseline, and Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN are also", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+ARL-SEN, Pointer-Gen+Pos, and Text style transfer baseline, ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-SEN and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT. \n", " Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, and Pointer-Gen+ARL-SEN \n\n", " Pointer-Gen is used as a baseline model trained by optimizing LMLE. ", " Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, and Test set. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN and Test set. \n\n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set, and Pointer-Gen+ARL-ROUGE is not used for comparison. \n", " Pointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$, Pointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0.5, Pointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.1, Pointer-Gen+Pos-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.5, Pointer-Gen+RL-ROUGE is the baseline", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN. \n", " Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, Test set. \n"], "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": [" The ability of the model to detect some biases in the process of collecting or annotating datasets, \n", " The authors present evidence that many errors are due to biases from data collection and rules of annotation, not the classifier itself. \n", " the ability of our model to detect some biases in the process of collecting or annotating datasets. \n", " The authors present the errors of the model to show that it can capture some biases in data annotation and collection. \n", " The model's high misclassifications of hate samples as offensive, despite the bias in the data. \n", " A deep analysis on the error of the model, ", " Some errors are due to biases from data collection and rules of annotation. \n\n", " The authors present evidence from their manual inspection of mislabeled items and a deeper analysis on the error of the model. \n\n", " Examination of the results shows the ability of the model to detect some biases in the process of collecting or annotating datasets. \n", " The errors are due to biases from data collection and rules of annotation, and not the classifier itself. \n", " The model's ability to detect some biases in the process of collecting or annotating datasets can be seen in its results, which show that it can correctly classify tweets that contain neither hate nor offensive content despite being annotated as such, and can also detect implicit abuse in some cases. \n\n", " The model can capture some biases in data annotation and collection by manually inspecting mislabeled items and analyzing the biases from data collection and annotation rules. \n", " The ability of the model to detect some biases in the process of collecting or annotating datasets. \n", " The authors present the results from the confusion matrices and manual inspections of the test datasets. \n", " Tables TABREF20 and TABREF21, ", " The authors present that examining the results shows the ability of their model to detect some biases in the process of collecting or annotating datasets, particularly misclassifications of hate samples as offensive samples, which is related to the bias in data. \n", " Tables TABREF20 and TABREF21. \n", " The ability of the model to detect some biases in the process of collecting or annotating datasets is shown in the evaluation results. \n", " The authors present some misclassified samples by their model, which, upon manual inspection, show that the bias in the data is a major contributor to these misclassifications. \n", " The model can capture some biases in data annotation and collection by misclassifying hateful samples as offensive. \n", " They manually inspect the test dataset and record some of the mislabeled items to show that many errors are due to biases from data collection and rules of annotation. \n\n", " The ability of our model to detect some biases in the process of collecting or annotating datasets. \n", " They present evaluation results and manual inspection of errors. \n", " The authors present some error analysis. \n", " They present error analysis results where the model misclassifies hate samples as offensive, ", " \n", " They show that the model can detect some biases in the process of collecting or annotating datasets. \n", " The authors present error analysis as evidence. \n", " The authors' model detects some biases in data collection and annotation. \n", " The authors present evidence that many errors in their model are due to biases from data collection and rules of annotation, and not the classifier itself, through a manual inspection of a subset of the data and an analysis of the error of the model. \n", " Examination of results from mislabeled items shows that many errors are due to biases from data collection and rules of annotation, not the classifier itself. \n", " They present results that show the ability of their model to detect some biases in the process of collecting or annotating datasets. \n", " The authors present the model's ability to detect some biases in the process of collecting or annotating datasets as a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets. \n\n", " The model can accurately classify tweets that are neither hate nor offensive by leveraging knowledge-aware language understanding that it has, which is learned from general corpora. \n", " It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data. \n", " It can detect biases by examining the results and it is a valuable clue for future studies. \n", " They present that the model misclassifies some hate samples as offensive in tweets containing disrespectful words that were annotated by annotators without consideration of social context, which can be related to bias in data collection and annotation rules. \n", " The authors present evidence from their analysis of misclassified samples and comparison with recent studies. \n", " The model can detect biases in data collection and annotation. \n", " The model's high misclassifications of hate samples as offensive, and its ability to detect implicit hatred content. \n", " Manual inspection of mislabeled items by the model, ", " Examination of results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. \n", " The model can detect biases in the process of collecting or annotating datasets. \n", " They present the manual inspection on a subset of the data and show that many errors are due to biases from data collection and rules of annotation. \n", " The authors present the evaluation results and manual inspection of mislabeled items to show that the model can capture some biases in data annotation and collection. \n", " The authors present some misclassified items as evidence that their model can capture some biases in data annotation and collection. \n", " They present manual inspection results and confusion matrices that show high misclassifications of hate samples as offensive, which are likely due to biases in the data. \n\n", " Manual inspection on a subset of the data, and record some of the mislabeled items. \n", " They present manual inspection and analysis on a subset of mislabeled items to confirm the biases in data collection and annotation rules. \n", " Manual inspection of mislabeled items. \n", " A deep analysis of the model's errors, which show that many errors are due to biases from data collection and rules of annotation. \n", " The model's ability to misclassify hate samples as offensive, and the manual inspection of mislabeled items. \n", " The ability of the model to detect some biases in the process of collecting or annotating datasets. \n", " They present confusion matrices, error analysis, and manual inspection of mislabeled items by the model, as well as the comparison with recently studies that have shown biases in data collection and annotation. \n", " The model can capture biases in data annotation and collection by showing higher misclassifications in certain categories and also detecting some biases in the process of collecting or annotating datasets. \n", " The authors present the misclassifications of the model as evidence that it can capture some biases in data annotation and collection. \n", " They show that the model is able to detect some biases in the process of collecting or annotating datasets, by leveraging the contextual information embedded in the BERT’s layers. ", " The model's ability to detect biases in the process of collecting or annotating datasets. \n", " The evaluation results indicate that the model can detect some biases in the process of collecting or annotating datasets, particularly in cases where the data contains specific language or geographic restrictions, such as tweets written within the African American Vernacular English or from the United States of America. \n", " The model can detect biases in the process of collecting or annotating datasets. \n", " The evaluation results of the model. \n", " The authors present the ability of their model to detect some biases in the process of collecting or annotating datasets. \n", " The authors present manual inspection results showing that many errors are due to biases from data collection and rules of annotation. \n\n", " Some errors of the model were due to biases from data collection and rules of annotation. \n", " The evaluation results show the model can detect some biases in the process of collecting or annotating datasets. \n", " The model can capture some biases in data annotation and collection by misclassifying hate samples as offensive, which can be due to biases from data collection and rules of annotation. \n", " The model's high misclassifications of hate samples as offensive, but accurate detection of implicit hatred content, suggest that the model can detect some biases in the data. \n", " The model's misclassifications and manual inspection show that biases in data collection and rules of annotation are the main reasons for misclassifications. \n\n", " The authors present that the majority of errors come from misclassifying hateful categories as hatless (neither) and vice versa, with some errors being due to biases from data collection and rules of annotation. \n", " They show that the model can misclassify hate samples as offensive, but can accurately detect implicit hatred content. \n", " The model can capture some biases in data annotation and collection, and it can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies. \n", " They show that the model can detect biases in data annotation and collection by misclassifying some samples, particularly those containing explicit abusive words, as hate or offensive. \n", " It can be a valuable clue in using the pre-trained BERT model to alleviate bias in hate speech datasets in future studies, by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data. \n", " Examination of the test datasets and their confusion matrices resulted from the BERTbase + CNN model shows that the model can capture some biases in data annotation and collection. \n", " The ability of our model to detect some biases in the process of collecting or annotating datasets. \n", " They show that the model can differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding that it has and it can misclassify hate samples as offensive despite the bias in the data. \n", " The authors present evidence through error analysis and inspection of mislabeled items by their model, which show that many errors are due to biases from data collection and rules of annotation, rather than the classifier itself. \n", " The model can detect biases in the process of collecting or annotating datasets. \n", " The authors present manual inspection on mislabeled items and some tweets containing implicit abuses. \n", " The authors present manual inspection of mislabeled items, including samples with implicit abuse and samples containing specific language or geographic restriction. \n", " The model can detect some biases in the process of collecting or annotating datasets, as shown by the results. \n\n", " The authors present the errors in classification as evidence that the model can capture some biases in data annotation and collection. \n", " The model is able to misclassify hate samples as offensive, and manual inspection of the data shows that this is due to biases in data collection and annotation rules. \n", " They present some examples of misclassified samples and analysis of these errors which suggests that biases in data collection and annotation are the main reason for these misclassifications, rather than the classifier itself. \n", " Tables TABREF20 and TABREF21 show the mislabeled items by the model, which contain implicit abuses or biases. \n\n", " The authors present the model's ability to detect biases in the process of collecting or annotating datasets. \n", " They mention that the model can detect some biases in the process of collecting or annotating datasets. \n", " Some errors are due to biases from data collection and rules of annotation and not the classifier itself. \n", " The authors present the results that show high misclassification rates of hate samples as offensive (63%), which suggests that the model can differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding. \n", " The model shows the ability to detect some biases in the process of collecting or annotating datasets. \n\n", " The authors present evidence of the model's ability to detect biases in data annotation and collection by its high misclassifications of hate samples as offensive, despite the presence of bias in the training data. \n", " The errors in the model's predictions, as illustrated in the confusion matrices, are largely due to biases in data collection and rules of annotation, rather than the classifier itself. \n", " Tables TABREF20 and TABREF21, \n", " The model can accurately distinguish between hate and offensive samples by leveraging knowledge-aware language understanding. \n", " The model is able to detect some biases in the process of collecting or annotating datasets, ", " The authors present evidence from error analysis of the model, indicating that many errors are due to biases from data collection and annotation rules, but the model can still differentiate hate and offensive samples accurately by leveraging knowledge-aware language understanding. \n\n", " Examining the results shows the ability of our model to detect some biases in the process of collecting or annotating datasets. \n", " Tables TABREF20 and TABREF21. \n", " The authors present a manual inspection of misclassified items, showing that many errors are due to biases from data collection and rules of annotation, rather than the classifier itself. \n", " The authors present evidence that the model can capture some biases in data annotation and collection by examining the results that show the ability of their model to detect some biases in the process of collecting or annotating datasets. \n"], "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": [" Yes, \n", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, Answerability baselines include SVM, CNN, BERT, and also several baselines for answer sentence selection, including a No-Answer Baseline, Word Count Baseline, and BERT-based baselines.  The human performance baseline is also included to compare with the results of the neural baselines. \n\n", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, \n", " yes, \n", " yes, ", " Yes, ", " yes, ", " Yes, \n", " yes, \n", " yes, ", " Yes, ", " Yes, ", " Yes, \n", " Yes, ", " Yes, \n", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, \n", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, \n", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, \n", " Yes, \n", " yes, ", " yes, ", " yes, ", " yes, \n", " yes, ", " Yes, \n", " yes, ", " Yes, ", " Yes, ", " Yes, \n", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, Answerability identification baselines (SVM, CNN, BERT) and Privacy question answering baselines (No-Answer, Word Count, BERT) were tested. \n", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, several baselines were tested to compare with the neural baseline. \n", " yes, ", " yes, ", " yes, \n", " Yes, ", " yes, ", " Yes, \n", " Yes, \n", " yes, \n", " yes, \n", " Yes, \n", " yes, \n", " Yes, \n", " Yes, ", " Yes, ", " Yes, \n", " Yes, Answerability baselines were presented in Table TABREF31, including SVM, CNN, and BERT, while baselines for answer sentence selection task were described in Table TABREF32, including No-Answer, Word Count, and BERT. ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, \n", " Yes, ", " yes, \n", " Yes, ", " Yes, several baselines were tested and compared to the neural baseline, including the No-Answer Baseline (NA), Word Count Baseline, and Human Performance baseline. \n", " Yes, ", " Yes, ", " yes, ", " yes, \n", " yes, ", " Yes, "], "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": [" The data from BIBREF0 and eye-tracking, self-paced reading time and ERP data. \n", " EEG data from BIBREF0, MEG activity from BIBREF8, and eye-tracking data., ", " Unanswerable. ", " The datasets used are the data from BIBREF0, eye-tracking data, self-paced reading time, and neural activity. ", " BIBREF0, BIBREF7, BIBREF8, BIBREF9, TABREF9, TABREF16, TABREF23, TABREF24, TABREF25, BIBREF0, BIBREF4, BIBREF5, BIBREF6, BIBREF1, BIBREF2, BIBREF3,  TABREF9, TABREF16, TABREF23, TABREF24, TABREF25, \n", " eye-tracking data, self-paced reading time, and ERP data. \n", " BIBREF0 and Harry Potter and the Sorcerer’s Stone, ", " eye-tracking, self-paced reading, ERP components. ", " The data from BIBREF0. ", " The datasets used include EEG and behavioral data from BIBREF0. ", " EEG data from BIBREF0 and eye-tracking, self-paced reading time data. \n", " Human language processing data, ", " Neural activity data from BIBREF0 and eye-tracking data., ", " BIBREF0, BIBREF9. \n", " EEG data from BIBREF0, eye-tracking data, self-paced reading time data, and language modeling data, ", " Neural networks pretrained using a language modeling objective, data from BIBREF0 and data from an experiment of reading a chapter of Harry Potter and the Sorcerer’s Stone, and datasets associated with human language processing. ", " EEG and eye-tracking datasets. ", " BIBREF0, BIBREF7, BIBREF8, BIBREF9. ", " EEG data from BIBREF0, eye-tracking data, and self-paced reading time. ", " The datasets used are EEG, eye-tracking, self-paced reading, and the ERP data from BIBREF0. ", " The datasets used include the data from BIBREF0 and the dataset used in BIBREF8 which records magnetoencephalography (MEG) activity.  The text also mentions the use of a chapter of Harry Potter and the Sorcerer’s Stone from BIBREF9. However, the primary dataset used in the main work is the data from BIBREF0. In addition to the primary dataset, the authors also analyze the relationships between ERP components and eye-tracking data, and between ERP components and self-paced reading time. \n\n", " BIBREF0 and Harry Potter and the Sorcerer's Stone, ", " The EEG data from BIBREF0. \n", " EEG data from BIBREF0 and eye-tracking data. ", " The datasets used in the work include EEG data from BIBREF0, magnetoencephalography (MEG) activity from BIBREF9, eye-tracking data, and self-paced reading time. ", " EEG and behavioral data. ", " The EEG data from BIBREF0. ", " The datasets used include those from BIBREF0, the chapter of Harry Potter and the Sorcerer’s Stone from BIBREF9, eye-tracking data, self-paced reading data, and behavioral data. ", " Human language processing prediction datasets associated with eye-tracking, self-paced reading, and ERP components, and in particular between behavioral and neural data. ", " BIBREF0, BIBREF7, BIBREF8, BIBREF9. \n", " The datasets used are from the work of BIBREF0. \n", " Datasets used include EEG data from BIBREF0, eye-tracking data, self-paced reading data, and language modeling data. ", " BIBREF0, eye-tracking data, self-paced reading data, and MEG activity data, ", " BIBREF0, BIBREF9, TABREF9, TABREF16, TABREF23, TABREF24, TABREF25, ", " EEG data and eye-tracking data, self-paced reading time data, and sentence position in the experiment. \n", " EEG and eye-tracking data, as well as self-paced reading data. ", " BIBREF0, BIBREF9, and eye-tracking data. \n", " ERP data from BIBREF0 and behavioral data. ", " BIBREF0, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9. \n", " Eye-tracking data, self-paced reading time data, and ERP (electroencephalography) data, and also data from BIBREF0. \n", " BIBREF0, eye-tracking data, self-paced reading data, and MEG activity from Harry Potter and the Sorcerer’s Stone. \n", " The datasets from BIBREF0 and also a chapter of Harry Potter and the Sorcerer’s Stone. ", " EEG and eye-tracking data, ", " EEG data from BIBREF0 and eye-tracking data, self-paced reading time data. \n", " EEG data, eye-tracking data, self-paced reading time, Table TABREF23 and TABREF24., ", ", ", " Various EEG datasets including the one from BIBREF0 and MEG activity recorded while participants read a chapter of Harry Potter and the Sorcerer’s Stone from BIBREF9. \n", " The data from BIBREF0 and other studies such as BIBREF7 and BIBREF8, as well as eye-tracking data. ", " The datasets used include ERP data from BIBREF0, and datasets from BIBREF7, BIBREF8 and BIBREF9. ", " ERP data from BIBREF0 and eye-tracking data. ", " BIBREF0, TABREF23, TABREF24, TABREF25, \n", " ERPs data from BIBREF0, eye-tracking data, and self-paced reading data. ", ", ", " The datasets used are BIBREF0, BIBREF4, BIBREF5, BIBREF6, and BIBREF9. ", " The datasets used include the data from BIBREF0, eye-tracking data, self-paced reading data, and EEG data from a chapter of Harry Potter and the Sorcerer’s Stone. \n", " ERP data from BIBREF0 and eye-tracking data, ", " The datasets used are EEG, eye-tracking data, self-paced reading time, and ERP components from the paper BIBREF0. \n", " The data from BIBREF0 and a chapter of Harry Potter and the Sorcerer’s Stone. \n", " The authors use the ERP data from BIBREF0, and they also use eye-tracking data, self-paced reading time and other heterogeneous types of data in their multitask learning analysis. \n\n", " BIBREF0 and BIBREF9, ", " Unanswerable, ", " The ERP data from BIBREF0, eye-tracking data and self-paced reading time. ", " The datasets from BIBREF0, BIBREF4, and BIBREF7, and the chapter of Harry Potter and the Sorcerer’s Stone. ", " ERP components from BIBREF0 and a chapter of Harry Potter and the Sorcerer’s Stone. ", " The datasets used are not explicitly listed, but they include a chapter of Harry Potter and the Sorcerer’s Stone. The article also mentions data from BIBREF0, a study in which the authors relate the surprisal of a word, i.e. the (negative log) probability of the word appearing in its context, to each of the ERP signals considered here. Additionally, eye-tracking data and self-paced reading time are used. However, it is worth noting that the specific datasets used for the ERP signals in the study are from the work by the authors mentioned in BIBREF0 and BIBREF7.,", " A dataset from BIBREF0, Harry Potter chapter, and multiple other datasets, ", " The authors used the data from BIBREF0 and also the chapter of Harry Potter and the Sorcerer’s Stone. ", " The article uses the dataset from BIBREF0. \n", " The dataset from BIBREF0 and a chapter of Harry Potter and the Sorcerer’s Stone BIBREF9. \n", " EEG data from BIBREF0 and eye-tracking data, self-paced reading time. \n", " The datasets used include EEG data from BIBREF0 and magnetoencephalography (MEG) data from Harry Potter and the Sorcerer's Stone.  ", " BIBREF0, eye-tracking, self-paced reading time and ERP data., ", " EEG and behavioral data. \n", " The datasets include EEG data, eye-tracking data, self-paced reading data, and language models, specifically the data from BIBREF0. \n", " EEG and behavioral data from the participants, ", " ERPs data from BIBREF0 and eye-tracking, self-paced reading time data, and behavioral data. \n", " EEG and MEG datasets from BIBREF0, eye-tracking data, self-paced reading data. ", " EEG data from BIBREF0, eye-tracking data, and self-paced reading time. \n", " BIBREF0, BIBREF7, BIBREF8, BIBREF9, TABREF9, TABREF16, TABREF23, TABREF24, TABREF25. \n", " EEG data from the work by the authors in BIBREF0 and eye-tracking data., ", " EEG data from BIBREF0. ", " unanswerable, ", " EEG data from BIBREF0,  and eye-tracking data. ", " ERP components and behavioral data, including eye-tracking and self-paced reading time data. \n", " EEG and eye-tracking data, ", " EEG data and eye-tracking data, ", " Neural networks pretrained as language models, eye-tracking data, self-paced reading time, and EEG data. ", " The datasets used include ERP data from BIBREF0, and eye-tracking data, self-paced reading time. \n\n", " The datasets used include ERP data from BIBREF0 and eye-tracking data. ", " BIBREF0, BIBREF7, BIBREF8, and a Harry Potter and the Sorcerer’s Stone chapter. ", " EEG and eye-tracking data, as well as data from self-paced reading, are used. ", " The datasets used include the data from BIBREF0 and data from a chapter of Harry Potter and the Sorcerer’s Stone. ", " the data from BIBREF0 and other datasets mentioned, such as eye-tracking and self-paced reading data. ", " Human language processing prediction datasets. ", " The EEG data from BIBREF0 and chapter of Harry Potter and the Sorcerer’s Stone, the eye-tracking data. ", " EEG and behavioral data from participants in experiments, and language modeling data. ", " BIBREF0, BIBREF9, and Harry Potter and the Sorcerer’s Stone. ", " The datasets used include EEG data from BIBREF0 and data from self-paced reading time. ", " The authors use ERP data from BIBREF0, eye-tracking data, self-paced reading time data, and the data from Harry Potter and the Sorcerer’s Stone.  Also, they used language model pretrained on a large corpus of text to make predictions of ERP components. \n", " ERP data from a study with which this work is most closely related, a study that used eye-tracking data, self-paced reading time, and ERP data. \n"], "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": [" A diachronic corpus pair from DTA18 and DTA19. \n", " DTA corpus. ", " DTA18 and DTA19. ", " The DTA corpus pair. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19, which consist of subparts of the DTA corpus. \n\n", " DTA18 and DTA19, ", " The diachronic corpus pair DTA18 and DTA19. \n", " DTA18 and DTA19. ", " DTA corpus, ", " Diachronic corpus pair from DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " \n", " DTA18 and DTA19 corpora, \n", " The DTA corpus, ", " DTA18 and DTA19. \n", " DTA18 and DTA19, ", " DTA18 and DTA19, ", " The DTA18 and DTA19 corpora. \n", " The corpus used for the task is the DTA corpus. \n\n", " DTA corpus, ", " DTA18 and DTA19. \n", " DTA18 and DTA19, ", " DTA18 and DTA19. \n", " The diachronic corpus pair. ", " DTA18 and DTA19. \n", " The DTA18 and DTA19 corpus pair. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19, ", " DTA corpus, ", " DTA18 and DTA19. \n", " DTA corpus pair of DTA18 and DTA19. ", " The DTA corpus, ", " Diachronic corpus pair from DTA18 and DTA19. \n", " A diachronic corpus pair of German texts, ", " The diachronic corpus pair from DTA18 and DTA19. \n", " DTA18 and DTA19, ", " DTA corpus. \n", " DTA corpus, consisting of subparts of DTA18 and DTA19. ", " Diachronic corpus pair from DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19, which correspond to the diachronic corpus pair from BIBREF0. \n", " DTA18 and DTA19. ", " DTA18 and DTA19 corpora. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " Diachronic corpus pair of DTA18 and DTA19. \n", " Diachronic corpus pair from BIBREF0: DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " The DTA18 and DTA19 corpora. \n", " DTA corpus. ", " The DTA18 and DTA19 corpus pair. \n", " The corpus used for the task corresponds to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. \n\n", " DTA18 and DTA19. \n", " The DTA corpus. \n", " DTA corpus, ", " DTA18 and DTA19. \n", " A diachronic corpus pair from BIBREF0: DTA18 and DTA19. \n", " DTA corpus, ", " DTA18 and DTA19. ", " DTA18 and DTA19. \n", " DTA18 and DTA19. ", " DTA18 and DTA19. ", " DTA corpus pair. \n", " Diachronic corpus pair DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA corpus, ", " The DTA corpus, ", " DTA18 and DTA19, subparts of DTA corpus. ", " DTA18 and DTA19, ", " DTA corpus. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. \n", " DTA corpus. \n", " DTA18 and DTA19. ", " The diachronic corpus pair from BIBREF0: DTA18 and DTA19, \n", " DTA corpus, ", " The DTA corpus, specifically DTA18 and DTA19. ", " DTA18 and DTA19. \n", " The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. \n\n", " The corpus used for the task is DTA18 and DTA19, which consist of subparts of the DTA corpus. \n", " DTA18 and DTA19. \n", " DTA18 and DTA19. ", " DTA corpus, ", " DTA18 and DTA19, \n", " DTA18 and DTA19, \n", " DTA18 and DTA19. \n", " The DTA corpus is used for the task. \n", " DTA18 and DTA19. \n", " DTA corpus, ", " DTA18 and DTA19. ", " DTA18 and DTA19, ", " DTA18 and DTA19. \n", " DTA18 and DTA19, "], "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": [" Various machine learning classifiers and neural network models, ", " Traditional machine learning classifiers, neural network models. ", " Neural network models, ", " Neural network models and traditional machine learning classifiers., ", " Neural network based models, traditional machine learning classifiers and ensemble models,", " Traditional machine learning classifiers, neural network models, and their variants. \n", " Neural network models and traditional machine learning models. ", " Traditional machine learning classifiers and neural network models. ", " Various machine learning classifiers and neural network models. \n", " Bidirectional GRU networks with LTC and Convolutional Neural Networks (CNN), ", " Neural network models and traditional machine learning classifiers, ", " Neural network models, including CNN, RNN and their variants, and traditional machine learning models. \n\n", " Neural network models and traditional machine learning classifiers. \n\n", " Traditional machine learning classifiers and neural network models. \n", " Machine learning classifiers and neural network based models, ", " Neural network models and traditional machine learning classifiers, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models. ", " Traditional machine learning classifiers and neural network based models, ", " Neural network based models and traditional machine learning classifiers, ", " Machine learning classifiers and neural network models are used, ", " Traditional machine learning classifiers and neural network models. \n", " Various traditional machine learning classifiers and neural network based models. \n", " Feature engineering based machine learning classifiers and neural network based models, ", " Traditional machine learning classifiers, neural network based models. \n", " Neural network models and traditional machine learning models. \n", " Traditional machine learning models and neural network models. \n", " Traditional machine learning models and neural network based models. \n", " Neural network models, traditional machine learning classifiers, and ensemble models. \n", " Traditional machine learning classifiers and neural network based models. \n", " Neural network models and traditional machine learning classifiers. \n", " Feature engineering based machine learning classifiers and neural network models are used on the dataset. \n", " Bidirectional GRU networks with LTC, ", " Neural network models including CNN and RNN are used on the dataset, ", " Neural network models and traditional machine learning classifiers are used on the dataset. \n\n", " Neural network models, traditional machine learning models, and ensemble models, ", " Neural network models, \n", " Traditional machine learning models and neural network models. ", " Machine learning classifiers and neural network models. \n", " Neural network based models and traditional machine learning models. \n", " Feature engineering based machine learning classifiers and neural network based models are used on the dataset. \n", " Traditional machine learning classifiers and neural network models, ", " Neural network based models including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and HybridCNN, and traditional machine learning classifiers such as Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. Additionally, ensemble models and variants of neural network models such as RNN with LTC modules are also used. \n\n", " Machine learning classifiers and neural network models. \n", " Neural network models, ", " ", " Neural network models and traditional machine learning models are used on the dataset. \n", " Neural network models and traditional machine learning models, ", " Machine learning classifiers, neural network based models, ensemble models, CNN, RNN, and HybridCNN, ", " Neural network models and traditional machine learning classifiers. \n\n", " Neural network models, ", " Both traditional machine learning classifiers and neural network based models are used. \n", " Neural network models and traditional machine learning models are used on the dataset. \n", " Bidirectional GRU networks with Latent Topic Clustering (LTC), ", " Neural network models, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models, along with traditional machine learning classifiers such as Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), and Gradient Boosted Trees (GBT). \n\n", " Traditional machine learning classifiers and neural network based models. \n", " Neural network based models, traditional machine learning models, and ensemble models. \n", " Traditional machine learning classifiers and neural network models, ", " Machine learning classifiers and neural network models, ", " The authors use five traditional machine learning classifiers and several neural network based models on the dataset, ", " Bidirectional GRU networks, ", " Feature engineering based machine learning classifiers and neural network based models. ", " Traditional machine learning classifiers and neural network based models, ", " Neural network models are used, ", " Traditional machine learning classifiers and neural network models. \n", " Traditional machine learning classifiers and neural network based models, ", " Neural network models, including CNN, RNN, and their variants, are used on the dataset, along with traditional machine learning classifiers such as Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. ", " Neural network models, including CNN, RNN, and HybridCNN, and traditional machine learning classifiers such as Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. \n", " Traditional machine learning classifiers and neural network models. \n", " Traditional machine learning models and neural network based models. \n", " Neural network based models, including CNN, RNN, and HybridCNN, and traditional machine learning classifiers, including NB, LR, SVM, RF, and GBT. ", " Neural network models and traditional machine learning models. \n", " Traditional machine learning classifiers and neural network models, ", " Neural network based models, including CNN, RNN, and their variants. ", " The traditional machine learning models used are Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, and Gradient Boosted Trees. Additionally, neural network models used are Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variants such as HybridCNN and models with Latent Topic Clustering (LTC) modules. Context tweets are also used as additional features with these models. \n\n", " Neural network models, ", " Traditional machine learning classifiers and neural network based models. \n", " Traditional machine learning classifiers and neural network models, ", " Machine learning classifiers and neural network models. \n\n", " Traditional machine learning classifiers and neural network based models are used on the dataset, including Naïve Bayes, Logistic Regression, Support Vector Machine, Random Forests, Gradient Boosted Trees, Convolutional Neural Networks, Recurrent Neural Networks, HybridCNN, and bidirectional GRU networks. \n\n", " Neural network models, traditional machine learning classifiers, and ensemble models. \n", " Feature engineering based machine learning classifiers and neural network based models. \n", " Various learning models are used. \n", " Bidirectional GRU networks with Latent Topic Clustering (LTC), ", " Traditional machine learning classifiers and neural network models are used on the dataset. \n", " Feature engineering based machine learning classifiers and neural network models. \n", " Neural network models including CNN, RNN, and their variants. ", " Traditional machine learning models and neural network based models, ", " Various machine learning and neural network models, ", " Traditional machine learning classifiers, neural network models. \n", " Neural network models and traditional machine learning models. \n\n", " Neural network models are used. \n", " Neural network models, ", " Traditional machine learning classifiers and neural network based models, ", " Traditional machine learning models and neural network based models. \n\n", " Both traditional machine learning classifiers and neural network based models, ", " Neural network models, including traditional machine learning classifiers, are used on the dataset. \n\n", " Machine learning classifiers and neural network models, ", " Neural network models and traditional machine learning models are used. ", " Traditional machine learning classifiers and neural network based models, ", " Neural network models, including CNN, RNN, and their variants, and traditional machine learning models, including Naïve Bayes, Logistic Regression, SVM, Random Forests, and Gradient Boosted Trees. ", " Machine learning classifiers and neural network based models, "], "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": [" Stimulus-based speech. \n", " multimodal data for stimulus-based, imagined and articulated speech, \n", " multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Stimulus-based speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " None, ", " speech imagery data, \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech states. \n", " No specific data was presented to elicit event-related responses. \n", " Stimulus-based speech state, ", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state, ", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic and 4 words. \n\n", " None was presented. The data elicited event-related responses due to speech imagery. \n", " multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic. \n", " None, ", " \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Ocular artifact removal using blind source separation, bandpass filtering and subtracting mean value from each channel. \n", " Speech stimuli \n", " Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). \n\n", " No specific data was presented to elicit event-related responses. \n", " None, unanswerable. \n", " No specific data was presented to the subjects to elicit event-related responses. ", " Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) and 4 words(pat, pot, knew and gnaw). \n\n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data. \n", " multimodal data for stimulus-based, imagined and articulated speech states \n", " No data was mentioned in the article, \n", " None of the above, ", " None, ", " Multimodal data for stimulus-based, imagined and articulated speech. ", " None, \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined, and articulated speech state. \n", " \n", " Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic and 4 words. \n", " Ocular artifact removal using blind source separation, bandpass filtering and subtracting mean value from each channel \n", " Ocular artifact removal using blind source separation, bandpass filtering, and subtracting the mean value from each channel were performed on raw EEG data. \n\n", " multimodal data for stimulus-based, imagined and articulated speech state \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic categories, \n", " No data, ", " The speech stimulus corresponding to 7 phonemic/syllabic and 4 words. \n", " multimodal data for stimulus-based, imagined and articulated speech state. \n", " None, ", " Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic, and 4 words. \n", " Ocular artifacts were removed using blind source separation, and raw EEG data was band-pass filtered, and the mean value was subtracted from each channel. \n", " Multimodal data. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " None\n", " Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " The data presented to the subjects were stimulus-based and imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Stimulus-based speech state. \n", " multimodal data for stimulus-based, imagined and articulated speech. \n", " Speech imagery, \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " The dataset KARA ONE, ", " \n", "  Seven phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw), \n", " Stimulus-based speech state, ", " None, ", " multimodal data for stimulus-based, imagined and articulated speech state \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Stimulus-based, imagined and articulated speech state \n", " Stimulus-based speech data, \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Stimulus-based, imagined, and articulated speech. \n", " unanswerable \n", " multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic categories and 4 words. \n", " Stimulus-based speech state data, \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Ocular artifact removal using blind source separation, bandpass filtering, and subtracting the mean value from each channel. \n", " Imagined speech, \n", " Stimulus-based speech. \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Unanswerable, ", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Stimulus-based speech state corresponding to 7 phonemic/syllabic states, and 4 words. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech state. \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n", " Imagined speech state. \n", " The subjects were presented with speech and non-speech stimuli. \n", " multimodal data for stimulus-based, imagined and articulated speech states corresponding to 7 phonemic/syllabic categories and 4 words. \n", " multimodal data for stimulus-based, imagined and articulated speech state, ", " Stimulus-based, imagined and articulated speech state. \n", " None were specifically stated. \n", " Ocular artifact removal using blind source separation, bandpass filtering, and subtracting mean value from each channel of raw EEG data. \n", " Multimodal data for stimulus-based, imagined and articulated speech. \n"], "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": [" The size of the dataset is almost ten times bigger, \n", " 64%, 16% and 20% of the total dataset is divided into training set, development set and test set respectively. However, the exact total size of the dataset is not provided in the article, ", " 6946 sentences and 16225 unique words, ", " unanswerable, ", " 72782, ", " The dataset size is not provided in a single figure but it has three classes Person (PER), Location (LOC), and Organization (ORG), with a total of 64%, 16%, and 20% of the total dataset into training set, development set, and test set respectively. ", " 6946 sentences, ", " unanswerable, ", " 72782, ", " unanswerable, ", " 6946 sentences and 16225 unique words. \n", " The size of OurNepali dataset is almost ten times bigger compared to ILPRL dataset in terms of entities, with OurNepali dataset volume being bigger by ten times. ", " 6946 sentences, ", " unanswerable, ", " 72782, ", " The dataset has 6946 sentences and 16225 unique words. \n", " unanswerable, ", " Unanswerable, ", " 6946 sentences and 16225 unique words, ", " Unanswerable, ", " 72782 unique words, ", " unanswerable, ", " 6946 sentences, ", ", ", " The dataset is not specified in terms of the number of elements. However, it is mentioned to be ten times smaller in terms of entities compared to OurNepali dataset. \n\n", " unanswerable, ", " unanswerable, ", " unanswerable, ", " The dataset is 64% for training, 16% for development, and 20% for testing. \n", " 72782, ", " 6946 sentences, ", " 10 times bigger compared to ILPRL dataset in terms of entities, ", " 72782 words, ", " The dataset contains a total of 64% for training set, 16% for development set, and 20% for test set respectively. The exact number of entities is not mentioned., ", " \n", " unanswerable, ", " unanswerable, ", " Unanswerable, ", " OurNepali dataset has three major classes Person (PER), Location (LOC) and Organization (ORG), ", " The dataset volume is almost ten times bigger compared to ILPRL dataset in terms of entities. ", " The dataset size varies, ", " 14 million words, ", " 6946 sentences, ", " 64% of the dataset is used for training, 16% for development, and 20% for testing. The total dataset size is not explicitly stated. However, the dataset volume of OurNepali is nearly 10 times bigger than that of ILPRL dataset in terms of entities. \n\n", " unanswerable, ", " unanswerable, ", " OurNepali dataset volume is almost ten times bigger compared to ILPRL dataset in terms of entities, ", " Not specified in terms of the number of data points, but it is said to be \"almost ten times bigger\" compared to the ILPRL dataset. \n", " The dataset is around 10 times bigger compared to ILPRL dataset in terms of entities. \n", " Unanswerable, ", " 72782, ", " unanswerable, ", " 72782 unique words, ", " The size of the dataset is 6946 sentences and 16225 unique words, \n", " unanswerable, ", " \n", " Not directly answerable from the article, ", " unanswerable, ", " Unanswerable, ", " 64% of the total dataset is divided into training set., ", " unanswerable, ", " The dataset size is not explicitly stated but the total entities (PER, LOC, ORG and MISC) from both the datasets is 15,143 and 1,572 for OurNepali dataset and ILPRL dataset respectively, ", " 6946 sentences, ", " unanswerable, ", " Not explicitly mentioned, ", " 6946 sentences and 16225 unique words. \n", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " The size of the dataset is 64% for training set, 16% for development set and 20% for test set. \n\n", " The size of the dataset is around 64%, 16% and 20% of the total dataset, with 64% for training set, 16% for development set and 20% for test set, respectively, for both OurNepali dataset and ILPRL dataset. ", " unanswerable, ", " unanswerable, ", " 72782 unique words, ", " 64%, 16% and 20% of the total dataset, ", " 6946 sentences, ", " unanswerable, ", " unanswerable, ", " \n", " unanswerable, ", " unanswerable, ", " 64% of the total dataset into training set, 16% into development set and 20% into test set. \n", " Unanswerable, ", " unanswerable, ", " 72782 words, ", " The size of the dataset is 64% for training set, 16% for development set, and 20% for test set, ", " unanswerable \n", " unanswerable, ", " The dataset sizes are not provided in a single number, ", " 72782, ", " 64% of the dataset for training set, 16% for development set, and 20% for test set. \n", " 72782, ", " 72782 words, ", " unanswerable, ", " The size of the dataset is almost ten times bigger in terms of entities in the OurNepali dataset compared to the ILPRL dataset. \n", " unanswerable, ", " 64%, 16%, and 20% of the total dataset into training set, development set and test set respectively, ", " unanswerable, "], "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": [" Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n\n", " The 16 languages considered are: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " The languages that are explored are the ones for which we have morphosyntactic lexicons at our disposal. ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n\n", " 16 languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish, and languages for which BIBREF20 provide results. \n", " 16 languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " \n", " [Your concise final answer here], ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages are explored. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " The 16 languages explored in the article are: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " 16 languages including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish, \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages, ", " 16 languages, ", " 16 languages. \n", " English, Bulgarian, Croatian, Czech, Danish, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " 16 languages, \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish. \n", " English, Bulgarian, Croatian, Czech, Danish, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish, ", " 16 languages. \n", " 16 languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish., ", " 16 languages. \n", " English, Indonesian, Italian, and several other languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish, ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish., ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages, ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages, ", " sixteen languages are explored in this paper, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages, including Indo-European sub-families. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish. \n", " 16 languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. ", " 16 languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " 16 languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " 16 languages. \n", " sixteen languages, ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish, ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. ", " 16 languages, ", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " ", " 16 languages are explored, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " 16 languages, including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.\n\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages including Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n", " 16 languages. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n\n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n\n", " English, Danish, Bulgarian, Croatian, Czech, Indonesian, Italian, French, German, Norwegian, Polish, Portuguese, Slovenian, Spanish, Swedish, and Persian. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. ", " 16 languages are explored. \n", " 16 languages, ", " The languages explored are 16 languages from Universal Dependencies v1.2. \n", " English, French, Italian, Indonesian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish, Bulgarian, Croatian, Czech, Danish, German. \n", " Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish. \n", " 16 languages. \n", " 16 languages. \n", " \n", " 16 languages. ", " 16 languages. \n"], "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": [" Bi-directional and uni-directional transformer-based language models. ", " Bi-directional and uni-directional, ", " Bi-directional and uni-directional language model architectures. \n", " Bi-directional and uni-directional models, both using self-attention. ", " Bi-directional and uni-directional transformer architectures. \n", " Bi-directional and uni-directional language models. \n", " The bi-directional language model and the uni-directional model are used. \n", " bi-directional and uni-directional. \n", " Bi-directional and uni-directional transformer models, ", " Bi-directional and uni-directional models. \n", " Bi-directional and uni-directional language model architectures, ", " Bi-directional and uni-directional models using self-attention. \n", " Bi-directional language model and uni-directional model, ", " A bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. \n", " Bi-directional and uni-directional language models. \n", " Bi-directional and uni-directional transformer models. \n", " A bi-directional language model and a uni-directional language model. ", " Bi-directional and uni-directional language model architectures, ", " Bi-directional and uni-directional. \n", " Bi-directional language model and uni-directional model. \n", " Bi-directional and uni-directional transformer-based language models. \n", " Bi-directional language model and uni-directional model. \n", " Bi-directional and uni-directional models. \n", " bi-directional and uni-directional models. ", " The bi-directional language model and the uni-directional language model. ", " Bi-directional and uni-directional language model architectures are used. ", " The bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. ", " Bi-directional language model and uni-directional model. \n", " Bi-directional and Uni-directional, ", " The bi-directional and uni-directional language models are used. \n", " Bi-directional and Uni-directional language models. \n", " Bi-directional and uni-directional language models. \n", " The language model architectures used are Big Transformer and bi-directional language model. \n", " Bi-directional and uni-directional transformer-based language models are used. \n", " Bi-directional and uni-directional transformer-based language models, ", " A bi-directional language model and a uni-directional model. \n", " Bi-directional and uni-directional transformer-based language models, ", " Bi-directional and uni-directional, ", " A bi-directional language model and a uni-directional model. \n", " Bi-directional and uni-directional transformer-based language models. ", " The bi-directional language model and the uni-directional language model are used. \n", " Bi-directional and uni-directional language models. ", " The bi-directional language model and the uni-directional model. ", " Bi-directional and uni-directional transformer models. \n", " Bi-directional and uni-directional transformer-based language models are used. \n", " Bi-directional and uni-directional transformer-based language models, ", " Bi-directional and uni-directional transformer-based language models. \n", " Bi-directional and uni-directional, ", " Bi-directional and uni-directional language model architectures. \n", " Bi-directional and uni-directional language models. \n", " A bi-directional language model and a uni-directional model, both based on the Big Transformer architecture. \n", " Bi-directional and uni-directional language model architectures. ", " A bi-directional language model and a uni-directional model. ", " The bi-directional and uni-directional models. ", " Bi-directional and uni-directional transformer-based language models. \n", " Bi-directional language model and uni-directional language model. \n", " Bi-directional and Uni-directional transformer-based language models. \n", " Bi-directional and uni-directional transformer-based language models are used. ", " Bi-directional and uni-directional transformer models, ", " Bi-directional and uni-directional language models, as well as Big Transformer architectures. ", " The bi-directional and uni-directional models, as well as the standard Big Transformer architecture. ", " ", " bi-directional and uni-directional models, ", " bi-directional and uni-directional. \n", " Bi-directional and uni-directional models, ", " Bi-directional and uni-directional models using self-attention. ", " Bi-directional language model and uni-directional model. \n", " Bi-directional and uni-directional models using self-attention, ", " bi-directional and uni-directional models. \n", " Bi-directional language model and uni-directional model. \n", " bi-directional and uni-directional models. \n", " The language model architectures used are bi-directional and uni-directional models, both of which are transformer-based models with different settings. ", " Bi-directional language model and uni-directional language model. \n", " Bi-directional language model and Uni-directional language model. \n", " A bi-directional language model and a uni-directional language model are used. \n", " Bi-directional and uni-directional language models are used, both using self-attention. \n", " Bi-directional and uni-directional models. \n", " Bi-directional and Uni-directional transformer architectures. ", " Two types of architectures are used: a bi-directional language model and a uni-directional model. \n", " Two types of architectures are used. ", " Bi-directional and uni-directional models. \n", " Bi-directional and uni-directional models using self-attention. \n", " Bi-directional and uni-directional transformer-based language models. ", " A bi-directional and a uni-directional language model, ", " Bi-directional and uni-directional language model architectures are used. \n", " Bi-directional and uni-directional. ", " Bi-directional and uni-directional transformer models. \n", " Bi-directional and uni-directional transformer-based language models, and Big Transformer sequence to sequence architecture. \n", " Bi-directional and Uni-directional models. \n", " A bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. \n", " The article presents two types of architectures: a bi-directional language model and a uni-directional language model. \n", " Bi-directional and uni-directional transformer models, ", " Bi-directional and uni-directional transformer models, ", " Bi-directional and uni-directional transformer-based language models are used. \n", " Bi-directional and uni-directional models. ", " Bi-directional and uni-directional language models using transformer architecture. \n", " Bi-directional and uni-directional, ", " Bi-directional and uni-directional language models, ", " Bi-directional and uni-directional language models are used. ", " Bi-directional and uni-directional transformer-based language models. \n"], "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": [" Using a dynamic weight adjusting strategy that associates each training example with a weight in proportion to $(1-p)$ and this weight dynamically changes as training proceeds. \n", " dynamically based on $(1-p)$, ", " Dynamically changed as training proceeds, with higher weight on hard-negative examples. \n", " dynamically changed as training proceeds. \n", " A dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p). \n", " dynamically changed as training proceeds. \n", " dynamically changing as training proceeds. \n", " dynamically adjusted as $(1-p)$, ", " dynamically changed as training proceeds. ", " dynamically adjusted based on $(1-p)$. ", " They are adjusted by using a dynamic weight adjusting strategy that associates each training example with a weight in proportion to (1-p), where p is the probability that the model assigns a positive label to the example. \n", " By associating each training example with a weight in proportion to (1-p), ", " By using a weight in proportion to (1-p), which dynamically changes as training proceeds. \n", " A dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p). \n", " Through a decaying factor of (1-p), ", " dynamically based on (1-p) where p is the probability that a model assigns the positive label to an instance. \n", " Dynamically changing as training proceeds. ", " Through a dynamic weight adjusting strategy that associates each training example with a weight in proportion to (1-p). \n", " In proportion to (1-p). \n", " weights are dynamically adjusted using a strategy that associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds. \n", " dynamically change as training proceeds, ", " through a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n\n", " By multiplying the soft probability p with a decaying factor (1-p). ", " The weights are dynamically adjusted using a strategy that associates each training example with a weight in proportion to (1-p). \n", " Dynamic weights adjusting strategy associates each training example with a weight in proportion to (1-p). \n", " Dynamically changing as training proceeds. ", " by a strategy that associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " dynamically according to $(1-p)$, ", " As training proceeds. \n", " They are dynamically adjusted in proportion to $(1-p)$, ", " They are adjusted in proportion to (1-p). \n", " in proportion to (1-p) \n", " Dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " The weights are adjusted based on (1-p). \n", " Dynamically adjusted through the weight associated with each example, which changes as training proceeds. \n", " Dynamically adjusted by $(1-p)$, ", " Dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p). ", " Dynamically adjusted weights are associated with each training example, in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " Dynamic weights are adjusted as they change as training proceeds, based on a decaying factor (1-p). ", " Dynamically changing weights are associated with examples in proportion to $(1-p)$. \n", " dynamically in proportion to (1-p). \n", " Dynamically adjusting the weights based on (1-p) where p is the probability of each example being correctly classified. ", " in proportion to (1-p), ", " Dynamically by multiplying the soft probability with a decaying factor (1-p), ", " According to a dynamic weight adjusting strategy associated with each training example in proportion to $(1-p)$, where $p$ is the probability assigned by the model. \n", " dynamically as training proceeds, ", " Through a strategy that associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " through a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to $(1-p)$. \n", " Dynamically according to $(1-p)$, ", " Dynamically adjusted weights are associated with each training example in proportion to $(1-p)$, ", " Dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. ", " The weights are dynamically adjusted based on a decaying factor (1-p), ", " Dynamically changed as training proceeds, in proportion to (1-p). \n", " Dynamically based on (1-p), ", " Through a dynamic weight adjusting strategy that associates each training example with a weight in proportion to (1-p). ", " They are adjusted based on $(1-p)$, ", " The weights are dynamically adjusted based on the formula $(1-p)$, where $p$ is the probability of the example, and are adjusted to deemphasize confident examples during training. ", " The weights are dynamically adjusted using a strategy that associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " dynamically based on the confidence of the model's predictions. \n", " Through a strategy that associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " In proportion to (1-p). \n", " weights dynamically change as training proceeds by multiplying soft probabilities with a decaying factor. ", " Using a dynamic weight adjusting strategy. ", " By multiplying the soft probability $p$ with a decaying factor $(1-p)$. \n", " Dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p), ", " Through a strategy that associates each training example with a weight in proportion to (1-p). ", " dynamically changes as training proceeds, ", " According to $(1-p)$, ", " dynamically changed as training proceeds. ", " dynamically adjust weights based on $(1-p)$. ", " Dynamically, weights are adjusted in proportion to (1-p). \n", " Dynamic weights are adjusted in proportion to (1-p). \n", " They are adjusted by associating each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " They are adjusted based on (1-p), ", " weights are dynamically adjusted through a strategy that associates each training example with a weight in proportion to (1-p). \n", " dynamically changes as training proceeds and is associated with the weight $(1-p)$. \n", " dynamically change as training proceeds. ", " In proportion to $(1-p)$. \n", " dynamically adjusted based on (1-p) ", " They are adjusted based on (1-p). \n", " They are dynamically adjusted based on (1-p), ", " By multiplying the soft probability p with a decaying factor (1-p). ", " The weights are dynamically adjusted by multiplying the soft probability $p$ with a decaying factor $(1-p)$ and then using this new weight in the training objective. \n", " The weights are dynamically adjusted using a strategy that associates each training example with a weight in proportion to (1-p), where p is the model's predicted probability of the example belonging to the positive class. The weight dynamically changes as training proceeds. \n", " A dynamic weight adjusting strategy associates each training example with a weight in proportion to (1-p), which dynamically changes as training proceeds. \n", " They are dynamically adjusted using a strategy that associates each training example with a weight in proportion to (1-p), where p is the probability assigned by the model. ", " As training proceeds, the weights are dynamically adjusted based on $(1-p)$, and they change from 1 to 0 as $p$ approaches 1. ", " The weights are dynamically adjusted using a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " According to the value of $(1-p)$, ", " By associating each training example with a weight in proportion to (1-p), and this weight dynamically changes as training proceeds. \n", " dynamically based on (1-p), \n", " They are dynamically adjusted in proportion to $(1-p)$. \n", " by associating each training example with a weight in proportion to (1-p). \n", " The weights are dynamically adjusted by associating each training example with a weight in proportion to $(1-p)$, where $p$ is the probability of the positive label. \n", " Through a dynamic weight adjusting strategy, which associates each training example with a weight in proportion to (1-p). \n", " They are dynamically adjusted through a decaying factor (1-p). \n", " Based on the soft probability p, the weights are dynamically adjusted by multiplying the soft probability p with a decaying factor (1-p). \n", " Dynamically based on (1-p), ", " dynamically change as training proceeds, ", " dynamically as training proceeds. "], "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": [" It is identified based on the speaker annotations. ", " Using automatic phonetic alignment. ", " By excessive silence, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and non-verbal articulations. \n", " It is identified through automatic removal of excessive silence from the start, middle and end of turns (based on occurrence of F0). \n", " It is labeled as \"mispronunciations\". \n", " It is identified through annotations. ", " Not specified, ", " Using annotations for noises and disfluencies. \n", " It is identified through the annotation of non-verbal articulations, and undefined sound or pronunciations. \n", " It is identified by using a now more standard text encoding format than what was used when the data was first collected, along with standard procedure for speech recognition and synthesis datasets. \n", " Using audio and speech recognition techniques. ", " By labeling non-verbal articulations and undefined sound or pronunciations, and noises and disfluencies. ", " Non-standard pronunciation is identified as mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and non-verbal articulations.  \n", " The non-standard pronunciation is identified through the annotations of the Spanish words interspersed in Mapudungun speech. \n", " Not explicitly identified. ", " It is identified through the annotations. \n", " \n", " by phonetic alignment. \n", " They are labelled as mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " No specific method is mentioned for identifying non-standard pronunciation in the article. However, the article mentions that foreign words (in this case Spanish words) are labeled as such in the transcription, and disfluencies such as mispronunciations and aborted words are also annotated. ", " It is labelled as mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and non-verbal articulations. ", " It is labeled as \"mispronunciations\". \n", " Using a technique in BIBREF7 for found data, including re-aligning the text to the audio and finding out which turns are best (or worst) aligned, so that the segments that give the most accurate alignments can be selected. \n\n", " Through the use of phonetic alignment. ", " unanswerable, ", " It is labelled as such. ", " Through the phonetic alignment of transcribed text with audio. \n", " By annotating non-verbal articulations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " It is identified as non-verbal articulations, aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and noises. \n", " By using diacritics such as apostrophes to indicate sounds that are not found in Spanish. \n", " By analyzing F0, ", " It is identified by annotation, ", " It is identified through annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " Through annotated labels for noises and disfluencies, including mispronunciations. \n", " unanswerable, ", " By labeling as \"mispronunciations\". ", " unanswerable, ", " Non-standard pronunciation is identified by annotating non-verbal articulations and noises. ", " By annotating non-verbal articulations, undefined sound or pronunciations, non-standardized orthographic transcriptions, and noises. \n", " Through annotations of the Spanish words interspersed in Mapudungun speech. \n", " By excessive silence, pauses, non-verbal articulations, aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations. \n", " It is labeled as mispronunciations. \n", " By removing excessive silence from the start, middle and end of the turns. \n", " Through annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " By labeling aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " Automatically, ", " Not specified in the article. ", " By identifying aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and non-verbal articulations. ", " By labeling mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and non-verbal articulations. \n", " Through annotations of non-standardized orthographic transcriptions. ", " \n", " By labeling as non-verbal articulations and undefined sound or pronunciations. \n", " Not specified, ", " Not explicitly identified, ", " It is identified as mispronunciations, aborted words, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and non-verbal articulations. \n", " It is identified by annotations for noises and disfluencies. \n", " By marking it as a non-verbal articulation, undefined sound or pronunciation, or a mispronunciation. \n", " Using Kaldi and automatic phonetic alignment. ", " By analyzing mispronunciations, poor intelligibility, and non-verbal articulations. ", " through the use of a transcription tool and annotations, ", "  The resource has annotations for non-verbal articulations and noises and disfluencies. \n", " By using the generated phonetic lexicon. \n", " By a non-trivial number of utterance boundaries and speaker annotations were flawed., ", " Using the start and end points for each turn in the dialogues, and selecting those of the most frequent speaker, nmlch. ", " Using annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " By annotating non-verbal articulations and noises, such as aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, and pauses. \n", " By the annotations of non-verbal articulations, poor intelligibility, and mispronunciations. \n", " By identifying \"mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses\". \n", " Using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech generation. \n", " Using techniques from BIBREF7. \n", " It is identified by annotated labels for non-standardized orthographic transcriptions, as well as foreign words labelled as such. \n", " By labeling as such. ", " The non-standard pronunciation is identified using Mel Cepstral Distortion, a standard method for automatically measuring quality of speech generation. \n", " It is identified as non-standard using annotations. \n", " By using audio alignment techniques. \n", " By the annotation of the speaker in the audio. \n", " Automatically. \n", " Using mispronunciations, poor intelligibility, aborted words, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " Using annotations of non-verbal articulations. \n", " By annotated labels such as \"non-verbal articulations, and pauses\" in the transcription. \n", " Through annotations for noises and disfluencies, including aborted words, mispronunciations, poor intelligibility. \n", " Non-standard pronunciation is identified through annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " Unanswerable, ", "  Using found data to re-align the text to the audio and identify the turns that give the most accurate alignments. \n", " By labeling noisy segments in the transcription including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " Using the TransEdit transcription tool v.1.1 beta 10 and other techniques. \n", ", ", " Non-standard pronunciation is identified by annotations for noises and disfluencies. \n", ", ", " By using noise and disfluency annotations. \n", " Using annotated transcripts, ", " Using phonetic alignment, ", " Through the annotation of noises and disfluencies, including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. ", " It is identified by labelling as non-verbal articulations and undefined sound or pronunciations. \n", " It is identified by annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " They are labelled as \"non-verbal articulations\". \n", " It is annotated as non-verbal articulations. ", " By annotations of non-verbal articulations and pauses, and by removing excessive silence from the start, middle and end of turns. \n", " Using mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. \n", " Not clearly identified in the article, "], "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": [" +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, \n", " +0.58 for MRPC and +0.73 for QQP, \n", " +0.58 for MRPC and +0.73 for QQP. \n", "  +0.58, ", " +0.58 for MRPC and +0.73 for QQP. \n", " 0.58 and 0.73, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58, +0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, \n", " ", " +0.58 for MRPC and +0.73 for QQP. \n", " 0.58 and 0.73, ", " +0.58 for MRPC and +0.73 for QQP. ", " +0.73 for QQP. \n", " +0.58 and +0.73, \n", " +0.58 and +0.73, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " 0.58, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " 0.58, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n\n", " +0.58 for MRPC and +0.73 for QQP, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73 \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58, +0.73, \n", " 0.58 and 0.73, \n", " 0.58 for MRPC and 0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, \n", " +0.58 and +0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, \n", " +0.58 for MRPC and +0.73 for QQP, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " 0.58, ", " +0.58 and +0.73 for BERT and XLNet respectively, ", " 0.58 and 0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " 0.58, \n", " +0.58 for MRPC and +0.73 for QQP.\n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 for MRPC and +0.73 for QQP, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73, \n", " +0.58 for MRPC and +0.73 for QQP \n", " 0.58 and 0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, \n", " +0.58 and +0.73, \n", " +0.58, +0.73, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 and +0.73 for F1 score. \n", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, ", " +0.58 and +0.73, \n", " +0.58 for MRPC and +0.73 for QQP. \n", " +0.58 for MRPC and +0.73 for QQP, ", " 0.58 and 0.73,  \n", " +0.58 and +0.73. ", " +0.58 for MRPC and +0.73 for QQP, \n"], "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": [" They outperformed the state-of-the-art collective methods across five different datasets. \n", " Effective, ", " It outperforms various baselines with a favorable generalization ability. \n", " It outperforms the state-of-the-art collective methods across five different datasets. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " The NCEL approach outperforms various baselines consistently, demonstrating its effectiveness. \n", " The NCEL approach outperforms state-of-the-art collective methods across five different datasets, demonstrating its effectiveness. \n\n", " NCEL consistently outperforms various baselines. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability.  \n", " It achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. ", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " It achieves significant improvements over state-of-the-art methods on several datasets, with an average gain of 2% on Micro F1 and 3% Macro F1 on GERBIL. ", " Effective, ", " It achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. \n", " It outperforms the state-of-the-art collective methods across five different datasets. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n\n", " Consistently outperforms baselines with favorable generalization ability. \n", " Effective, ", " It outperforms various baselines with a favorable generalization ability. \n", " Effective, ", " It outperforms the state-of-the-art collective methods across five different datasets, achieving an average gain of 2% on Micro F1 and 3% Macro F1. \n", " NCEL consistently outperforms various baselines. \n", " It is very effective. \n", " It consistently outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n\n", " It outperforms various baselines with a favorable generalization ability. ", " It consistently outperforms various baselines with a favorable generalization ability. \n", " Effective, ", " It is very effective, consistently outperforming various baselines with a favorable generalization ability. \n", " It is effective. \n", " NCEL outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " Effective, ", " Effective, ", " It is effective, ", " Their NCEL approach shows a good generalization ability to difficult EL and consistently outperforms various baselines across five different datasets. ", " The NCEL approach achieves the best performance in most cases. \n", " NCEL outperforms the state-of-the-art collective methods across five different datasets. \n", " Effective, ", " Effective, ", " NCEL outperforms the state-of-the-art collective methods across five different datasets. \n\n", " Effective, ", " NCEL outperforms state-of-the-art collective methods across five different datasets. \n", " It outperforms all baseline methods in both \"easy\" and \"hard\" cases. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines. \n", " Effective, ", " It is effective, ", " NCEL consistently outperforms various baselines. \n", " It consistently outperforms various baselines with a favorable generalization ability. \n", " NCEL outperforms various baselines with a favorable generalization ability. \n", " It is highly effective. \n", " The NCEL approach outperforms various baselines with a favorable generalization ability. \n", " The NCEL approach performs consistently well on all datasets, demonstrating good generalization ability. \n", " It achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. \n", " NCEL consistently outperforms various baselines, ", " It outperforms various baselines with a favorable generalization ability. \n", " NCEL outperforms state-of-the-art collective methods across five different datasets. ", " Highly effective, ", " Consistently outperforms various baselines, ", " The NCEL approach outperforms other state-of-the-art methods across all five datasets tested with an average gain of 2% on Micro F1 and 3% on Macro F1. \n", " NCEL outperforms the state-of-the-art collective methods across five different datasets. \n", " Effective. \n", " It outperforms various baselines with a favorable generalization ability, ", " Effective, ", " Effective, ", " NCEL consistently outperforms various baselines, ", " NCEL consistently outperforms various baselines with a favorable generalization ability. ", " They outperform the state-of-the-art collective methods across five different datasets. \n", " It outperforms various baselines with a favorable generalization ability. \n", " Effective, ", " Consistently outperforms other baselines.  \n", " NCEL achieves the best performance in most cases with an average gain of 2% on Micro F1 and 3% Macro F1. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " It outperforms the state-of-the-art collective methods across five different datasets. ", " Effective, ", " It outperforms various baselines with a favorable generalization ability. \n", " They outperform the state-of-the-art collective methods across five different datasets, ", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability, ", " It outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines. \n", " NCEL outperforms various baselines with a favorable generalization ability. \n\n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " Their NCEL approach outperforms the state-of-the-art collective methods across five different datasets. \n", " NCEL outperforms various baselines with a favorable generalization ability. ", " They outperform state-of-the-art collective methods across five different datasets. \n", " Effective, ", " It outperforms various baselines with a favorable generalization ability. ", " It is effective overall. \n", " It outperforms various baselines with a favorable generalization ability. \n", " NCEL consistently outperforms various baselines with a favorable generalization ability. \n", " It is very effective, outperforming various baselines with a favorable generalization ability. ", " It outperforms state-of-the-art collective methods across five different datasets, \n", " Their NCEL approach outperforms state-of-the-art collective methods. \n", " It outperforms various baselines with a favorable generalization ability. \n", " It outperforms various state-of-the-art baselines with a favorable generalization ability. \n\n", " NCEL outperforms the state-of-the-art collective methods across five different datasets. \n\n", " Their NCEL approach is effective overall. \n"], "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": [" The results show that both KG-A2C-chained and KG-A2C-Explore can pass the bottleneck of a score of 40. \n", " Both KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. ", " Both methods (KG-A2C-chained and KG-A2C-Explore) successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores. ", " Yes, both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n\n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, with both KG-A2C-chained and KG-A2C-Explore both passing the bottleneck of a score of 40. \n\n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error, ", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " Effective, ", " ", " Both agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. ", " The proposed strategies were able to pass the bottleneck of a score of 40 in the game of Zork1. \n", " They successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp. \n", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Both proposed strategies successfully pass the bottleneck in Zork1. \n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, and KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n\n", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C.\n", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. ", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " The knowledge graph appears to be critical and the proposed strategies KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, surpassing existing methods, whereas the baseline agents (A2C and KG-A2C) fail to pass the bottleneck. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n\n\n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error, with KG-A2C-chained being significantly more sample efficient and converging faster. \n\n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, and both KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " The proposed exploration strategies successfully pass the bottleneck of a score of 40 in the game Zork1. \n", " The proposed strategies pass the bottleneck in Zork1. \n", " The proposed exploration strategies allow agents to successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. ", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. Both proposed strategies, KG-A2C-chained and KG-A2C-Explore, pass the bottleneck of a score of 40. \n", " Both KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. KG-A2C-chained is significantly more sample efficient and converges faster, implying that chained exploration with backtracking is particularly suited to sequential decision making problems that can be represented as acyclic directed graphs as in Figure FIGREF1. However, the knowledge graph cell representation appears to be a better indication of what a promising state is as opposed to just the textual observation, as seen in the faster and more consistent convergence of KG", " The proposed strategies (KG-A2C-chained and KG-A2C-Explore) successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp, and reach comparable scores within a margin of error. \n", " They pass the bottleneck corresponding to entering the cellar and lighting the lamp, but KG-A2C-chained is significantly more sample efficient and converges faster. \n", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " The proposed strategies successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp. \n", " Both agents utilizing knowledge-graphs and enhanced exploration method far outperform the baseline A2C and KG-A2C. \n", " Both KG-A2C-chained and KG-A2C-Explore agents successfully pass the bottleneck and reach comparable scores. ", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n\n", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp, and reach comparable scores within a margin of error. \n", " The proposed strategies are able to pass the bottleneck in the Zork1 game, and some agents reach a comparable score within a margin of error, while others converge faster. ", " The results from these proposed strategies show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, with both the knowledge graph cell representation and chaining policies by explicitly detecting bottlenecks leading to success in passing the bottleneck of a score of 40. \n", " Both agents successfully pass the bottleneck, with KG-A2C-chained being significantly more sample efficient and converging faster. ", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error., ", " The proposed strategies are successful in surpassing the bottlenecks in the Zork1 game, with KG-A2C-chained and KG-A2C-Explore both passing the bottleneck corresponding to a score of 40. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp. \n", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it., ", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. \n", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp. \n", " The proposed strategies successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. \n", " They far outperform the baseline A2C and KG-A2C. ", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, with both KG-A2C-chained and KG-A2C-Explore passing the bottleneck of a score of 40. \n", " The proposed strategies show that agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. Both proposed strategies, KG-A2C-chained and KG-A2C-Explore, are able to pass the bottleneck of a score of 40. \n\n", " The proposed strategies of KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. Both proposed methods are able to pass the bottleneck of a score of 40. KG-A2C-chained is more sample efficient, but KG-A2C-Explore takes longer to reach a similar reward. \n", " Both methods successfully pass the bottleneck. \n", " Both agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C and pass the bottleneck of a score of 40. \n", " They successfully pass the bottleneck, ", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck, whereas A2C-Explore gets to the bottleneck but cannot surpass it. \n", " The proposed strategies KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " The knowledge graph-based agents using enhanced exploration methods successfully pass the bottleneck in Zork1. \n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. ", " Both KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. ", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp, but KG-A2C-chained is significantly more sample efficient and converges faster. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp, ", " Both proposed strategies successfully pass the bottleneck and reach comparable scores, with the chained exploration method being significantly more sample efficient and converging faster. \n", " The proposed strategies successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp, and reach comparable scores within a margin of error, with KG-A2C-chained being significantly more sample efficient and converging faster. \n", " The proposed strategies succeed in passing the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Both KG-A2C-chained and KG-A2C-Explore agents successfully pass the bottleneck, with KG-A2C-chained being significantly more sample efficient. \n", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Both agents successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Both KG-A2C-chained and KG-A2C-Explore succeed in surpassing existing methods. \n", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, with both KG-A2C-chained and KG-A2C-Explore passing the bottleneck of a score of 40. \n", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. \n", " The proposed exploration strategies pass the bottleneck of a score of 40. \n", " Both agents using the proposed exploration strategies successfully pass the bottleneck and reach comparable scores within a margin of error. \n", " Both agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, and both successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n\n", " They both pass the bottleneck, but KG-A2C-chained converges faster and is more sample efficient. ", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " Both methods are able to pass the bottleneck, but the chained policy method is more sample efficient and converges faster. \n", " Both KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. ", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " The proposed strategies both pass the bottleneck corresponding to entering the cellar and lighting the lamp. ", " Agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C and pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it. \n", " Both agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C and pass the bottleneck of a score of 40. \n\n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck, while A2C-Explore gets to the bottleneck but cannot surpass it. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck. ", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp, and reach comparable scores. ", " Both agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. \n", " Both KG-A2C-chained and KG-A2C-Explore are able to pass the bottleneck of a score of 40, but with different efficiency. ", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40. \n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C, with KG-A2C-chained and KG-A2C-Explore both passing the bottleneck of a score of 40. \n", " KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40, whereas A2C-Explore gets to the bottleneck but cannot surpass it, while agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. \n", " The knowledge graph appears to be critical for surpassing the bottleneck. \n", " The proposed strategies were able to pass the bottleneck at a score of 40, whereas the baseline A2C and KG-A2C were not. \n", " The agents utilizing knowledge-graphs in addition to either enhanced exploration method far outperform the baseline A2C and KG-A2C. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck corresponding to entering the cellar and lighting the lamp and reach comparable scores within a margin of error. \n", " Both KG-A2C-chained and KG-A2C-Explore successfully pass the bottleneck and reach comparable scores. \n"], "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": [" It is an architecture that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character based RNN. \n", " A semicharacter architecture is an RNN-based semi-character word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " It is an RNN-based model that predicts correct words at each step, treating the first and the last characters individually, and is agnostic to the ordering of the internal characters. \n\n", " A semi-character architecture is a word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN. \n", " A semicharacter RNN (ScRNN) is an architecture for word recognition that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " It is an RNN-based semi-character word recognition model. \n", " Semi-character RNN. \n", " A semi-character based RNN (ScRNN). \n", " A semi-character architecture is a BiLSTM cell that treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. \n", " A semi-character RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture. ", " A semicharacter-based RNN. \n", " Semi-character RNN, ", " It is a semi-character RNN (ScRNN). \n", " A semi-character architecture is a BiLSTM-based RNN that treats the first and last characters individually, and is agnostic to the ordering of the internal characters. \n", " A semicharacter architecture is an RNN-based model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture is a word recognition model that treats the first and last characters individually and is agnostic to the ordering of internal characters.\n", " A BiLSTM cell. ", " A semi-character architecture is a type of recurrent neural network (RNN) based word recognition model. \n", " A BiLSTM model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture uses a one-hot vector of the first character, a one-hot representation of the last character, and a bag of characters representation of the internal characters. \n", " A semi-character based RNN (ScRNN). \n", " A semi-character architecture is a type of word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " A semi-character architecture is a recurrent neural network that processes a sequence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semicharacter architecture is a type of word recognition model that predicts the correct words given a full sequence of possibly misspelled inputs. \n", " Semi-character RNNs for word recognition. \n", " A semi-character RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture is a neural network model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " A semi-character RNN-based word recognizer, ", " A semi-character RNN. \n", " Semi-character RNN (ScRNN) for word recognition. \n", " It is a RNN-based semi-character word recognition model. \n", " Semi-character RNN, ", " A semi-character RNN. \n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " Semi-character based RNN (ScRNN). \n", " A semi-character architecture that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture is a type of recurrent neural network (RNN) based semi-character word recognition model. \n\n", " Semi-character RNNs for word recognition. ", " A semi-character based RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN, or ScRNN, that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " It is a semi-character based RNN (ScRNN) model that predicts the correct words at each step in a sentence with misspelled characters. \n\n", " A semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semicharacter architecture is a type of RNN-based word recognition model. \n", " A semi-character architecture, ", " A semi-character RNN model, ", " It is a recurrent neural network (RNN) architecture. \n", " A semi-character architecture is a type of neural network that treats the first and last characters of each word individually and is agnostic to the ordering of the internal characters. \n", " A BiLSTM cell. \n", " Semi-character RNN. \n", " A semi-character based RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character based RNN (ScRNN), ", " A semicharacter RNN for word recognition. \n", " It is an RNN-based word recognition model. \n", " Semi-character RNN \n", " \n", " A semi-character RNN (ScRNN) \n", " A semi-character based RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " Semi-character RNN, ", " A semi-character RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step, inspired by psycholinguistic studies. \n", " A semi-character based RNN. \n", " A semi-character RNN architecture processes a sentence of words with misspelled characters, predicting the correct words at each step, treating the first and the last characters individually, and is agnostic to the ordering of the internal characters. \n", " A semi-character architecture, ", " A semi-character RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " Semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " A semi-character RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character based RNN (ScRNN). \n", " A semi-character RNN. \n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " semicharacter RNNs \n", " It is a semi-character RNN for word recognition. \n", " A semi-character RNN (ScRNN) architecture. \n", " A semi-character architecture is a type of word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN architecture that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A BiLSTM cell architecture. \n", " \n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " Semi-character architecture \n", " A semi-character RNN model. \n", " It is an RNN-based model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture is a neural network architecture that combines character-level and word-level information for text processing tasks. ", " Semi-character RNN for word recognition. \n", " It is a semi-character RNN. \n", " A semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n\n", " A semi-character RNN that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character architecture is a recurrent neural network (RNN) model that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " Semi-character RNN (ScRNN), ", " A semi-character RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n", " A semi-character RNN. \n", " A semi-character RNN, ScRNN, that processes a sentence of words with misspelled characters, predicting the correct words at each step. \n"], "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": [" Each tweet is annotated as no evidence of depression or evidence of depression, and if evidence of depression, further annotated with one or more depressive symptoms. \n", " The dataset is annotated with 7 feature groups with associated feature values binarized. \n", " Each tweet is annotated as no evidence of depression or evidence of depression. If there is evidence of depression, the tweet is further annotated with one or more depressive symptoms. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. ", " The dataset is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, such as depressed mood, disturbed sleep, or fatigue or loss of energy. \n", " The dataset contains 9,473 annotations for 9,300 tweets, where each tweet is annotated as no evidence of depression or evidence of depression, and if annotated as evidence of depression, it is further annotated with one or more depressive symptoms. \n\n", " Each tweet is annotated as no evidence of depression or evidence of depression, with further annotation with one or more depressive symptoms if a tweet is annotated as evidence of depression. ", " The dataset contains 9,473 annotations for 9,300 tweets, each annotated as either no evidence of depression or evidence of depression. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. ", " Tweets are annotated as no evidence of depression or evidence of depression with further annotations of depressive symptoms. ", " Each tweet is annotated as no evidence of depression or evidence of depression, and if annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n\n", " The dataset is annotated as no evidence of depression or evidence of depression, and if evidence of depression, it is further annotated with one or more depressive symptoms. \n", " \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, it is further annotated with one or more depressive symptoms. ", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy. ", " It is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets. \n", " Each tweet is annotated as no evidence of depression or evidence of depression, with evidence of depression further annotated with one or more depressive symptoms. \n\n", " Tweets are annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, they are further annotated with one or more depressive symptoms. ", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset is annotated for depression or no evidence of depression, and if evidence of depression, it is further annotated with one or more depressive symptoms. \n", " It is encoded with 7 feature groups with associated feature values binarized. ", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and further annotated with one or more depressive symptoms if there is evidence of depression. \n\n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if annotated as evidence of depression, it is further annotated with one or more depressive symptoms. ", " Each tweet is annotated as no evidence of depression or evidence of depression, with the latter further annotated with one or more depressive symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression. If there is evidence of depression, it is further annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy. \n", " The dataset is annotated with a hierarchical model of depression-related symptoms. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. ", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset is annotated with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying depression-related classes. Each feature group is described by type, subtype, and one or more examples of words representing the feature subtype from a tweet, including lexical features, unigrams, syntactic features, parts of speech, emotion features, emoticons, demographic features, age and gender, sentiment features, polarity and subjectivity terms with strengths, personality traits, neuroticism, and LIWC Features, indicators of an individual's thoughts, feelings,", " The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated as evidence of depression, it is further annotated with one or more depressive symptoms. \n\n", " \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. ", " The dataset is annotated with 9,473 annotations for 9,300 tweets, with each tweet being annotated as no evidence of depression or evidence of depression, and further annotated with one or more depressive symptoms if there is evidence of depression. \n\n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. ", " The dataset is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated as evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets. ", " The dataset is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression, with further annotation of depressive symptoms for the class evidence of depression. \n\n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset is annotated to identify whether a tweet contains evidence of depression or not. If a tweet contains evidence of depression, it is further annotated with one or more depressive symptoms. Each depressive symptom is annotated with a specific subtype (e.g., depressed mood, disturbed sleep, or fatigue or loss of energy). \n", " The dataset is annotated as no evidence of depression or evidence of depression. ", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " Tweets are annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated for no evidence of depression or evidence of depression, and if there is evidence of depression, the dataset is further annotated with one or more depressive symptoms. ", " The dataset is annotated based on a hierarchical model of depression-related symptoms, ", " Each tweet is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. ", " The dataset is annotated as no evidence of depression or evidence of depression. If there is evidence of depression, then it is further annotated with one or more depressive symptoms. ", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets, with each tweet annotated as no evidence of depression or evidence of depression, and further annotated with one or more depressive symptoms for evidence of depression. \n", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated to indicate whether a tweet contains no evidence of depression or evidence of depression. ", " It is annotated with a hierarchical model of depression-related symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated as evidence of depression, it is further annotated with one or more depressive symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression, with each evidence of depression further annotated with one or more depressive symptoms. \n", " The dataset is annotated as either no evidence of depression or evidence of depression, with further annotations of one or more depressive symptoms if a tweet is annotated as evidence of depression. ", " The dataset is annotated to indicate whether a tweet contains evidence of depression or not, and if it does, it is further annotated with one or more depressive symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets, with each tweet being annotated as no evidence of depression or evidence of depression, and further annotated with one or more depressive symptoms if evidence of depression is present. \n", " The dataset contains 9,473 annotations for 9,300 tweets, where each tweet is annotated as either no evidence of depression or evidence of depression. If there is evidence of depression, the tweet is further annotated with one or more depressive symptoms. \n", " The dataset is annotated with a hierarchical model of depression-related symptoms. ", " Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated as evidence of depression, it is further annotated with one or more depressive symptoms. ", " It is annotated with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying depression-related classes. ", " The dataset is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms, such as depressed mood, disturbed sleep, or fatigue or loss of energy. \n", " It is annotated for depression or no evidence of depression. If there is evidence of depression, the dataset is further annotated for one or more depressive symptoms. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. \n", " Tweets are annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. ", " Each tweet is annotated as no evidence of depression or evidence of depression, and if annotated as evidence of depression, it is further annotated with one or more depressive symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if evidence of depression is present, it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated as either no evidence of depression or evidence of depression, with further annotation of one or more depressive symptoms for tweets that are annotated as evidence of depression. \n", " The dataset is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms., ", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if annotated as evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression, and if there is evidence of depression, it is further annotated with one or more depressive symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, with further annotation of one or more depressive symptoms if evidence of depression is present. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. \n", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, with evidence of depression further annotated with one or more depressive symptoms. \n\n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if evidence of depression, then it is further annotated with one or more depressive symptoms. \n\n", " The dataset is annotated as no evidence of depression or evidence of depression with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy. ", " The dataset is annotated to contain 9,473 annotations for 9,300 tweets. ", " The dataset is annotated based on a hierarchical model of depression-related symptoms, with each tweet being assigned one of the following labels: no evidence of depression or evidence of depression. If a tweet is labeled as evidence of depression, it is further annotated with one or more depressive symptoms. ", " The dataset contains 9,473 annotations for 9,300 tweets. ", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression and if a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. \n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, it is further annotated with one or more depressive symptoms. \n\n", " Each tweet is annotated as no evidence of depression or evidence of depression, and if a tweet is annotated evidence of depression, it is further annotated with one or more depressive symptoms. ", " It is annotated as no evidence of depression or evidence of depression, with the evidence of depression further annotated with one or more depressive symptoms. \n", " The dataset is annotated based on a hierarchical model of depression-related symptoms. ", " The dataset is annotated for 9,473 tweets, with each tweet binarized as the positive class (e.g., depressed mood = 1) or negative class (e.g., not depressed mood = 0). ", " The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms. ", " Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated as evidence of depression, it is further annotated with one or more depressive symptoms. \n", " The dataset is annotated as no evidence of depression or evidence of depression, with further annotation of one or more depressive symptoms if the tweet is annotated as evidence of depression. \n\n", " The dataset is annotated as no evidence of depression or evidence of depression, with further annotation of depressive symptoms. \n", " The dataset is annotated with 9,473 labels for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression, and if evidence of depression, then it is further annotated with one or more depressive symptoms. "], "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": [" Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, \n", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, \n", " Yes, \n", " yes, ", " Yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, \n", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, \n", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": [" Categorical cross-entropy with respect to the correct labels. \n", " The baseline used was the available training set, and also the error detection system by Rei2016, trained using the same FCE dataset. \n", " AdaDelta \n", " AdaDelta. \n", " AdaDelta \n", " AdaDelta \n", " AdaDelta.  \n", " Baseline performance using only manual annotation. ", " the original annotated dataset. \n", " Rei2016, ", " Rei2016 system. \n", " Rei2016 \n", " AdaDelta, ", " The baseline used was the available training set. \n", " AdaDelta BIBREF15. \n", " Rei2016's error detection system trained using the same FCE dataset. ", " AdaDelta. \n", " AdaDelta \n", " Baseline of Rei2016. \n", " baseline performance compared to previous results. ", " AdaDelta, ", " The baseline performance. \n", " The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset. ", " AdaDelta BIBREF15 \n", " AdaDelta. \n", " No baseline is explicitly mentioned but training error detection models using only the available training set is implied. \n\n", " AdaDelta. \n", " Rei2016. \n", " The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset. \n", " \n", " AdaDelta \n", " AdaDelta \n", " The baseline used was the FCE training set, but the actual baseline was the error detection system by Rei2016, which was trained using the same FCE dataset. However, this baseline was actually the performance of the error detection system trained using the available training set only, with no additional artificial data. \n\n", " None, unanswerable. \n", " AdaDelta. \n", " AdaDelta BIBREF15. ", " Adam delta was used as the baseline for calculating an adaptive learning rate during training. ", "  available training set. \n", " Categorical cross-entropy. \n", " The baseline performance. \n", " AdaDelta. \n", " the original annotated dataset \n", " The baseline performance compared to previous results was higher when using AdaDelta. \n\n", " The baseline used was the error detection system by Rei2016, trained using the same FCE dataset. \n\n", " The baseline used was the original annotated dataset. \n", " AdaDelta \n", " Rei2016 \n", " AdaDelta \n", " AdaDelta, ", " AdaDelta, \n", " AdaDelta \n\n", " AdaDelta \n", " Adam delta. \n", " the error detection system by Rei2016. \n", " No baseline is explicitly used, but Rei2016 system was trained using the same FCE dataset as a reference point for comparison. \n", " Rei2016. \n", " the model trained using the FCE dataset. \n", " Rei2016. \n", " AdaDelta, ", " Categorical cross-entropy with respect to the correct labels. \n", " AdaDelta. \n", " The baseline used was from Rei2016. \n", " Moses BIBREF7. \n", " The baseline used was Rei2016. \n", " AdaDelta BIBREF15. ", " \n", " Categorical cross-entropy with respect to the correct labels. ", " The baseline used was the Rei2016 system trained using the same FCE dataset. \n", " \n", " Categorical cross-entropy with respect to the correct labels. ", " AdaDelta. ", " Moses. \n", " AdaDelta. ", " The original annotated dataset. \n", " [Your answer here], ", " Moses. \n", " AdaDelta, ", " AdaDelta. \n", " Categorical cross-entropy with respect to the correct labels. \n", " AdaDelta. ", " Baseline performance. \n", " The baseline was the performance of the error detection system by Rei2016. \n", " Categorical cross-entropy \n", " AdaDelta \n", " AdaDelta \n\n", " The baseline used was the performance of the error detection system by Rei2016. \n", " Baseline performance. \n", " Baseline FCE. \n", " AdaDelta. \n", " The baseline used was Rei2016. ", " Rei2016 \n", " AdaDelta. ", " AdaDelta. \n", " AdaDelta, ", " [Insert your answer here] \n", " The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset. ", " AdaDelta. \n", " The baseline was the performance of the error detection system by Rei2016, trained using the same FCE dataset. ", " AdaDelta. \n", " AdaDelta \n"], "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": [" unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Book corpus and PPDB, ", " The book corpus. \n", " unanswerable. ", " Book corpus and Twitter, ", " They use a book corpus and a Paraphrase Database (PPDB). ", " The Twitter dataset, ", " The book corpus, ", " Unanswerable, ", " unanswerable, ", " Book corpus, ", " unanswerable, ", " They use Twitter dataset. ", " The book corpus and Paraphrase Database (PPDB),", " The book corpus., ", " Book corpus, ", " unanswerable, ", " None, ", " Book corpus, ", " unanswerable, ", " unanswerable, ", " Book corpus and Twitter, ", "  ", " Book corpus and PPDB, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " unanswerable, ", " The book corpus, ", " Book corpus, ", " Book corpus, ", " Book corpus and Paraphrase Database (PPDB), ", " unanswerable, ", " The book corpus, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " Book corpus and Twitter, ", " unanswerable. \n", " Twitter, ", " unanswerable, ", " Unanswerable, ", " The Book corpus. \n", " Book corpus, ", " The article does not provide a specific dataset used by the authors. \n", " unanswerable, \n", " Unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Book corpus. \n", "  The article does not explicitly state the dataset used by the authors, but it mentions several datasets and corpora used in the various models surveyed, including the book corpus, PPDB, and a paraphrase database.  However, the specific dataset they used in their own work is not mentioned.  They also mention that they collected distant-supervised tweets using positive and negative emoticons, but this is not a typical dataset used in machine learning. The models surveyed use a wide variety of datasets, including the book corpus, PPDB, and a paraphrase database.  \n", " unanswerable, ", " unanswerable, ", " The book corpus, ", " Book corpus, ", " Book corpus, ", " Book corpus and PPDB, ", " Book corpus and Twitter. ", " Book corpus, ", " Book corpus and Twitter, ", " unanswerable, ", " The Book Corpus and Twitter, ", " They use PPDB. \n", " The book corpus. ", " unanswerable, ", " unanswerable, ", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " unanswerable, ", " book corpus, ", " unanswerable, ", " book corpus, ", " unanswerable, ", " unanswerable, ", " unanswerable, \n", " The book corpus, ", " unanswerable, \n", " Book corpus, ", " unanswerable, ", " Book corpus, ", " unanswerable, ", " unanswerable, ", " Unanswerable, ", " unanswerable, ", " Book corpus, ", " The book corpus, \n", " The book corpus. ", " Book corpus and Twitter, ", " Book corpus, "], "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": [" A monolingual model, ", " Bayesian models for each language. ", " individual Bayesian models for each language \n", " A crosslingual latent variable, a monolingual model, and a multinomial distribution for the predicate-specific roles. \n", " A Bayesian model for each language. ", " A Bayesian model for each language, ", " Individual models for each language and crosslingual latent variables. \n", " A crosslingual latent variable and a Bayesian model for each language. ", " Bayesian models for each language, ", " Individual models for each language. ", " A monolingual Bayesian model for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents. \n", " Individual Bayesian models for each language. ", " Bayesian models for each language. \n", " individual Bayesian models for each language. ", " Bayesian models for each language. ", " Individual Bayesian models for each language, crosslingual latent variables to incorporate soft role agreement between aligned constituents. \n", " Bayesian models for each language. ", " A Dirichlet-multinomial model. \n", " Individual Bayesian models for each language. \n", " Bayesian models for each language. \n", " a Bayesian model for each language. ", " A crosslingual latent variable to incorporate soft role agreement between aligned constituents, and a Bayesian model for each language. ", " A Bayesian model for each language, ", " Bayesian models for each language. ", " Bayesian models for each language. ", " A set of Bayesian models for each language. ", " A Dirichlet-multinomial model with multinomial and binomial distributions. \n", " Bayesian models for each language. ", "  individual Bayesian models for each language and crosslingual latent variables. ", " individual Bayesian models for each language and crosslingual latent variables. \n", " Individual Bayesian models for each language and crosslingual latent variables. \n", " A monolingual Bayesian model for semantic role induction in one language. ", " A Dirichlet-multinomial conjugacy. ", " Individual Bayesian models for each language, ", " A Bayesian model for each language. \n", " individual Bayesian models for each language. \n", " a Bayesian model for each language. \n", " individual Bayesian models for each language, ", " Bayesian models for each language. ", " Bayesian models for each language. ", " Individual Bayesian models for each language. \n", " Individual Bayesian models for each language. ", " A Bayesian model for each language and a crosslingual latent variable. ", " A Dirichlet-multinomial conjugate model. ", " A Bayesian model. ", " An individual Bayesian model consists of a Dirichlet distribution and a multinomial distribution. \n", " [Your final answer here], ", " A Bayesian model for each language. \n", " [Your concise final answer here], ", " A Bayesian model for each language, ", " Bayesian models for each language. ", " A monolingual Bayesian model. ", " Individual Bayesian models for each language and crosslingual latent variables. ", " a Bayesian model for each language. \n", " Individual Bayesian models for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents. ", " A Bayesian model for each language. \n", " individual Bayesian models for each language and crosslingual latent variables. \n", " Individual Bayesian models for each language. ", " individual Bayesian models for each language, crosslingual latent variables to incorporate soft role agreement between aligned constituents. ", " individual Bayesian models for each language. ", " A Bayesian model for each language. \n", " Individual Bayesian models for each language. ", " A Bayesian model for each language. ", " Bayesian models for each language. ", " Individual Bayesian models for each language and crosslingual latent variables. ", " A crosslingual latent variable to incorporate soft role agreement between aligned constituents. ", " Bayesian models for each language. ", " individual Bayesian models for each language, with crosslingual latent variables to incorporate soft role agreement between aligned constituents. ", " A Dirichlet-multinomial conjugate model. ", " A Bayesian model for each language and cross-lingual latent variables. \n", " Bayesian models for each language. ", " Bayesian models for each language. ", " Individual Bayesian models for each language. \n", " Bayesian models for each language. ", " A monolingual Bayesian model with crosslingual latent variables to incorporate soft role agreement between aligned constituents. \n", " A Bayesian model for each language. ", " Bayesian models for each language. \n", " [Your concise final answer here], ", " A Bayesian model for each language. \n", " individual Bayesian models for each language. ", " A Bayesian model for each language. \n", " Bayesian models for each language. \n", " individual Bayesian models for each language, ", " Bayesian models for each language. ", " A Bayesian model for each language. \n", " Individual Bayesian models for each language. ", " An individual Bayesian model for each language. ", " A monolingual Bayesian model for each language. ", " A monolingual model for each language. ", " Individual Bayesian models for each language. \n", " A Bayesian model for each language. ", " A Bayesian model for each language. \n", " A Bayesian model for each language with individual parameters. ", " A Bayesian model for each language. \n", " A Bayesian model for each language, ", " [Your concise final answer here], ", " Individual Bayesian models for each language and cross-lingual latent variables. ", " The individual model consists of a monolingual Bayesian model for each language, ", " Bayesian models for each language. \n", " A Bayesian model for each language. \n"], "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": [" eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2, \n", " Eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT, \n", " Eight publicly available NER tasks used in BIBREF2, \n", " The eight publicly available NER tasks used in BioBERT. ", " The eight publicly available NER tasks used in BioBERT, \n", " The eight publicly available NER tasks used in BIBREF2, ", " unanswerable \n", " eight publicly available NER tasks, ", " eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BIBREF2, ", " Eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT, ", " Eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in the BioBERT paper. \n", " eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks. \n", " The eight publicly available NER tasks used in BioBERT. \n", " Eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2, \n", " Eight publicly available NER tasks. \n", " The eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks, ", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2, ", " The eight publicly available NER tasks used in BioBERT. \n", " Eight publicly available NER tasks. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT. \n", " eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks used in BioBERT. \n", " \n", " They evaluated on eight publicly available NER tasks used in BIBREF2, which are publicly available. ", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2, ", " The eight publicly available NER tasks used in BioBERTv1.0. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " the eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks used in BioBERT, \n", " eight publicly available NER tasks used in BIBREF2, ", " The eight publicly available NER tasks used in BIBREF2. ", " The eight publicly available NER tasks used in BioBERT, ", " The eight publicly available NER tasks used in BioBERTv1.0. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " unanswerable, ", " The eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks. \n", " They evaluated on eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT, \n", " The eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT. \n", " Eight publicly available NER tasks, ", " The eight publicly available NER tasks used in BIBREF2. \n", " the eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2, ", " The eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BioBERT. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in BioBERT. \n", " unanswerable \n", " The eight publicly available NER tasks used in BioBERT, ", " eight publicly available NER tasks used in BIBREF2, \n", " eight publicly available NER tasks used in BIBREF2, ", " The eight publicly available NER tasks used in BIBREF2, ", " The eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BioBERT, \n", " eight publicly available NER tasks used in BIBREF2. \n", " Eight publicly available NER tasks used in BioBERT. \n", " Eight publicly available NER tasks, ", " The eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BioBERT. \n", " The eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in the BIBREF2. \n", " Eight publicly available NER tasks used in BioBERT. \n", " Eight publicly available NER tasks used in BIBREF2. \n", " eight publicly available NER tasks used in BIBREF2. \n", " The eight publicly available NER tasks used in the original BioBERT paper. \n", " Eight publicly available NER tasks used in BIBREF2, \n", " The eight publicly available NER tasks used in BIBREF2. \n"], "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": [" the 2010 i2b2/VA dataset, ", " 2010 i2b2/VA, ", " 2010 i2b2/VA, ", " The 2010 i2b2/VA data. \n", " 2010 i2b2/VA BIBREF0 \n", " 2010 i2b2/VA \n", " The CE task in 2010 i2b2/VA, \n", " 2010 i2b2/VA \n", " the 2010 i2b2/VA data. \n", " 2010 i2b2/VA. \n", " 2010 i2b2/VA BIBREF0. \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " 2010 i2b2/VA \n", " 2010 i2b2/VA \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " 2010 i2b2/VA. \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " i2b2/VA, \n", " 2010 i2b2/VA. \n", " the CE task in 2010 i2b2/VA. \n", " 2010 i2b2/VA, \n", " The 2010 i2b2/VA data. \n", " 2010 i2b2/VA BIBREF0, ", " 2010 i2b2/VA \n", " i2b2/VA, \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA. \n", " 2010 i2b2/VA. \n", " the CE task in 2010 i2b2/VA, ", " 2010 i2b2/VA \n", " 2010 i2b2/VA, ", " i2b2 data. \n", " 2010 i2b2/VA. \n", " 2010 i2b2/VA, \n", " unanswerable \n", " 2010 i2b2/VA, \n", " i2b2/VA 2010, \n", " the 2010 i2b2/VA, ", " 2010 i2b2/VA BIBREF0 \n", " the 2010 i2b2/VA, \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " 2010 i2b2/VA \n", " the CE task in 2010 i2b2/VA, \n", " The CE task in 2010 i2b2/VA \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " From 2010 i2b2/VA, \n", " 2010 i2b2/VA, \n", " 2010 i2b2/VA, \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA \n", " from the 2010 i2b2/VA BIBREF0 challenge. \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA \n", " From the CE task in 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " from 2010 i2b2/VA, ", " the 2010 i2b2/VA, ", " the 2010 i2b2/VA BIBREF0 \n", " 2010 i2b2/VA \n", " 2010 i2b2/VA. \n", " 2010 i2b2/VA, \n", " The CE task in 2010 i2b2/VA \n", " 2010 i2b2/VA, \n", " 2010 i2b2/VA, \n", " 2010 i2b2/VA, \n", " 2010 i2b2/VA \n", " 2010 i2b2/VA BIBREF0 \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA. \n", " 2010 i2b2/VA \n", " 2010 i2b2/VA. \n", " the 2010 i2b2/VA, \n", " 2010 i2b2/VA, ", " from the CE task in 2010 i2b2/VA \n", " i2b2/VA, ", " i2b2/VA, ", " 2010 i2b2/VA \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " from the 2010 i2b2/VA BIBREF0. \n", " i2b2 data \n", " The 2010 i2b2/VA dataset, \n", " the 2010 i2b2/VA dataset, \n", " 2010 i2b2/VA \n", " i2b2/VA BIBREF0, \n", " 2010 i2b2/VA, \n", " 2010 i2b2/VA, \n", " the 2010 i2b2/VA, ", " 2010 i2b2/VA, \n", " 2010 i2b2/VA, ", " 2010 i2b2/VA \n", " from the 2010 i2b2/VA CE task, \n", " 2010 i2b2/VA, \n", " the 2010 i2b2/VA dataset. \n", " 2010 i2b2/VA, \n"], "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": [" TF-IDF features, ", " TF-IDF, ", " TF-IDF features. ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features. ", " TF-IDF features and LDA techniques, ", " TF-IDF features. \n", " TF-IDF features. \n", " TF-IDF features and LDA, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features. \n", " TF-IDF features and topic modeling. ", " TF-IDF features. \n", " [Your final answer here], ", " TF-IDF features, \n", " TF-IDF, ", " TF-IDF features, ", " TF-IDF features. ", " TF-IDF and LDA features, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF and LDA. \n", " TF-IDF features, ", " TF-IDF, ", " TF-IDF features, ", " TF-IDF and LDA features. \n", " TF-IDF features, ", " TF-IDF, ", " TF-IDF, ", " TF-IDF, ", " TF-IDF and LDA, ", " TF-IDF features, ", " TF-IDF, ", " TF-IDF, ", " TF-IDF features. \n", " TF-IDF features, ", " TF-IDF features. ", " TF-IDF features. \n", " TF-IDF features, ", " TF-IDF features. \n", " TF-IDF features. \n", " TF-IDF features. \n", " TF-IDF features, ", " TF-IDF features. \n", " TF-IDF and keyword extraction. ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features and LDA, ", " TF-IDF, ", " TF-IDF features, ", " TF-IDF features and LDA, ", " TF-IDF features. \n", " TF-IDF features. \n", " TF-IDF features. \n", " TF-IDF features, \n", " TF-IDF features, ", " TF-IDF features. ", " TF-IDF features and machine learning classifiers. \n", " TF-IDF, ", " TF-IDF features, ", " TF-IDF features. ", " TF-IDF and topic modeling, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF, ", " TF-IDF features and classification models. ", " TF-IDF and LDA features are used. \n", " TF-IDF features and LDA techniques, ", " TF-IDF, ", " TF-IDF features. \n", " TF-IDF features, ", " TF-IDF and topic modeling. ", " TF-IDF features. \n", " TF-IDF, ", " TF-IDF features, ", " TF-IDF features and LDA techniques. \n", " TF-IDF features, ", " TF-IDF and LDA, ", " TF-IDF features, ", " TF-IDF features and LDA, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF features. \n", " TF-IDF features and keywords extracted using LDA. \n", " TF-IDF features, ", " TF-IDF features, ", " TF-IDF, ", " TF-IDF features. ", " TF-IDF features, ", " TF-IDF features and LDA techniques, ", " TF-IDF features. \n", " TF-IDF features, ", " TF-IDF features. "], "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": [" English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English.\n", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " Yes \n", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English\n", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, ", " English, "], "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": [" The training data was translated into Spanish using the machine translation platform Apertium. ", " Using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. ", " By using machine translation platform Apertium. \n", " \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " The training data was translated into Spanish using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium, ", " By using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. ", " Using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " The training data was translated into Spanish from English using Apertium. \n", " Machine translation was used to translate the datasets. \n", " By using the machine translation platform Apertium, ", " The training data was translated using the machine translation platform Apertium, \n", " By using the machine translation platform Apertium. \n", " The training data was translated by machine translation platform Apertium, ", " Using the machine translation platform Apertium. ", " The training data was translated into Spanish using the machine translation platform Apertium. ", " Using the machine translation platform Apertium, \n", " The training data was translated using the machine translation platform Apertium. ", " The training data was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium, ", " The training data was translated using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium, ", " Using the machine translation platform Apertium. ", " Using the machine translation platform Apertium, \n", " \n", " The training data was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium, ", " It was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " \n", " The training data was translated using the machine translation platform Apertium. ", " The training data was translated using the machine translation platform Apertium. \n", " The training data was translated into Spanish using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium, ", " The training data was translated using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. ", " By using the machine translation platform Apertium. ", " By using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. ", " By using the machine translation platform Apertium. ", " The training data was translated using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. ", " Using machine translation platform Apertium. \n", " Using the machine translation platform Apertium, ", " The training data was translated using the machine translation platform Apertium, and the datasets from English were translated into Spanish, leaving the labels intact. \n", " Using the machine translation platform Apertium. \n", " It was translated using the machine translation platform Apertium. ", " Using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium, ", " By using machine translation platform Apertium. ", " By machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using machine translation platform Apertium. \n", " It was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " \n", " By machine translation platform Apertium, ", " Using the machine translation platform Apertium, ", " Using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " By machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium. ", " The training data was translated using the machine translation platform Apertium. \n", ", ", " The training data was translated using the machine translation platform Apertium. \n", " Using the machine translation platform Apertium, ", " The training data was translated using the machine translation platform Apertium, \n", " Using machine translation platform Apertium. \n", " The training data was translated using the machine translation platform Apertium, ", " The training data was translated using the machine translation platform Apertium. \n", " By using the machine translation platform Apertium, \n", " By using the machine translation platform Apertium. \n"], "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": [" Naive Bayes classifier. \n", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", " None, ", " Naive Bayes classifier. \n", " Multinomial Naive Bayes classifier, ", " \n", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes. \n", " They used multinomial Naive Bayes classifier. ", " A multinomial Naive Bayes classifier. \n", " Unanswerable ", " Multinomial Naive Bayes classifier. \n", " Naive Bayes classifier, \n", " Stacked generalization (or late fusion). \n", " Multinomial Naive Bayes classifier, ", " Naive Bayes classifier, ", " Stacked generalization.  \n", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", " Multinomial Naive Bayes classifier. \n", " Text-based, ", " Multinomial Naive Bayes classifier, ", " Naive Bayes classifier. ", " unanswerable, ", " Naive Bayes classifier, ", " Naive Bayes classifier, ", " Multinomial Naive Bayes. ", " multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes. \n", " None mentioned, ", " Multinomial Naive Bayes classifier. \n", " Naive Bayes classifier, ", " multimonial Naive Bayes, ", " Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", " multinomial Naive Bayes classifier. ", " Naive Bayes classifier, ", " unanswerable, ", " multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", " unanswerable, ", " Multinomial Naive Bayes, ", " Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", " multinomial Naive Bayes classifier. \n", " Multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier \n", " Multinomial Naive Bayes classifier. \n", " Multinomial Naive Bayes classifier. \n", " Multinomial Naive Bayes, ", " Naive Bayes classifier. \n", " multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier \n", " Multinomial Naive Bayes classifier, ", " The multinomial Naive Bayes classifier, ", " Naive Bayes classifier, ", " Naive Bayes classifier, ", " unanswerable, ", " Naive Bayes classifier. \n", " Multinomial Naive Bayes. \n", " Naive Bayes classifier.  \n", " Multinomial Naive Bayes classifier. \n", " multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes, ", " Multinomial Naive Bayes classifier, ", " Naive Bayes classifier. \n", " Naive Bayes classifier. \n", " Naive Bayes classifier, ", " multinomial Naive Bayes classifier, ", " None, ", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes classifier. \n", "  multinomial Naive Bayes classifier, ", " multinomial Naive Bayes classifier \n", " multinomial Naive Bayes classifier, ", " Stacked generalization. \n", " multilinear Naive Bayes classifier, ", " Naive Bayes classifier. \n", " multinomial Naive Bayes classifier, ", " Multinomial Naive Bayes. \n", " Multinomial Naive Bayes classifier. \n", " Multinomial Naive Bayes classifier, ", " multinomial Naive Bayes, ", " Naive Bayes classifier. \n", " Naive Bayes classifier, ", " multinomial Naive Bayes classifier, ", " multiclassifier, ", " Unanswerable, ", " multinomial Naive Bayes classifier \n", " unanswerable \n", " Multinomial Naive Bayes classifier, ", " They used multinomial Naive Bayes classifier. \n", " Multinomial Naive Bayes classifier. \n"], "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": [" By using the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. \n", " The political bias of sources is included in the model by referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased, and training on either left-biased or right-biased articles. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. \n", " The political bias of different sources is included by training the model separately on left-biased and right-biased sources. \n\n", " It is included by training the model on either left-biased or right-biased articles separately. \n", " It is included by referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By assigning a political bias label to different US outlets, following the procedure described in BIBREF2, and by training the model only on left-biased or right-biased outlets and testing on the entire set of sources. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " The political bias of different sources is included in the model by referring to the procedure proposed in BIBREF2 to label different outlets. ", " By assigning a political bias label to different US outlets following the procedure described in [2], and training only on left-biased (or right-biased) outlets and testing on the entire set of sources. \n", " The political bias of different sources is included in the model by referring to the procedure proposed in BIBREF2 to label different outlets. \n\n", " It is included by referring to the procedure proposed in BIBREF2. \n", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n\n", " By referring to the procedure proposed in BIBREF2, the political bias of different news sources is labelled and accounted for in the model. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " The political bias of different sources is included by using a Balanced Random Forest with default parameters. \n", " It is included by assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " It is accounted for by excluding particular sources that outweigh the others in terms of samples. \n\n", " By referring to the procedure proposed in BIBREF2 to label different outlets, \n", " Using the procedure proposed in BIBREF2 to label different outlets. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By assigning a political bias label to different US outlets and sources, and excluding or including particular sources in the training and testing sets. \n", " Using the procedure proposed in BIBREF2 to label different outlets. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By labeling different outlets as left-biased or right-biased, and then training the model on these biased sources and testing on the entire set of sources. \n\n", " By training only on left-biased or right-biased outlets of both disinformation and mainstream domains and testing on the entire set of sources. \n\n", " The political bias of different sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in [BIBREF2]. The procedure is also referred to for the Italian scenario. ", " It is taken into account by training only on left-biased or right-biased articles and testing on the entire set of sources. \n", " It is included in the model by referring to the procedure proposed in BIBREF2 to label different outlets. ", " By training on left-biased or right-biased outlets. \n", " The political bias of different sources is included by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in reference BIBREF2. \n", " According to political bias labels assigned to different US outlets, the model was trained and tested on left-biased and right-biased networks. ", " The political bias of different sources is assigned according to the procedure described in BIBREF2, and is taken into account by training the model only on left-biased or right-biased articles. \n", " The political bias of sources is included by assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " The political bias of different sources is included by referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. \n", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " By training only on left-biased or right-biased articles, ", " By referring to the procedure proposed in BIBREF2 to label different outlets, \n", " By accounting for the political bias of sources when training and testing the model, and by excluding two specific sources that outweigh the others in terms of samples to avoid over-fitting. ", " It is accounted for by labeling different outlets as left-biased or right-biased and training the classifier only on left-biased or right-biased articles. \n", " It is included by labeling different outlets with a political bias label following the procedure described in a reference paper. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. ", " By referring to the procedure proposed in BIBREF2 to label different outlets. ", " It is included by referring to the procedure proposed in reference [2] to label different outlets., ", " By assigning a political bias label to different US outlets, following the procedure described in BIBREF2, and performing classification experiments by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains. \n\n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By labeling different outlets, referring to the procedure proposed in BIBREF2. \n", " By using a Balanced Random Forest with default parameters, and by training on left-biased or right-biased networks and testing on the entire set of sources. \n", " It is included by referring to the procedure proposed in BIBREF2 to label different outlets. ", " By referring to the procedure proposed in BIBREF2 to label different outlets. ", " The political bias of different sources is included in the model by training only on left-biased or right-biased networks and testing on the entire set of sources, or by excluding particular sources that outweigh the others in terms of samples. \n", " The political bias of different sources is included in the model by referring to the procedure proposed in [2] to label different outlets, and training on left-biased or right-biased articles. \n", " The political bias of different sources is included in the model using the procedure proposed in [BIBREF2], where different outlets are assigned a political bias label, and classification experiments are performed by training only on left-biased or right-biased outlets and testing on the entire set of sources. ", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " Using the procedure proposed in reference [2], to label different outlets. \n", " By assigning a political bias label to different US outlets, and then training only on left-biased (or right-biased) outlets and testing on the entire set of sources. \n", " It is assigned using the procedure described in [2]. \n", " The political bias of sources is included by referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. ", " By referring to the procedure proposed in BIBREF2 to label different outlets, ", " It is included by training only on left-biased or right-biased outlets of both disinformation and mainstream domains and testing on the entire set of sources. \n\n", " It is included by referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. \n", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2, and then performing classification experiments by training only on left-biased (or right-biased) networks and testing on the entire set of sources. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets, ", " It is included by referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By assigning a political bias label to different US outlets, and training only on left-biased or right-biased networks and testing on the entire set of sources. ", " They are assigned a political bias label using a procedure described in BIBREF2. \n", " By assigning a political bias label to different US outlets, the authors used the procedure described in BIBREF2. ", " The political bias is accounted for in the model by training on left-biased or right-biased outlets only, while testing on the entire set of sources. \n\n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " The political bias of different sources is included by using a source-based approach and a classification strategy, where disinformation articles are obtained from websites known for producing low-credibility content and fact-checked by reputable journalists. \n", " The political bias of different sources is included in the model by assigning a political bias label to different US outlets (and therefore news articles) following the procedure described in [BIBREF2]., ", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By referring to the procedure proposed in [BIBREF2] to label different outlets. ", " Political bias of different sources is included by labeling different outlets based on their political bias, such as left-biased or right-biased, and then using these labels to train and test the model. ", " It is accounted for during training on left-biased or right-biased outlets, and also by training and testing on the entire set of sources. \n", " It is included by referring to the procedure proposed in BIBREF2 to label different outlets. \n", " It is included by referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By assigning a political bias label to different US outlets following the procedure described in [2]. \n", " Political bias of different sources is accounted for by assigning a label to different US outlets following the procedure described in [BIBREF2]. \n", " By assigning a political bias label to different US outlets and then accounting for it during classification experiments. \n", " By referring to the procedure proposed in BIBREF2, political biases inherent to different news sources are included. \n", " Following the procedure described in BIBREF2. \n", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " The political bias of different sources is included by referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By labeling different outlets with a political bias using a procedure proposed in a previous study. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets, ", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " The political bias of different sources is included by referring to the procedure proposed in BIBREF2 to label different outlets as left-biased or right-biased. \n", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " The political bias of different sources is included in the model by using a Balanced Random Forest with default parameters, and by training only on left-biased or right-biased networks and testing on the entire set of sources. \n", " By training only on left-biased or right-biased networks and testing on the entire set of sources. \n", " By assigning a political bias label to different US outlets following the procedure described in BIBREF2. \n", " It is accounted for, ", " By referring to the procedure proposed in BIBREF2 to label different outlets. \n", " By using the procedure described in BIBREF2 to label different outlets. \n"], "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": [" To enhance the decoder using BERT's contextualized representations, ", " To further improve the naturalness of the generated sequence, we cooperate reinforcement objective with the refine decoder, ", " This process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It concentrates on one word at a time, based on the source document as well as other words. ", " To feed a complete input sequence to BERT, ", " It allows the refine decoder to receive a more complete input sequence. \n", " It helps the decoder to refine words given other words of the summary, ", " It helps to make good use of BERT's context modeling ability, \n", " To provide a more complete input sequence for BERT. \n", " It enhances the decoder using BERT's contextualized representations. \n", " It helps the refine decoder predict the refined word given other words of the summary, ", " to enhance the decoder using BERT's contextualized representations, ", " It helps the refine decoder learn to predict words given all the other ground-truth words of summary. \n\n", " It allows the model to receive a more complete input sequence which is consistent with the pre-training process of the language model. \n", " It is helpful because it allows the BERT model to generate context vectors. \n", " It is helpful to make use of BERT's context modeling ability. \n", " It helps to make use of BERT's context modeling ability. \n", " It provides a more complete input sequence which is consistent with BERT's pre-training processes. \n", " It helps to enhance the decoder using BERT's contextualized representations, ", " It helps to refine the generated sequence using BERT's context vectors. \n", " To make good use of BERT's context modeling ability and to help the decoder to learn to generate more fluent and natural sequences. \n", " It allows the refine decoder to utilize BERT's context modeling ability and predict refined word one-by-one. \n", " It allows the refine decoder to feed the BERT model with a complete input sequence that is consistent with its pre-training process. \n", " It provides a more complete input sequence for the contextual language model. \n", " It enables the BERT model to generate more accurate context vectors. \n", " to enhance the decoder using BERT's contextualized representations. \n\n", " To provide a more complete input sequence which is consistent with BERT's pre-training processes. \n", " It enhances the decoder using BERT's contextualized representations. \n", " It allows BERT to generate context vectors, \n", " To enhance the decoder using BERT's contextualized representations, \n", " It helps the decoder utilize BERT's ability to generate high-quality context vectors. \n", " It helps refine the content of the summary, ", " It helps the refine decoder generate high-quality context vectors, ", " By masking words in the decoder, we can make good use of BERT's context modeling ability. \n", " It helps to enhance the decoder using BERT's contextualized representations. \n", " The masking words allows the decoder to receive more complete context information when using the pre-trained language model BERT in the refine process, ", " It helps refine the content of the summary. \n", " It helps the model generate more fluent and natural sequences. \n", " To enhance the decoder using BERT's contextualized representations, ", " The refine decoder helps by receiving a generated summary draft, masking each word, and predicting a refined summary word, ", " It helps to generate refined words using a BERT module, which can produce better context vectors. \n", " To further improve the naturalness of the generated sequence, ", " It provides a more complete input sequence which is consistent with BERT's pre-training processes. \n", " It provides a more complete input sequence consistent with BERT's pre-training processes, \n", " To enhance the decoder using BERT's contextualized representations, \n", " To enhance the decoder using BERT's contextualized representations, \n", " To generate more fluent and natural sequences, ", " To enhance the decoder using BERT's contextualized representations. \n", " To generate more fluent and natural sequences, ", " It provides a more complete input sequence which is consistent with BERT's pre-training processes, ", " It allows the decoder to generate each word considering both context sides, ", " It helps the model to generate context-aware word representations. \n", " It allows the decoder to use BERT's context modeling ability. \n", " To utilize BERT's ability to generate high quality context vectors. \n", " It helps the decoder to utilize BERT's context modeling ability. \n", " It is helpful for the refine decoder to learn to generate refined summaries. \n\n", " It allows the refine decoder to predict a refined summary word using BERT's contextualized representations. \n", " To enhance the decoder using BERT's contextualized representations. \n", " To enhance the decoder using BERT's contextualized representations, ", " It provides a more complete input sequence which is consistent with pre-training processes of contextual language models, ", " It enhances the decoder using BERT's contextualized representations, \n", " To enhance the decoder using BERT's contextualized representations, \n", " It helps to enhance the decoder using BERT's contextualized representations. \n", " Because it provides a more complete input sequence which is consistent with the pre-training process of contextualized embeddings. \n", " It provides a more complete input sequence for the BERT module to generate context vectors. \n", " To enhance the decoder using BERT's contextualized representations. \n", " To enhance the decoder using BERT's contextualized representations, ", " To enhance the decoder using BERT's contextualized representations. \n", " To utilize BERT's contextualized representations. \n", " It allows the refine decoder to feed a complete input sequence to BERT. \n", " To generate more fluent and natural sequences. \n", " To enhance the decoder using BERT's contextualized representations, ", " It provides a more complete input sequence which is consistent with BERT's pre-training processes, ", " To enhance the decoder using BERT's contextualized representations, ", " It allows the model to make use of BERT's contextualized representations. \n", " Masking words in the decoder helps to feed BERT with a complete input sequence. \n", " It enhances the decoder using BERT's contextualized representations. \n", " This is helpful because this process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It concentrates on one word at a time, based on the source document as well as other words., ", " To enhance the decoder using BERT's contextualized representations, ", " Because this process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It concentrates on one word at a time, based on the source document as well as other words. \n", " To enhance the decoder using BERT's contextualized representations. \n", " This process helps the decoder to generate context-aware word representations. \n", " It helps the refine decoder learn to predict words given all the other ground-truth words of summary. \n", " It helps the decoder to generate context-aware word representations, especially during the inference process. \n", " To enhance the decoder using BERT's contextualized representations, ", " It provides a more complete input sequence which is consistent with BERT's pre-training processes. \n\n", " It allows the decoder to receive a more complete input sequence. \n", " To further improve the naturalness of the generated sequence. \n", " It is helpful to allow the refine decoder to edit the draft and make use of BERT's contextualized representations. \n", " To make good use of BERT's context modeling ability. \n", " To enhance the decoder using BERT's contextualized representations, ", " To enhance the decoder using BERT's contextualized representations. \n", " To enhance the decoder using BERT's contextualized representations. \n", " It is helpful to feed BERT the generated summary with masked words and predict the refined words, ", " It helps the decoder to make use of BERT's context modeling ability. \n", " It allows the model to refine the content of the summary. \n", " To utilize BERT's context modeling ability and generate high-quality context vectors. \n", " To enhance the decoder using BERT's contextualized representations, \n", " To feed a more complete input sequence to BERT. \n\n", " It allows BERT to predict refined words by conditioning on both sides' context information. \n", " It is helpful because this process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It concentrates on one word at a time, based on the source document as well as other words, and by using the ability of the contextual language model the decoder can generate more fluent and natural sequences. \n"], "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": [" Chinese, ", ", ", " English and Chinese. \n", " Chinese, ", " Chinese datasets, ", " English and Chinese, ", " \n", "  unanswerable, ", " Chinese datasets. \n", " English and Chinese, ", " unanswerable, ", " English and Chinese, ", " unanswerable, ", " Chinese PTB, ", " English and Chinese, ", " English and Chinese. \n", " Chinese, \n", " unanswerable, ", "  Chinese. \n", " Chinese, ", " unanswerable, \n", " English and Chinese, ", " unanswerable, ", " unanswerable, ", " unanswerable \n", " Chinese, ", " English and Chinese, ", " Chinese datasets were used. \n", " Chinese datasets were used, ", " Chinese datasets were used. \n", " \n", " Chinese, ", " Chinese datasets. \n", " \n", "  Chinese datasets, ", " English and Chinese, ", " English and Chinese, ", " Chinese datasets were used, but specific names were not mentioned. \n", " Chinese Penn Treebank and possibly other Chinese datasets, ", " Chinese datasets were used, ", " Chinese, \n", " , ", " English and Chinese, ", " Chinese, \n", " Chinese datasets were used in the experiments. \n", " Chinese \n", " Chinese Penn Treebank (CTB) is likely used \n", " unanswerable, ", " The Penn Chinese Treebank (CTB5). \n", " Chinese datasets were used. ", " English and Chinese datasets. \n", " Chinese datasets used included, \n", " \n", " Chinese, \n", " unanswerable, \n", "  Chinese, ", " Chinese datasets used are BIES and PENNT. \n", " Chinese datasets, ", " Chinese Penn Treebank. \n", " Chinese, ", " English and Chinese. \n", " Chinese datasets, ", " unanswerable, ", " Chinese, ", " unanswerable \n", " Chinese datasets were used. \n", " Chinese datasets were used. \n", " Chinese. \n", " unanswerable, ", " The Chinese Penn Treebank and Benchmarks for Parsing and Semantics, \n", ", ", " Chinese \n", "  Chinese, ", " Chinese, ", " Chinese Penn Treebank \n", " Chinese, ", " Chinese; ", " unanswerable, ", " \n", " Chinese, ", " English and Chinese, ", " unanswerable, ", " English and Chinese, ", " English and Chinese \n", " Chinese datasets were used. \n", " Chinese, ", " Chinese datasets used in the paper include PTB and another dataset from Yonatan Belinkov and Shay Cohen. ", " English and Chinese. ", " Chinese, ", " Chinese, ", " chinese. \n", " Chinese datasets. \n", "  Chinese datasets, ", " English and Chinese, ", " unanswerable, ", " Chinese PTB, ", " Chinese Penn Treebank, \n", " unanswerable, ", " unanswerable \n", " unanswerable, "], "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": [" Baseline, ", " A simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. ", " Simple logistic regression classifier, ", " A very simple logistic regression classifier with default parameters. \n", " The baseline for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. ", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " Random selection of one of the 18 techniques for FLC task, and a simple logistic regression classifier with default parameters for SLC task. \n", " The baseline system for the SLC task was a very simple logistic regression classifier with default parameters, while the baseline for the FLC task generates spans and selects one of the 18 techniques randomly. \n", " A very simple logistic regression classifier with default parameters. ", " A very simple logistic regression classifier. \n", " A simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " A very simple logistic regression classifier. \n", " [Your concise final answer here], ", " Logistic regression with default parameters, ", " A simple logistic regression classifier, ", " A very simple logistic regression classifier with default parameters, where the input instances were represented by a single feature: the length of the sentence. ", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. \n", " The baseline system for the SLC task was a simple logistic regression classifier with default parameters, and for the FLC task, it generated spans and selected one of the 18 techniques randomly. ", " The baseline for the SLC task was a simple logistic regression classifier with default parameters. \n", " Unanswerable \n", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. ", " A very simple logistic regression classifier with default parameters. ", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. ", " Yes, it is a logistic regression classifier, ", " The baseline was a simple logistic regression classifier for the SLC task and a model that generates spans and selects one of the 18 techniques randomly for the FLC task. \n", " A very simple logistic regression classifier with default parameters, \n", " A simple logistic regression classifier with default parameters and a random baseline, ", " Simple logistic regression classifier, ", " [Your concise final answer here], ", " A very simple logistic regression classifier with default parameters., ", " Logistic regression classifier with default parameters, ", " A very simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence. ", " A very simple logistic regression classifier with default parameters. \n", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. ", " Very simple logistic regression classifier with default parameters. \n", " Random classifier with one feature: the length of the sentence, ", " A very simple logistic regression classifier with default parameters. ", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. \n", " The baseline for the FLC task was a random baseline that generates spans and selects one of the 18 techniques randomly, while the baseline for the SLC task was a very simple logistic regression classifier with default parameters. \n\n", " The baseline was a simple logistic regression classifier for the SLC task and a model that generates spans and selects one of the 18 techniques randomly for the FLC task. ", " Simple logistic regression classifier, ", " a very simple logistic regression classifier with default parameters. \n", " Very simple logistic regression classifier. \n", " A simple logistic regression classifier. ", " A simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence, ", " A simple logistic regression classifier with default parameters, ", " A very simple logistic regression classifier, ", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence,  and a random baseline. ", " Logistic regression with default parameters \n", " A very simple logistic regression classifier with default parameters. ", " A simple logistic regression classifier with default parameters. ", " A very simple logistic regression classifier with default parameters. \n", " A very simple logistic regression classifier with default parameters. \n", " A very simple logistic regression classifier with default parameters. \n", " Simple logistic regression classifier with default parameters. \n", " A simple logistic regression classifier with default parameters. \n", " The baseline for the SLC task was a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. For the FLC task, the baseline generated spans and selected one of the 18 techniques randomly. ", " A simple logistic regression classifier, ", " A very simple logistic regression classifier with default parameters. ", " The baseline for the SLC task was a very simple logistic regression classifier with default parameters. The baseline for the FLC task generated spans and selected one of the 18 techniques randomly. \n", " The baseline system for the SLC task was a very simple logistic regression classifier with default parameters. ", " Random baseline, ", " A simple logistic regression classifier with default parameters. \n", " A very simple logistic regression classifier with default parameters, ", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " Simple logistic regression classifier with default parameters. \n", " Simple logistic regression classifier with default parameters. \n", " Unanswerable \n", " A very simple logistic regression classifier. \n", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. ", " Simple logistic regression classifier with default parameters, ", " A very simple logistic regression classifier with default parameters, ", " A very simple logistic regression classifier with default parameters and for the FLC task, it generates spans and selects one of the 18 techniques randomly. \n", " The baseline system for the sentence-level classification task was a simple logistic regression classifier. ", " The baseline system for the SLC task was a very simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence.  The baseline model for the FLC task generated spans and selected one of the 18 techniques randomly. The baseline system for the SLC task performed with an F$_1$ value of 0.69 and 0.64 on the test and development sets, respectively.  The baseline model for the FLC task performed with an F$_1$ value", " The baseline system for the SLC task was a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.  ", " The baseline for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. ", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence, ", " The baseline system for the sentence-level classification task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. ", " Simple logistic regression classifier, ", " very simple logistic regression classifier with default parameters, a simple random baseline \n", " A simple logistic regression classifier with default parameters, ", " a very simple logistic regression classifier with default parameters. \n", " [Your concise final answer here], ", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. ", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " A very simple logistic regression classifier with default parameters. \n", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. ", " A simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence, and the baseline for the FLC task generates spans and selects one of the 18 techniques randomly. \n", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. ", " A very simple logistic regression classifier with default parameters, ", " A simple logistic regression classifier with default parameters. \n", " A very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. \n", " The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. ", " A logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence, and a simple random baseline that generates spans and selects one of the 18 techniques randomly., ", " The baseline for the SLC task was a simple logistic regression classifier with default parameters. The performance of this baseline on the SLC task is shown in Tables TABREF33 and TABREF34. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables TABREF36 and TABREF41., ", " A logistic regression model fed with a single feature, the length of the sentence, for the SLC task and a random model for the FLC task. ", " A simple logistic regression classifier with default parameters was used for the SLC task as the baseline system, and a baseline for the FLC task generates spans and selects one of the 18 techniques randomly. \n"], "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": [" The internet and ancient Chinese history records. ", " The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, which were collected from the internet. \n\n", " Internet, \n", " The ancient Chinese data used in this study come from ancient Chinese history records in several dynasties (about 1000BC-200BC), and articles written by celebrities of that era. \n", ", ", " From the internet and ancient Chinese history records. \n", " [Your concise final answer here], ", " The ancient Chinese data comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n\n", " The internet, \n", " From the internet, ", " The internet, ", ", ", " From the internet and ancient Chinese history records in several dynasties, ", " The ancient Chinese data comes from ancient Chinese history records in several dynasties and articles written by celebrities of that era. \n", " From the internet, ancient Chinese history records in several dynasties, and articles written by celebrities of that era. \n", " The internet and ancient Chinese history records. \n", " From the internet, including ancient Chinese history records in several dynasties and articles written by celebrities. ", " [Your concise final answer here], ", " The ancient Chinese data comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese data used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese data mainly come from ancient Chinese history records in several dynasties and articles written by celebrities of that era. \n", " From the internet and ancient Chinese history records in several dynasties. ", " The ancient Chinese data comes from ancient Chinese history records in several dynasties and articles written by celebrities. \n", " From the internet and ancient Chinese history records in several dynasties, ", " The internet and ancient Chinese history records, ", " The internet, including ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " From ancient Chinese history records in several dynasties and articles written by celebrities, ", " The ancient Chinese data used in the study come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, mainly collected from the internet. \n", " The ancient Chinese data used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n\n", " The internet and historical records. \n", " Internet and ancient Chinese history records. \n", " [Your concise final answer here], ", " Ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n\n", " The ancient Chinese data mainly comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese dataset comes from 1.7K bilingual ancient-modern Chinese articles collected from the internet. \n", " The internet and ancient Chinese history records. \n", " The internet and ancient Chinese history records in several dynasties, ", " From the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese data comes from ancient Chinese history records in several dynasties and articles written by celebrities of that era. \n", " The ancient Chinese data mainly come from ancient Chinese history records in several dynasties (about 1000BC-200BC). \n", " From ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese data used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, which were collected from the internet. \n", " The internet. \n", " Ancient Chinese history records in several dynasties and articles written by celebrities. \n", " The internet, \n", " From the internet. ", " From the internet. \n", " The ancient Chinese data used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The internet, ", " The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, and internet resources. \n", " The internet, \n", " From the internet, specifically from ancient Chinese history records in several dynasties and articles written by celebrities of that era. \n", " The ancient Chinese dataset comes from the internet, and particularly from ancient Chinese history records in several dynasties and articles written by celebrities of that era. \n", " The internet. \n", " ancient Chinese history records in several dynasties (about 1000BC-200BC), and articles written by celebrities of that era. \n", " The ancient Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties and articles written by celebrities of that era. \n", " The ancient Chinese data mainly come from ancient Chinese history records in several dynasties. Additionally, some articles written by celebrities of that era were used. \n", " Internet,  \n", " ancient Chinese history records in several dynasties, ", " The internet, historical records and articles written by celebrities in several dynasties (about 1000BC-200BC), ", " The ancient Chinese data comes from ancient Chinese history records in several dynasties. \n", " From ancient Chinese history records in several dynasties, \n", " The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", ", ", " The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, which are collected from the internet. \n", " From the internet, specifically a large part of the ancient Chinese data used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. \n", " The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " From the internet, ", " The internet, Reasons:  According to the article, to build the large ancient-modern Chinese dataset, the authors collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data they used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.  They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials. This indicates that the internet is a source of the ancient Chinese dataset. Furthermore, the", " The ancient Chinese dataset comes from the internet, including ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. ", " The internet and ancient Chinese history records in several dynasties. \n", " The internet and ancient Chinese history records. \n", " From the internet. \n", " The internet, ancient Chinese history records in several dynasties, and articles written by celebrities of that era. \n", ", ", " From the internet and ancient Chinese history records. \n", " The ancient Chinese data come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese data mainly come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. Additionally, the data is also collected from the internet. \n\n", " It comes from the internet and ancient Chinese history records in several dynasties, ", " It comes from the internet, and specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " From the internet. \n", " The ancient Chinese dataset comes from the internet. \n", " The ancient Chinese data used in this paper come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese dataset comes from the internet, ", " From ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The ancient Chinese data comes from ancient Chinese history records in several dynasties, and articles written by celebrities of that era. \n", " The internet and ancient Chinese history records, ", " The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " From the internet and ancient Chinese history records in several dynasties (about 1000BC-200BC). \n", " From the internet, \n", " The ancient Chinese data used in the paper comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era, which were collected from the internet. \n", " Internet. \n", " The ancient Chinese dataset comes from the internet. ", " From the internet, particularly ancient Chinese history records and articles written by celebrities. ", " The internet, ", " The internet and ancient Chinese history records in several dynasties. \n", " The ancient Chinese data mainly come from ancient Chinese history records in several dynasties. \n", " The ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n", " The internet. \n"], "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": [" Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe. ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " [Your concise final answer here], ", ", ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe. \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, \n", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, ", " Galatasaray and Fenerbahçe, "], "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": [" 4, ", " 3, ", " 3, ", " unanswerable, ", " The UTCNN model has more than 2 layers. \n", " 3, ", " 3, ", " unanswerable, ", ", ", " The UTCNN model has 3 layers, including the user and topic vector embeddings, the document representation, and the fully connected network, ", " 2, ", " The UTCNN model has more than two layers. \n", " 3, ", " unanswerable, ", " 5, ", " 2, ", " \n", " 2, ", " 3, ", " unanswerable, ", " 5, ", " 4, ", " 3, ", " 3, ", " unanswerable, ", " 1, ", " Unanswerable, ", " 2, ", " ", " The article does not specify, ", " unanswerable, ", "  ", " 3, ", " 2, ", " 3 layers in the convolutional layer, ", " 3, ", " No specific number is given, ", " 3, ", " 5 layers, ", " Unanswerable, ", " 2, ", ", ", " 3, ", " unanswerable, ", " 2 layers and a fully connected network, ", " 4, ", " unanswerable\n", " unanswerable, ", " 3 layers, ", " unanswerable \n", " Unanswerable, ", " 5, ", " 5, ", " unanswerable, ", " Not clearly stated, ", " 4, ", " The number of layers in the UTCNN model is not specified, ", " 5, ", " 3, ", " ", " The answer cannot be determined. ", " \n", " 5, ", " 3, ", " 3, ", " 5, ", " 3, ", ", ", " 5, ", " Unanswerable, ", " Not directly answered, ", " unanswerable, ", " unanswerable, \n", " 3, ", " 4, ", " unanswerable, ", " unanswerable, ", " 4, ", " Unanswerable, ", " Unanswerable, ", " 9, ", " 1, ", " 2 layers of pooling, ", " unanswerable, ", " 3, ", " unanswerable, ", " 2 layers, ", " \n", " 4, ", " 4, ", " 4, ", " unanswerable, ", " 4, ", " 4, ", " 3, ", " 4, ", " 7 layers, ", " 5, ", " 5 layers, ", " 2, "], "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": [" European network of nature protected sites Natura 2000 dataset, 180 categorical features (CORINE land cover classes at level 1, 2, and 3, and 116 soil types), nine real-valued numerical features (latitude, longitude, elevation, population, and five climate related features), ScenicOrNot website crowdsourced dataset for landscape beauty.  and Flickr tags. \n", " European network of nature protected sites Natura 2000 dataset. \n", " European dataset, \n", " European Flickr photo dataset. \n", " European network of nature protected sites Natura 2000 dataset and crowdsourced dataset from the ScenicOrNot website. \n", " structured datasets, including CORINE land cover classes and SoilGrids. \n", " structured environmental datasets and Flickr tags, \n", " structured environmental datasets, ", " Natura 2000 dataset, ", " The dataset used is the Natura 2000 dataset. \n", " structured datasets and Natura 2000 dataset. \n", " European dataset, ", " Natura 2000 dataset and European network of nature protected sites dataset. \n", " European network of nature protected sites Natura 2000 dataset. \n", " European Natura 2000 dataset, the ScenicOrNot website dataset, Flickr dataset, SoilGrids dataset, CORINE dataset. \n", " structured environmental datasets. \n", " The Natura 2000 dataset is used in the paper. \n", " European network of nature protected sites Natura 2000 dataset. \n", " The dataset used in this paper includes nine (real-valued) numerical features and 180 categorical features. ", " The ScenicOrNot dataset and several structured datasets. \n", " European network of nature protected sites Natura 2000 dataset, European Flickr photos with coordinates dataset, crowdsourced dataset from the ScenicOrNot website, CORINE land cover classes, SoilGrids. \n", " structured environmental dataset, ", " structured environmental datasets. \n", " the Natura 2000 dataset, \n", " Structured environmental dataset. \n", " The datasets used include nine numerical features, such as latitude, longitude, elevation, population, and five climate related features, as well as 180 categorical features. \n", " European network of nature protected sites Natura 2000 dataset, along with Flickr and other environmental datasets. \n\n", " European network of nature protected sites Natura 2000 dataset, ", " the European network of nature protected sites Natura 2000 dataset. \n", " The Natura 2000 dataset and the ScenicOrNot dataset. \n", " European network of nature protected sites Natura 2000 dataset. \n", " unanswerable, \n", " the dataset includes nine (real-valued) numerical features, and 180 categorical features. \n", " A variety of datasets including Flickr tags, 9 real-valued numerical features, and 180 categorical features. \n", " structured environmental datasets. \n", " Various datasets, ", " Various environmental datasets. \n", " the Natura 2000 dataset and the ScenicOrNot dataset are used. \n", " a dataset from Flickr, which includes 70 million photos with coordinates in Europe, and some structured environmental datasets. \n", " Natura 2000 dataset \n", " Natura 2000 dataset, ", " The ScenicOrNot dataset and Natura 2000 dataset, among others, ", " The dataset used is a collection of 70 million Flickr photos with coordinates in Europe. \n", " Various structured datasets, ", " unanswerable, ", " European network of nature protected sites Natura 2000 dataset, ScenicOrNot website, and Flickr. \n", " European network of nature protected sites Natura 2000 dataset, \n", " The dataset is not explicitly mentioned in the paper. \n", " structured datasets, including numerical and categorical features. \n", " The European network of nature protected sites Natura 2000 dataset. \n", " European dataset, ", " Natura 2000 dataset and SoilGrids dataset. \n", " European network of nature protected sites Natura 2000 dataset, ", " the Natura 2000 dataset \n", " European nature protected sites Natura 2000 dataset. \n", " European network of nature protected sites Natura 2000 dataset. \n", " European dataset. \n", " Structured environmental datasets. \n", " structured environmental datasets. \n", " structured scientific information and Flickr tag dataset \n", " 70 million Flickr photos with coordinates in Europe, ", " European network of nature protected sites Natura 2000 dataset and SoilGrids, ", " European dataset and Flickr dataset, ", " The European network of nature protected sites Natura 2000 dataset and  the ScenicOrNot website as ground truth for scenicness evaluation. Also,  the SoilGrids dataset is used for soil type and CORINE dataset for land cover classes. \n", " the datasets used in this paper are the Natura 2000 dataset, the same datasets as BIBREF7, and the crowdsourced dataset from the ScenicOrNot website. \n", " European network of nature protected sites Natura 2000 dataset. \n", " a European dataset. \n", " European dataset. ", " structured datasets (e.g., average temperature, average precipitation, CORINE land cover classes at level 1, level 2, and level 3, and 116 soil types) and the ScenicOrNot dataset from the crowdsourced website for scenicness evaluation. \n", " European network of nature protected sites Natura 2000 dataset, ", " the European network of nature protected sites Natura 2000 dataset and the ScenicOrNot website as ground truth. \n", " structured environmental datasets. \n", " Various datasets, ", " the Natura 2000 dataset \n", " structured scientific datasets, ", " European network of nature protected sites Natura 2000 dataset,", " The Natura 2000 dataset, ", " 70 million Flickr photos with coordinates in Europe, ", " structured datasets from Europe. \n", " European dataset, \n", " The European network of nature protected sites Natura 2000 dataset, ScenicOrNot website crowdsourced dataset, and SoilGrids. \n", " The European network of nature protected sites Natura 2000 dataset is used in this paper, among other datasets. \n", " various structured datasets, including nine numerical features, 180 categorical features, and the Natura 2000 dataset. \n", " The Natura 2000 dataset, along with other structured environmental datasets such as nine numerical features and 180 categorical features, is used in this paper. \n\n", " The dataset used in this paper includes 70 million Flickr photos with coordinates in Europe, and several structured environmental datasets. \n", " European network of nature protected sites Natura 2000 dataset. \n", " European network of nature protected sites Natura 2000 dataset and SoilGrids dataset. \n", " The European network of nature protected sites Natura 2000 dataset. \n", " European network of nature protected sites Natura 2000 dataset. \n", " European Flickr photos with 70 million data points. \n", " European network of nature protected sites Natura 2000 dataset, and crowdsourced dataset from the ScenicOrNot website. \n", " European network of nature protected sites Natura 2000 dataset, Flickr photos with coordinates in Europe, the ScenicOrNot dataset, CORINE land cover classes, and SoilGrids. \n", " European network of nature protected sites Natura 2000 dataset. \n", " European dataset. \n", " Natura 2000 dataset and Flickr. ", " A variety of datasets including numerical and categorical features. \n", " European nature protected sites Natura 2000 dataset. \n", " various datasets. \n", " various datasets including Flickr, Natura 2000 dataset, SoilGrids. ", " The Natura 2000 dataset and the European network of nature protected sites.  In addition, the ScenicOrNot website dataset is used to predict people's subjective opinions of landscape beauty in Britain. "], "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": [" The metrics used to establish this are coverage, average Matthews correlation coefficient, and average positive F1 score.  ", " avg. MCC, avg. +ve F1 score, and Coverage, ", " Average MCC, avg. +ve F1 score, Coverage.\n\n", " ", " Matthews correlation coefficient (MCC), average MCC, average +ve F1 score, coverage, F1 score.\n", " Avg. MCC and avg. +ve F1 score, ", " Average MCC, ", " Average Matthews correlation coefficient (MCC) and average +ve F1 score. \n", " avg. MCC and avg. +ve F1 score, ", " MCC and average positive F1 score, ", " The predictive performance metrics Avg. MCC, avg. +ve F1 score, and the strategy formulation metric Coverage., ", " Average MCC and avg. +ve F1 score.\n", " Avg. MCC and avg. +ve F1 score, ", " Lifelong learning metrics are used, such as Matthews correlation coefficient (MCC), and +ve F1 score. \n", " avg. MCC and avg. +ve F1 score, ", " Average MCC (Matthews correlation coefficient) and average +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " Coverage and Average MCC and Average +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " Predictive performance metrics, \n", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " 3) Average MCC and average +ve F1 score\n", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score.\n", " MCC and Avg. +ve F1 score, Coverage( LiLi ), \n", " ", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC, avg. +ve F1 score, and coverage, ", " In the paper, two major metrics are used to evaluate the chatbots' knowledge: one is the fraction of total query data instances for which LiLi has successfully formulated strategies that lead to winning, and the other is the Average Matthews correlation coefficient (MCC) and average positive F1 score. In the paper, it is stated that LiLi achieves MCC = 0.97 on WordNet and MCC = 0.47 on Freebase. It is also stated that LiLi achieves an average positive F1 score of 0.97 on WordNet and 0.47 on Freebase. Therefore, LiLi is said to", " Avg. MCC and avg. +ve F1 score,  ", " Average MCC and average +ve F1 score, Coverage, and Average +ve F1 score.\n", " Avg. MCC and avg. +ve F1 score, ", " In the paper, 2 metrics are used to test and evaluate the proposed approach, coverage, average MCC and average +ve F1 score. Coverage refers to the fraction of total query data instances, for which the chatbot has successfully formulated strategies that lead to winning. This metric is used to evaluate the chatbot’s ability to formulate strategies and learn from user interactions. MCC (Matthews Correlation Coefficient) is a measure of the quality of binary classification, and it is used to evaluate the chatbot’s ability to predict whether a relation exists between two entities. The average +ve F1 score is a measure of the chatbot", " Avg. MCC and avg. +ve F1 score, ", " Average MCC and Average +ve F1 score, \n", " MCC and F1 score, \n", " Average Matthews correlation coefficient (MCC) and average +ve F1 score,  ", " Avg. MCC and avg. +ve F1 score, ", " Average MCC and average positive F1 score, ", " avg. MCC, avg. +ve F1 score, and Coverage, ", " ", " Coverage, Average MCC (Matthews correlation coefficient), and Average positive F1 score, ", " Average MCC and average +ve F1 score, ", " ", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, Coverage, \n\n", " Avg. MCC, avg. +ve F1 score, and Coverage are used to evaluate LiLi's performance, \n", " Coverage, Avg. MCC, and avg. +ve F1 score, ", " MCC and +ve F1 score, ", " Avg. MCC, avg. +ve F1 score and Coverage,  ", " Avg. MCC and avg. +ve F1 score, ", " In this paper, we believe that a chatbot with the LiLi approach can serve as a knowledge learning engine for conversations. To demonstrate this belief, we evaluate the system with two standard real-world KBs: Freebase and WordNet. Experimental results show that LiLi is highly effective in terms of its predictive performance and strategy formulation ability. We use several metrics to measure its effectiveness, including: Coverage, Avg. MCC, and avg. +ve F1 score. This demonstrates that LiLi makes chatbots more knowledgeable and better at learning and conversation. To be more specific, Coverage measures the fraction of total query data instances, for which", " Average Matthews correlation coefficient (Avg. MCC) and average positive F1 score (avg. +ve F1 score), ", " Avg. MCC and avg. +ve F1 score, ", " Average Matthews Correlation Coefficient (MCC) and average positive F1 score, \n", " avg. MCC and avg. +ve F1 score, ", " ", " Average MCC, avg. +ve F1 score, and Coverage, ", " Average MCC and Average +ve F1 score.\n", " ", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC, avg. +ve F1 score and Coverage. \n", " Coverage, average MCC, and average +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are predictive performance, strategy formulation ability, Coverage, Avg. MCC, and avg. +ve F1 score., ", " Avg. MCC and avg. +ve F1 score.\n", " Average Matthews Correlation Coefficient (MCC), Average positive F1 score, and Coverage (the fraction of total query data instances for which LiLi has successfully formulated strategies that lead to winning), ", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score.\n", " Avg. MCC and avg. +ve F1 score, ", " Question: (a) precision, (b) accuracy, (c) average MCC, (d) both (c) and (d), ", " Avg. MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " ", " ", " Average MCC and average +ve F1 score, ", " Average MCC (Matthews Correlation Coefficient), and average +ve F1 score. ", " d, ", " Average MCC and Average +ve F1 score, ", " MCC, avg. +ve F1 score, and Coverage are used to show the effectiveness of LiLi, \n", " Avg. MCC and avg. +ve F1 score, ", " MCC and avg. +ve F1 score, ", " Avg. MCC and avg. +ve F1 score, ", " The metrics used to establish that LiLi makes chatbots more knowledgeable and better at learning and conversation are predictive performance, strategy formulation ability, Coverage, Average MCC, and average positive F1 score.  ", " Matthews correlation coefficient (MCC) and +ve F1 score, ", " Coverage, MCC, and positive F1 score.\n", " Avg. MCC, avg. +ve F1 score, and Coverage, ", " Avg. MCC and avg. +ve F1 score, ", " Average Matthews correlation coefficient (MCC) and average positive F1 score, ", " Matthews correlation coefficient (MCC), average MCC, and average positive F1 score.\n\n", " Avg. MCC and avg. +ve F1 score, Coverage(INLINEFORM0), and +ve F1 score on each relation.\n", " Avg. MCC, avg. +ve F1 score, and Coverage(InlineForm0), ", " Avg. MCC and avg. +ve F1 score.  ", " Predictive quality and strategy formulation ability.\n", " Avg. MCC and avg. +ve F1 score,  ", " Avg. MCC and avg. +ve F1 score, and Coverage measure.\n", " Average MCC and avg. +ve F1 score, along with Coverage.\n", " Avg. MCC and avg. +ve F1 score, "], "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": [" The CRF baseline. \n", " State-of-the-art sequence labeling models. ", " CRF and a pipeline model. \n", " Baselines that do not adopt joint learning. ", " Baselines that do not adopt joint learning, and also a CRF baseline. \n", " Prior works that did not employ joint learning. \n", " Conditional Random Fields (CRF), ", " Conditional Random Fields (CRF), ", " They compare with prior studies that did not employ joint learning. \n", " BIBREF3, BIBREF4, BIBREF5, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF29, BIBREF30,", " CRF, ", " Baselines that do not adopt joint learning. ", " Several baselines that do not employ joint learning, including a rule-based system for pun location, and prior work on sequence labeling such as BiLSTM-CNN, GRNN, LSTM-CRF, LSTM-CNN-CRF, and LM-LSTM-CRF. \n\n", " Prior works that did not employ joint learning. \n", " Prior works that did not employ joint learning and a baseline model based on conditional random fields (CRF). \n", " Conditional Random Fields (CRF) with features like POS tags, n-grams, label transitions, word suffixes and relative position to the end of the text. \n", " CRF. \n", " Conditional Random Fields (CRF) and rule-based system for homographic pun location. \n", " prior works that did not employ joint learning. \n", " Previous works that did not employ joint learning and a baseline model based on conditional random fields (CRF). \n", " Other models without joint learning. \n", " A baseline model based on conditional random fields (CRF), the system of BIBREF25, prior works that did not employ joint learning, a rule-based locator. \n", " They compare with baselines that do not adopt joint learning, a CRF model, and a rule-based locator, and also a \"pipeline\" method where the classifier for pun detection is regarded as perfect. ", " They compare with baselines that do not adopt joint learning. \n", " CRF and previous works that do not employ joint learning. \n", " BIBREF22, BIBREF24, BIBREF25, BIBREF10, BIBREF23, UWAV, BIBREF4 and prior works that did not employ joint learning. \n\n", " A baseline model based on conditional random fields (CRF), as well as a pipeline method where the classifier for pun detection is regarded as perfect. \n", " Conditional random fields (CRF), \n", " Prior works that did not employ joint learning. \n", " Baselines that do not adopt joint learning. \n", " The baselines that do not adopt joint learning and a model based on conditional random fields (CRF). \n", " The baselines they compare with include a conditional random fields (CRF) model, and the state-of-the-art system for homographic pun location is a neural method, where the word senses are incorporated into a bidirectional LSTM model. Also, they compare with the prior work of BIBREF4. \n", " Conditional Random Fields (CRF) baseline and a pipeline method. \n", " Conditional Random Fields (CRF) and prior works that did not employ joint learning. \n", " Baselines that do not adopt joint learning. \n", " Prior studies and a conditional random fields (CRF) model. \n", " Prior works that did not employ joint learning. \n", " Baselines that do not adopt joint learning. ", " CRF. \n", " Their model with the INLINEFORM0 tagging scheme and the INLINEFORM1 tagging scheme compared to baselines that do not adopt joint learning in the first block, a model based on conditional random fields (CRF), a model without position knowledge and a \"pipeline\" method. \n", " Baselines that do not adopt joint learning. \n", " Baselines that do not adopt joint learning. \n", " They compare with a baseline model based on conditional random fields (CRF), and also with prior studies that did not employ joint learning. \n", " CRF and previous studies. \n", " Their model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block. \n\n", " Their model with the INLINEFORM0 tagging scheme compared with the baselines that do not adopt joint learning, and their model with the INLINEFORM1 tagging scheme compared with the baseline CRF. \n", " Prior works that did not employ joint learning. \n", " Baselines that do not adopt joint learning. \n", " Baselines that do not adopt joint learning, and a baseline model based on conditional random fields (CRF). \n", " Prior works that did not employ joint learning. \n", " Conditional Random Fields (CRF) and a rule-based locator. \n", " They compare with baseline models that do not adopt joint learning, as well as a pipeline approach that performs pun detection followed by pun location, a CRF-based model, and a rule-based pun locator. \n", " prior works that did not employ joint learning, as well as a baseline model based on conditional random fields (CRF), and a pipeline method. \n", " Prior works that did not employ joint learning. \n", " Conditional Random Fields (CRF) baseline and prior works that did not employ joint learning. \n", " CRF, ", " CRF and prior works that did not employ joint learning. ", " The baseline model based on conditional random fields (CRF), a rule-based system for pun location, and the state-of-the-art system for homographic pun location, a neural method with word senses. \n", " Their BiLSTM-CRF model, either with the INLINEFORM0 or the INLINEFORM1 scheme, yields significantly higher recall on both detection and location tasks, while the precisions are relatively close, compared to a conditional random fields (CRF) baseline that considers features like POS tags produced by the Stanford POS tagger, n-grams, label transitions, word suffixes and relative position to the end of the text. \n", " Conditional Random Fields (CRF) baselines, \n", " Baselines that do not adopt joint learning. \n", " Their baseline model. \n", " Baselines that do not adopt joint learning. \n", " Conditional random fields, ", " Prior works that did not employ joint learning. \n", " Several prior works that did not employ joint learning. \n", " CRF, ", " Baselines that do not adopt joint learning, ", " Prior works that did not employ joint learning. \n", " Conditional Random Fields (CRF), ", " BIBREF22 and UWAV. \n", " The baselines they compare with are the systems that do not adopt joint learning and the conditional random fields (CRF) model. ", " A baseline model based on conditional random fields (CRF), \n\n", " Conditional Random Fields (CRF) and a pipeline method using a classifier for pun detection and a weakly supervised approach for pun location. ", " Baselines that do not adopt joint learning. \n", " Baselines that do not adopt joint learning. \n", " Conditional Random Fields (CRF), ", " Their model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block. \n\n", " Prior works that did not employ joint learning, and a conditional random fields (CRF) model. \n", " Prior works that did not employ joint learning and a baseline model based on conditional random fields (CRF). \n", " A baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger, n-grams, label transitions, word suffixes and relative position to the end of the text are considered. \n\n", " Prior works that did not employ joint learning. \n", " CRF model, \n", " Conditional Random Fields (CRF), ", " Conditional Random Fields (CRF) and rule-based locator. \n", " Prior works that did not employ joint learning and a baseline model based on conditional random fields (CRF). \n", " baselines that do not adopt joint learning, ", " Prior works that did not employ joint learning and a baseline model based on conditional random fields (CRF). \n\n", " Prior works that did not employ joint learning. \n", " CRF and the system of BIBREF25, ", " Prior works that did not employ joint learning. \n", " CRF and pipeline methods. \n", " State-of-the-art sequence labeling models. \n", " State-of-the-art sequence labeling models, prior works, a rule-based system for pun location, a knowledge-based approach, a neural method, UWAV system, GRNN, LSTM-CRF, LSTM-CNN-CRF, LM-LSTM-CRF, and a baseline model based on conditional random fields (CRF). \n\n", " CRF, ", " CRF, ", " Prior works that did not employ joint learning, a baseline model based on conditional random fields (CRF), and a “pipeline” method. \n", " Other works that do not employ joint learning. \n", " Prior works that did not employ joint learning. \n", " Conditional Random Fields (CRF) and the prior work that did not employ joint learning. \n\n"], "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": [" Automatic evaluation, human evaluation and case study experiments are conducted. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences are conducted. \n", " Experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. \n", " Various experiments are conducted to test the performance of the model, including automatic evaluations and human evaluations, as well as experiments on the transformation from ironic sentences to non-ironic sentences. \n\n", " \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences are conducted. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Additional experiments. \n", " Automatic evaluation, human evaluation, and additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " The authors conduct experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences, ", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", ", ", " A series of experiments including human evaluation results, automatic evaluation results, case study, and error analysis are conducted. \n\n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Automatic evaluations and human evaluations are conducted on the transformation from non-ironic sentences to ironic sentences, and human evaluations are also conducted on the transformation from ironic sentences to non-ironic sentences. \n", " Human evaluation results are also obtained for the transformation from non-ironic to ironic sentences, while for the transformation from ironic to non-ironic sentences, the automatic evaluation results are reported, human evaluation results are also obtained and some examples are shown.  Additionally, some additional experiments are conducted on the transformation from ironic to non-ironic sentences. \n", " Various experiments,", " automatic evaluations and human evaluations. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Various experiments are conducted to evaluate the models. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Experiments on the transformation from ironic sentences to non-ironic sentences and experiments on the transformation from non-ironic sentences to ironic sentences are conducted. \n", " Automatic and human evaluations, experiments on irony generation and irony reversal. \n", " They conduct the transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences. ", " Irony generation experiments, \n", " Automatic and human evaluation experiments are conducted for both transformations from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", " Irony generation and sentiment preservation experiments, in addition to experiments on the transformation from ironic sentences to non-ironic sentences. \n", " The experiments include automatic evaluations, human evaluations, and case studies. \n", " Experiments on the transformation from non-ironic sentences to ironic sentences, and the transformation from ironic sentences to non-ironic sentences are conducted. \n", " Automatic evaluations and human evaluation results are shown in the paper to assess the model's performance. Additionally, case studies are presented to demonstrate the model's performance on transforming non-ironic sentences to ironic sentences and vice versa. Furthermore, error analysis is performed to analyze the model's limitations. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences are conducted, in addition to the transformation from non-ironic sentences to ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Automatic and human evaluation experiments, and experiments on the transformation from ironic sentences to non-ironic sentences, as well as the exploration of some additional experiments.\n\n\n", "  The experiments conducted include the automatic evaluation and human evaluation of the model's performance on the transformation from non-ironic sentences to ironic sentences, as well as additional experiments on the transformation from ironic sentences to non-ironic sentences. ", " Several types of experiments are conducted. \n", " Various experiments are conducted including auto-association experiment, back-translation experiment, auto-association with back-translation experiment, pre-training with auto-association and back-translation experiments, and the model is then fine-tuned with reinforcement learning.  ", " Various experiments including the transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences are conducted. \n\n", " Experiments for transformation from non-ironic to ironic sentences and transformation from ironic to non-ironic sentences are conducted, and additional experiments on the transformation from ironic sentences to non-ironic sentences are also described. \n", " The authors conduct experiments on the transformation from non-ironic sentences to ironic sentences, and from ironic sentences to non-ironic sentences, using a dataset of over 2M tweets. \n", " Various experiments are conducted on the task of transforming non-ironic sentences to ironic sentences and vice versa. \n", " Experiments of transformation from non-ironic sentences to ironic sentences and transformation from ironic sentences to non-ironic sentences are conducted. ", " Human evaluation and automatic evaluation experiments. \n", " Experiments are conducted on the transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences. \n", " Various experiments including a case study and additional experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. \n", " Experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences are conducted. \n", " The authors conduct experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", " Human and automatic evaluations. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Automatic and human evaluation experiments are conducted. \n", " Various experiments are conducted, including irony generation, irony detection, sentiment analysis, and style transfer. ", " Several experiments including automatic evaluation and human evaluation of irony generation, style transfer and other generative models. \n", " Experiments are conducted on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", "  There are experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. ", " Various experiments. \n", " Experiments are conducted on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n\n", " The paper conducts experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n\n", "  Additional experiments on the transformation from ironic sentences to non-ironic sentences, ", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Various experiments, including automatic evaluations and human evaluations, are conducted. \n", " Several experiments on style transfer, irony detection, dataset building, and irony generation are conducted, including human evaluation results. \n", " unanswerable. \n", " The transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences are conducted, ", " The authors conduct experiments in the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Human and automatic evaluations of several models, including the proposed model and baselines. \n", " unanswerable \n", " unanswerable, ", " Experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences are conducted. \n", " Automatic and human evaluation results are reported in the paper.  ", " Automatic evaluations and human evaluation experiments are conducted. \n", " Various experiments are conducted, including generating ironic sentences from non-ironic sentences and from ironic sentences. \n\n", " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. \n", " additional experiments on the transformation from ironic sentences to non-ironic sentences are conducted, ", ", ", " Automatic and human evaluation experiments are conducted for irony generation and sentiment preservation. ", " Various experiments are conducted to evaluate the performance of the irony generation model. \n", " Multiple experiments including human evaluation and automatic evaluation, as well as additional experiments on the transformation from ironic sentences to non-ironic sentences, are conducted to evaluate the performance of the model. \n", " Experiments on transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Human evaluation and automatic evaluation. \n", " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. \n", " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. \n", " Automatic and human evaluations. \n", " Various experiments are conducted to evaluate the performance of the irony generation model and the style transfer baselines. ", " Experiments on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences are conducted. \n", " Automatic evaluation and human evaluation experiments are conducted, ", "  Various experiments including human evaluation, automatic evaluation, and case studies. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " The paper describes the following experiments: \n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n\n", " Additional experiments are conducted on the transformation from ironic sentences to non-ironic sentences. \n\n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n", " Automatic and human evaluation experiments and additional experiments on the transformation from ironic sentences to non-ironic sentences, human evaluation results for both transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences. \n\n", " Additional experiments on the transformation from ironic sentences to non-ironic sentences. \n"], "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": [" Yes. \n", " No, ", " yes, ", " yes, ", " no, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " No, ", " no, ", " yes\n\n", " yes, ", " yes, ", " yes, ", " yes, ", " no, ", " no, ", " No, ", " no, ", " Yes, ", " yes, ", " No, ", " Yes, \n", " No, \n", " No, ", " yes, ", " yes \n", " Yes, ", " yes, ", " yes, ", " yes \n", " yes, ", " Yes, \n", " Yes, \n", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " Yes, \n", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, \n", " yes, ", " yes, ", " yes, ", " No, unanswerable. \n", " No \n", " No, \n", " yes, ", " yes, ", " yes, ", " yes, ", " no, ", " No, ", " no, ", " Yes, ", " yes, ", " yes, ", " yes, ", " No, ", " yes, ", " yes, ", " Yes, they do. \n", " no, they do not employ their indexing-based method to create a sample of a QA Wikipedia dataset. \n", " yes, ", " Yes, ", " yes, ", " Yes, they do, specifically by using Lucene indexing and {1,2,3}-grams. \n", " no \n", " Yes, ", " no, ", " Yes, ", " Yes, ", " no, ", " Yes, ", " No, ", " yes, ", " yes, ", " Yes, ", " no, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " no, ", " no, ", " Yes, ", " No \n", " Yes, \n", " yes, ", " yes, ", " Yes, "], "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": [" The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN. \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, \n", " NUBes-PHI and MEDDOCAN, ", " NUBes-PHI and MEDDOCAN, "], "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": [" Yes, ", " No, ", " unanswerable, ", " Yes, \n", " yes, ", " yes, ", " no, ", " yes, ", " yes, ", " yes, ", " yes, ", " unanswerable, ", " Yes, ", " unanswerable, ", " No, \n\n", " unanswerable, ", " Yes, it is possible to convert a cloze-style questions to a naturally-looking questions, although the process is described as noisy. \n", " yes, ", " No, ", " Yes, the authors used a standard NLP pipeline based on Stanford CoreNLP and the BANNER Named Entity Recognizer to identify entities and phrases, and then constructed cloze-style questions that were often paraphrased with the information in the passage. \n\n", " yes \n", " Yes, ", " unanswerable, ", " yes, ", " no, ", " Yes, ", " unanswerable \n", " yes, ", " yes, ", " yes \n", " yes, ", " yes, ", " unanswerable, ", " yes, ", " yes, ", " Yes, ", " yes, ", " yes, ", " No, ", " unanswerable \n", " yes, ", " unanswerable, ", " yes, ", " yes, ", " unanswerable, ", " unanswerable, ", " yes, ", " Yes, \n", " Yes, ", " yes, ", " Yes, \n", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, \n", " yes, ", " yes, ", " unanswerable, ", " yes, ", " No, \n", " Yes, ", " yes\n", " yes \n", " yes, ", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, \n", " yes, ", " yes, ", " Yes, ", " Yes, \n", " No. ", " no, ", " unanswerable, ", " unanswerable, ", " Yes, ", " yes, ", " Yes, ", " unanswerable \n", " yes, ", " unanswerable, ", " unanswerable ", " no, ", " No, \n", " yes, ", " yes, \n", " yes, ", " Yes, ", " unanswerable, ", " No, ", " Yes \n", " yes, ", " No, ", " no, ", " no, ", " unanswerable, ", " no, "], "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": [" Gaussian-masked directional multi-head attention uses a Gaussian weight matrix to cast the localness relationship between characters as a fix Gaussian weight for attention and only pay attention to the adjacent characters of each positions. ", " , ", " Gaussian-masked directional multi-head attention works by combining the Gaussian weight matrix with the self-attention, and applying a triangular matrix mask to focus on different weights. \n\n", " It pays attention to the adjacent characters of each position and casts the localness relationship between characters as a fix Gaussian weight for attention. \n", " Gaussian-masked directional multi-head attention adjusts the weight between characters and their adjacent characters to a larger value, which stands for the effect of adjacent characters. ", " \n", " Gaussian-masked directional multi-head attention adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. ", " It combines the Gaussian weight to the self-attention to model the relationship between two characters, making the relationship between two characters with long distances weaker than adjacent characters. \n\n", "  Gaussian-masked directional multi-head attention combines the Gaussian weight to the self-attention by producing the Hadamard product of the Gaussian weight matrix G and the score matrix produced by QK^T. It ensures that the relationship between two characters with long distances is weaker than adjacent characters. \n", " It uses Gaussian weight to cast localness relationship between characters as a fix weight for attention and combines it with the score matrix produced by QK^T., ", " It ensures that the relationship between two characters with long distances is weaker than adjacent characters. \n", " It combines Gaussian weights with standard scaled dot-product attention, ", " It combines the Gaussian weight with self-attention. \n", ", ", " It pays attention to the adjacent characters of each position and casts the localness relationship between characters as a fix Gaussian weight for attention. \n", " Gaussian-masked directional multi-head attention combines Gaussian weight with multi-head attention to capture localness relationship between characters. \n", " It is a type of self-attention mechanism that combines Gaussian weights with standard scaled dot-product attention to capture directional relationships between characters in a sequence. \n", " It combines the Gaussian weight matrix with the scaled dot-product attention to model the relationship between characters and their adjacent characters, ", " It combines the Gaussian weight to the self-attention to model the relationship between characters and their adjacent characters. \n", " It captures the localness relationship between characters as a fix Gaussian weight for attention, and produces the Hadamard product of Gaussian weight matrix G and the score matrix produced by QK^T to combine the Gaussian weight to the self-attention. \n", ", ", " It combines Gaussian weight to the self-attention to model localness relationship between characters and makes one character can affect its adjacent characters more compared with other characters. \n", ", ", " \n", " It combines the standard scaled dot-product attention with a directional mask and Gaussian weight to capture the relationship between adjacent characters in a sentence. \n", " It combines the Gaussian weight with the score matrix produced by $Q{K^{T}}$ to adjust the weight between characters and their adjacent character to a larger value. ", " It combines Gaussian weight to self-attention to let the self-attention focus on adjacent characters by adjusting the weight between characters and their adjacent characters to a larger value for CWS task. \n", " \n", " Gaussian-masked directional multi-head attention combines self-attention with Gaussian weights to capture localness and directional information of characters. \n", " It ensures that the relationship between two characters with long distances is weaker than adjacent characters, which makes one character can affect its adjacent characters more compared with other characters. \n", " It combines Gaussian weights with standard self-attention to give more importance to adjacent characters. \n", " It uses Gaussian weight to adjust the relationship between characters and their adjacent characters, ", " It combines the Gaussian weight with self-attention, ensuring that the relationship between two characters with long distances is weaker than adjacent characters. \n", " It combines self-attention with Gaussian weight to model the localness relationship between characters, ", " Gaussian-masked directional multi-head attention combines self-attention with Gaussian-weighted matrix to give higher weights to adjacent characters and lower weights to distant characters, ", " It uses a Gaussian weight matrix to represent localness relationship between characters and adjusts the weight between characters and their adjacent characters to a larger value, making adjacent characters have more effect. \n\n", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. \n", " Gaussian-masked directional multi-head attention is calculated by Hadamard product of a Gaussian weight matrix and the score matrix produced by QK^T. \n", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters, ", " It works by introducing a Gaussian weight matrix to adjust the weights between characters and their adjacent characters to a larger value which stands for the effect of adjacent characters, ", " It uses a Gaussian weight matrix to adjust the weight between characters and their adjacent character to a larger value, which stands for the effect of adjacent characters, and combines this weight with the self-attention to get the Gaussian-masked attention. \n", " It uses a Gaussian weight matrix to assign attention weights based on the distance between characters and a triangular matrix mask to focus on different weights. ", " Gaussian-masked directional multi-head attention combines Gaussian weights with standard scaled dot-product attention to pay attention to adjacent characters of each position. \n", " It models the relationship between two characters while adjusting the weight between characters and their adjacent characters to a larger value, standing for the effect of adjacent characters. ", " By casting the localness relationship between characters as a fixed Gaussian weight for attention, with the weight only relying on the distance between characters, ", " It models the relationship between characters with a stronger weight for adjacent characters and a weaker weight for non-adjacent characters. \n", " It combines the Gaussian weight to the self-attention to pay more attention to the adjacent characters in a sentence. \n", " Gaussian-masked directional multi-head attention combines the Gaussian weight matrix with the score matrix produced by QK^T to pay more attention to adjacent characters. ", " It uses a Gaussian weight matrix to adjust the weight between characters and their adjacent characters. ", " It adjusts the weight between characters and their adjacent character to a larger value. \n", " Gaussian-masked directional multi-head attention works by adjusting the weight between characters and their adjacent characters to a larger value based on the distance between them. \n", " It adjusts the weight between characters and their adjacent characters to a larger value which stands for the effect of adjacent characters. \n", " It pays attention to the adjacent characters of each position, ", " It combines Gaussian weight to the self-attention, ensuring that the relationship between two characters with long distances is weaker than adjacent characters, to adjust the weight between characters and their adjacent character to a larger value. \n", " By combining the Gaussian weight to the self-attention to ensure the relationship between two characters with long distances is weaker than adjacent characters, ", " It combines Gaussian weight matrix with the score matrix produced by QK^T and ensures that the relationship between two characters with long distances is weaker than adjacent characters. \n", " It works by using a Gaussian weight matrix to pay attention to adjacent characters and cast the localness relationship between characters as a fix Gaussian weight for attention, ", " It generates the attention by combining the Gaussian weight with the score matrix produced by Q K^{T}. \n", " It combines the self-attention and Gaussian weight matrix to focus on adjacent characters and reduce the effect of distant characters. \n", " Gaussian-masked directional multi-head attention combines Gaussian weights with self-attention, giving more importance to the relationship between adjacent characters. \n", " It generates the representation of input by paying attention to the adjacent characters of each positions and casting the localness relationship between characters as a fix Gaussian weight for attention, ", " It models the relationship between two characters with long distances being weaker than adjacent characters. \n", " Gaussian-masked directional multi-head attention combines standard scaled dot-product attention with Gaussian-weighted directional masks to focus on adjacent characters and their relationships. \n", " It combines the Gaussian weight to the self-attention to model the relationship between two characters based on their distances. \n", " Gaussian-masked directional multi-head attention combines self-attention with Gaussian weights to adjust the weight between characters based on their distance. \n", " It combines the Gaussian weight matrix with the scaled dot-product attention. \n", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. \n", " It adjusts the weight between characters and their adjacent character to a larger value by introducing a Gaussian weight matrix to model the localness relationship between characters. \n", " It captures the localness relationship between characters by using a Gaussian weight matrix. \n", " It combines Gaussian weight to self-attention to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention. ", " Gaussian-masked directional multi-head attention combines the Gaussian weight to the self-attention to model the localness relationship between characters. ", " Gaussian-masked directional multi-head attention works by paying attention to adjacent characters of each position and casting localness relationships between characters as fixed Gaussian weights for attention. It uses a Gaussian weight matrix to present localness relationships and ensures weights are larger for adjacent characters than for characters further apart. \n", " Gaussian-masked directional multi-head attention pays attention to the adjacent characters of each position and casts the localness relationship between characters as a fix Gaussian weight for attention, and then adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. \n", " It combines the Gaussian weight to the self-attention to pay attention to adjacent characters and cast the localness relationship between characters as a fix Gaussian weight for attention. \n\n", " It combines the Gaussian weight to the self-attention. ", " It uses Gaussian weight to pay attention to adjacent characters and adjust weights to a larger value for adjacent characters, and combines the weights with self-attention scores to capture the relationship between characters. ", " It uses Gaussian weights to model the localness relationship between characters and adjusts the weights between characters and their adjacent characters to a larger value. \n", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters, and uses a triangular matrix mask to let the self-attention focus on different weights. \n", " According to the article, Gaussian-masked directional multi-head attention is a variant of standard multi-head attention and combines Gaussian weight matrix with self-attention to weigh the relationship between characters based on their distances. ", " Gaussian-masked directional multi-head attention is a self-attention mechanism that combines multi-head attention with a Gaussian-masked attention mechanism, which uses a Gaussian weight matrix to capture localness relationships between characters and focuses on adjacent characters with long distances. \n", " It models the relationship between two characters with long distances as weaker than adjacent characters. \n", " It combines Gaussian weight to the self-attention by producing the Hadamard product of Gaussian weight matrix and the score matrix produced by $Q{K^{T}}$, ", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. ", " It combines Gaussian weight matrix with self-attention to generate weights between characters that captures localness relationship and directional information, \n", " It captures the representation of different directions to improve the ability of capturing the localness information and position information for CWS. \n", " It captures the representation of the input by paying attention to adjacent characters and casts the localness relationship between characters as a fixed Gaussian weight for attention. ", " By combining a Gaussian weight matrix with the self-attention score matrix, ensuring that the relationship between two characters with long distances is weaker than adjacent characters, ", " It models the relationship between two characters by adjusting the weight between characters and their adjacent character to a larger value for the effect of adjacent characters. \n", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters, ", " It models the relationship between two characters by combining self-attention with a Gaussian weight matrix, which adjusts the weight between characters and their adjacent character to a larger value for CWS task. \n", " It combines the Gaussian weight to the self-attention, ", " Gaussian-masked directional multi-head attention works by combining a Gaussian weight matrix to standard self-attention to adjust the weight between characters based on their distances, and using a multi-head architecture to capture information from different dimension positions. \n\n", " It generates the representation of input by dividing queries, keys and values to different heads, and gets information from different subspaces, with a Gaussian-masked attention that focuses on adjacent characters and downplays long-distance relationships. \n", " Gaussian-masked directional multi-head attention combines the Gaussian weight with the standard self-attention, \n", " It models the relationship between two characters while adjusting the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. \n", " \n", " It adjusts the weight between characters and their adjacent character to a larger value which stands for the effect of adjacent characters. \n", " Gaussian-masked directional multi-head attention combines the Gaussian weight matrix with the standard self-attention, focusing on the adjacent characters of each position, \n", " It adjusts the weight between characters and their adjacent characters to a larger value to make the relationship between adjacent characters more important, while weakening the relationship between characters and characters far away. \n", " It ensures that the relationship between two characters with long distances is weaker than adjacent characters, "], "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": [" Facebook and Twitter. \n", " Facebook status update messages. \n", " Facebook. ", " Twitter and Facebook. ", " Facebook and tweets, ", " Facebook and tweets, ", " Facebook, ", " Facebook. \n", " Facebook. ", " Facebook and Twitter, ", " Facebook. \n", " Facebook status update messages, ", " Facebook. \n", " Facebook status update messages, ", " Facebook. \n", " Facebook, ", " Facebook and Twitter, ", " Facebook. \n", " Facebook status update messages. ", " Facebook, ", " Facebook status update messages, ", " Twitter and Facebook. \n", " Facebook, ", " Facebook and Twitter, ", " Facebook. ", " Facebook. \n", " Facebook and Twitter, ", " Facebook. \n", " Twitter and Facebook, ", " Facebook and tweets. \n", " Facebook and Twitter, ", " Facebook, ", " Twitter and Facebook.  \n", " Twitter, ", " Facebook, ", " Facebook and tweets. \n", " Facebook and tweets. \n", " Facebook status update messages and tweets. ", " Facebook, ", " Facebook. \n", " Facebook status update messages and tweets. \n", " Facebook, ", " Facebook and tweets. \n", " Facebook. \n", " Facebook, ", " Facebook and Twitter, ", " Facebook and tweets. \n", " [Your final answer here], ", " Facebook. \n", " Facebook, ", " Facebook, ", " Facebook. \n", " Facebook and tweets. ", " Facebook. ", " Facebook \n", " Facebook and Twitter, ", " Facebook, ", " Facebook. ", " Facebook and tweets. \n", " Facebook and tweets. \n", " Twitter, ", " Facebook and Twitter. \n", " [Your concise final answer here], ", " Facebook and Twitter. ", " Facebook and Twitter. \n", " Facebook and Twitter, ", " Facebook status update messages. \n", " Facebook, ", " Facebook and tweets, ", " Facebook and Twitter, ", " Facebook. \n", " Facebook status update messages. \n", " Facebook and tweets. \n", " Facebook and Twitter. ", " Facebook, ", " Facebook. \n", " Facebook, ", " Facebook status update messages. ", " Facebook. ", " Facebook status updates and tweets. \n", " Facebook and Twitter. \n", " Twitter, ", " Facebook. \n", " Facebook and tweets, \n", " Facebook, ", " Facebook. \n", " Facebook status update messages. \n", " Facebook and Twitter, ", " Twitter, ", " Facebook. \n", " Facebook and Twitter, ", " Twitter and Facebook. ", ", ", " Facebook.  \n", " Facebook and tweets. \n", " Facebook and Twitter, ", " Facebook and tweets. ", " Facebook. \n", " Twitter and Facebook. ", " Facebook and Twitter, "], "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": [" They ranked second, second, fourth, and fifth. \n", " Their system ranked second, second, fourth, and fifth place in the SemEval AIT-2018 leaderboard. \n", " Their system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Second, second, fourth, and fifth place. \n", " Their submissions ranked second, second, fourth, and fifth. ", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Second, second, fourth and fifth place. \n", " Their system ranked second, second, fourth and fifth place. \n\n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " The system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n\n", " Their submissions ranked second, second, fourth, and fifth. \n", " Their system ranked second, second, fourth and fifth place. \n", " They ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg), and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " They ranked second, second, fourth, and fifth place. \n", " [Your concise final answer here], ", " Their system ranked second in EI-Reg, second in EI-Oc, fourth in V-Reg, and fifth in V-Oc. \n\n", " They scored second, second, fourth, and fifth place. \n\n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " They ranked second for EI-Reg and EI-Oc, fourth for V-Reg, and fifth for V-Oc. \n\n", " Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n\n", " The authors' system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Their system's scores were 2nd (EI-Reg), 2nd (EI-Oc), 4th (V-Reg) and 5th (V-Oc). \n", " \n", " Their system scored second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Their system ranked second, second, fourth and fifth place in the Semeval 2018 Shared Task on Affect in Tweets. \n\n", " Their system achieved official scores that placed them second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " They ranked second, second, fourth and fifth place. \n\n", " second, second, fourth, fifth. \n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Our official scores had placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n\n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Their system scored second for EI-Reg and EI-Oc, fourth for V-Reg, and fifth for V-Oc. \n\n", " They ranked second, second, fourth and fifth place. \n\n", " Their system scored second, second, fourth, and fifth in different subtasks. \n", " Their system scored second in EI-Reg and EI-Oc, fourth in V-Reg and fifth in V-Oc. \n\n", " [], ", " \n", " They ranked second in EI-Reg, second in EI-Oc, fourth in V-Reg, and fifth in V-Oc, with official scores on the SemEval AIT-2018 leaderboard. \n", " Their submissions ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Second, second, fourth and fifth place. \n", " Their system scored second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " They ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n", " Their system ranked second for two tasks and fourth and fifth for two tasks respectively. \n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " They ranked second in EI-Reg and EI-Oc, fourth in V-Reg, and fifth in V-Oc. \n\n", " Their system placed second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " The scores of their system were second for EI-Reg and EI-Oc, fourth for V-Reg, and fifth for V-Oc on the SemEval AIT-2018 leaderboard. \n", " Second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " The scores of their system placed them second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Their system ranked second, second, fourth and fifth place. \n\n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the leaderboard. \n", " Their system scored second (EI-Reg), second (EI-Oc), fourth (V-Reg), and fifth (V-Oc) on the leaderboard. \n\n", " ", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc), ", " Their system ranked second in EI-Reg, second in EI-Oc, fourth in V-Reg, and fifth in V-Oc. ", " Their system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " They ranked second, second, fourth and fifth place. ", " Their system placed them second, second, fourth, and fifth place in four different subtasks. \n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. ", " Their system scored second, second, fourth and fifth place in the test set. \n", " Their system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the test set. \n\n", " Their system ranked second, second, fourth and fifth place on the leaderboard. \n", " They scored second, second, fourth and fifth. \n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Second, second, fourth and fifth place. \n", " The system placed second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. ", " Their system ranked second, second, fourth and fifth place. \n\n", " The system ranked second in EI-Reg and EI-Oc, fourth in V-Reg, and fifth in V-Oc on the SemEval AIT-2018 leaderboard. ", " They ranked second in EI-Reg and EI-Oc, fourth in V-Reg, and fifth in V-Oc. \n", " They placed second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n", " The scores were 2nd (EI-Reg), 2nd (EI-Oc), 4th (V-Reg), and 5th (V-Oc). \n", " Their submissions ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " Their system achieved 2nd, 2nd, 4th and 5th places. \n", " Second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n\n", " They ranked second, second, fourth, and fifth place. \n", " They ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the leaderboard. \n\n", " second (EI-Reg), second (EI-Oc), fourth (V-Reg), and fifth (V-Oc) on the SemEval AIT-2018 leaderboard., ", " They ranked second, second, fourth, and fifth. \n", " Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. ", " Their system ranked second, second, fourth, and fifth on the leaderboard. \n", " The scores of their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n\n", " \n", " Their system scored second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard, and achieved a small increase in score on the test set, especially on the anger subtask for both EI-Reg and EI-Oc, the scores are considerably lower on the test set in comparison with the results on the development set. \n\n", " Their system ranked second, second, fourth and fifth place, and the final scores on the leaderboard were second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc). \n\n", " Second, second, fourth and fifth place. ", " \n", " Their submissions ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc), demonstrating that the proposed method is accurate in automatically determining the intensity of emotions and sentiment of Spanish tweets. \n", " Their system ranked second, second, fourth, and fifth. \n", " They ranked second, second, fourth and fifth place. "], "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": [" Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. ", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. ", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity. \n", " Unigrams and Pragmatic features. \n", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. \n", " Unigrams and Pragmatic features. \n", " Pragmatic features. ", " Unigrams and Pragmatic features. \n", " Unigrams and pragmatic features, stylistic patterns, and patterns related to situational disparity, hashtag interpretations. \n", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Pragmatic features, Stylistic patterns, and hashtag interpretations. ", " Unigrams and Pragmatic features. \n", " Linguistic, stylistic and pragmatic features. \n", " Unigrams and Pragmatic features, ", " Pragmatic features, stylistic patterns, patterns related to situational disparity, and hashtag interpretations. \n", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. ", " Unigrams and Pragmatic features, Stylistic patterns and patterns related to situational disparity, and Hastag interpretations. ", " Unigrams and Pragmatic features. \n", " Unigrams and Pragmatic features. \n", " Unigrams and Pragmatic features. \n", " Unigrams, pragmatic features, stylistic patterns, and patterns related to situational disparity, and hastag interpretations. ", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, Hastag interpretations. ", " Unigrams, Pragmatic features, Stylistic patterns and patterns related to situational disparity, and Hastag interpretations. ", " Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity, along with Hastag interpretations. ", " Pragmatic features, stylistic patterns and hashtag interpretations. \n", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, Hastag interpretations. ", " Unigrams and Pragmatic features, stylistic patterns, patterns related to situational disparity, Hastag interpretations. \n", " Unigrams and Pragmatic features. \n", " Pragmatic features, stylistic patterns, patterns related to situational disparity, and Hastag interpretations. ", " Unigrams and Pragmatic features. ", " Stylistic patterns and pragmatic features. ", " Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Unigrams, pragmatic features, stylistic patterns and patterns related to situational disparity, and Hastag interpretations. ", " Unigrams and Pragmatic features, Stylistic patterns and patterns related to situational disparity. \n", " Unigrams, pragmatic features, stylistic patterns, and hashtag interpretations. \n", " Unigrams, pragmatics, stylistic patterns, situational disparity, and hashtag interpretations. \n", " Unigrams and pragmatic features, stylistic patterns and patterns related to situational disparity, and hashtag interpretations. \n", " Unigrams and pragmatic features. ", " Unigrams, Pragmatic features, Stylistic patterns and patterns related to situational disparity. \n", " Unigrams and Pragmatic features. ", " Unigrams and Pragmatic features. \n", " BIBREF0, BIBREF1, BIBREF2, BIBREF3; unigrams and pragmatic features, BIBREF4; stylistic patterns, BIBREF5; patterns related to situational disparity, and BIBREF6, BIBREF7; Hastag interpretations. \n", " Unigrams and Pragmatic features. \n", " Unigrams and pragmatic features. ", " Unigrams and Pragmatic features, and Stylistic patterns, and patterns related to situational disparity, and Hastag interpretations. \n", " Unigrams and Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Unigrams and Pragmatic features. \n", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Unigrams and Pragmatic features. \n", " Unigrams and Pragmatic features, Stylistic patterns, and Hastag interpretations. \n", " Unigrams and Pragmatic features. \n", " Unigrams, Pragmatic features, Stylistic patterns, Hastag interpretations. ", " Unigrams and Pragmatic features. \n", " Pragmatic features, stylistic patterns and patterns related to situational disparity. \n", " Unigrams, pragmatic features, stylistic patterns, and patterns related to situational disparity. ", " Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Unigrams and Pragmatic features. \n", " Pragmatic features, stylistic patterns, and patterns related to situational disparity, and also hashtag interpretations. \n", " BIBREF0, BIBREF1, BIBREF2, BIBREF3. \n", " Pragmatic features, stylistic patterns, patterns related to situational disparity, and hastag interpretations. \n", " Pragmatic features, stylistic patterns and patterns related to situational disparity. \n", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity, and hashtag interpretations. ", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. \n", " Unigrams and Pragmatic features. \n", " Unigrams, Pragmatic features, Stylistic patterns, patterns related to situational disparity, and Hastag interpretations. \n", " Unigrams and Pragmatic features. ", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity, along with Hastag interpretations. ", " Unigrams and Pragmatic features, Readability, and Word count of the text. ", " unigrams, pragmatics, stylistic patterns, and hashtag interpretations. ", " Stylistic patterns, Unigrams and Pragmatic features, Hastag interpretations. ", " Unigrams and Pragmatic features, \n", " Pragmatic features, stylistic patterns, and hashtag interpretations. \n", " Unigrams, Pragmatic features, Stylistic patterns and patterns related to situational disparity. ", " Linguistic and sarcastic related features, ", " Unigrams, pragmatic features, stylistic patterns and patterns related to situational disparity. ", " unigrams and pragmatic features. \n", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity. ", " Unigrams, Pragmatic features, Stylistic patterns, and patterns related to situational disparity. \n", " Unigrams and Pragmatic features. ", " Unigrams, pragmatic features, stylistic patterns, patterns related to situational disparity, and Hastag interpretations. ", " Unigrams and Pragmatic features. ", " Unigrams and Pragmatic features. ", " Unigrams, Pragmatic features. ", " Unigrams and Pragmatic features. ", " Unigrams and Pragmatic features. \n", " Unigrams and pragmatic features. \n", " Unigrams and Pragmatic features. \n", " unigrams, pragmatic features, stylistic patterns, and patterns related to situational disparity. ", " Unigrams and Pragmatic features. ", " [Your concise final answer here], ", " Unigrams, Pragmatic features, Stylistic patterns and patterns related to situational disparity. ", " Unigrams and Pragmatic features, Stylistic patterns and patterns related to situational disparity, Hastag interpretations. \n", " Pragmatic features, stylistic patterns, and hashtag interpretations. ", " Unigrams and Pragmatic features. \n", " Unigrams and Pragmatic features, Readability and Word count, Stylistic patterns, Patterns related to situational disparity, Hastag interpretations. \n", " Pragmatic features, stylistic patterns, and patterns related to situational disparity. \n", " Unigrams, Pragmatic features, Stylistic patterns, situational disparity, and Hastag interpretations. \n", " Unigrams and Pragmatic features. \n", " unigrams and pragmatic features, stylistic patterns, patterns related to situational disparity, and Hastag interpretations. \n"], "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": [" The corpus consists of 53 documents with an average of 156.1 sentences per document. \n", " 53 documents. \n", " The corpus consists of 53 documents. \n", " 53 documents,", " The corpus consists of 53 documents. ", " The corpus consists of 53 documents. \n", " 53 documents, ", " The corpus consists of 53 documents. \n", " The corpus consists of 53 documents, with an average of 156.1 sentences per document, each with 19.55 tokens on average. \n", " The corpus consists of 53 documents. \n", " 53 documents, ", " The corpus consists of 53 documents, containing an average of 156.1 sentences per document, with 167,739 words in total. \n", " The corpus contains 53 documents with 8,275 sentences. \n", " The corpus comprises 53 documents. ", " 53 documents, ", " The corpus consists of 53 documents. ", " The corpus consists of 53 documents with an average number of 156.1 sentences per document, containing a total of 8,275 sentences and 167,739 words. \n\n", " The corpus consists of 53 documents, each with 156.1 sentences on average. \n", " The corpus consists of 53 documents. \n", " 53 documents. \n", " The corpus consists of 53 documents with 8,275 sentences and 167,739 words in total. \n", " The corpus consists of 53 documents with an average of 156.1 sentences per document, comprising 8,275 sentences and 167,739 words in total. \n\n", " The corpus consists of 53 documents. \n", " 53 documents, ", " The corpus comprises 53 documents, which contain an average number of 156.1 sentences per document.  There are 8,275 sentences and 167,739 words in total. \n", " The corpus consists of 53 documents, 8,275 sentences, and 167,739 words in total. \n", " It contains 53 documents with an average of 156.1 sentences per document, comprising 8,275 sentences and 167,739 words. ", " The corpus consists of 53 documents. \n", " The corpus consists of 53 documents. \n", " The corpus consists of 53 documents. ", " The corpus consists of 53 documents with 8,275 sentences and 167,739 words. \n", " The corpus consists of 53 documents. \n", " 53 documents. ", " The corpus consists of 53 documents. \n", " 53 documents, ", " The corpus consists of 53 documents. ", " 167,739 words in total, ", " The corpus consists of 53 documents with 8,275 sentences and 167,739 words in total. \n", " The corpus consists of 53 documents, with 156.1 sentences per document on average. ", " The corpus consists of 53 documents with an average number of 156.1 sentences per document. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document. \n", " The corpus consists of 53 documents. \n", " The corpus comprises 53 documents. ", " 53 documents. ", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document. It also comprises 8,275 sentences and 167,739 words in total. The corpus is distributed in BioC JSON format., ", " 53 documents, ", " The corpus consists of 53 documents. \n", " 53 documents, ", " 53 documents. ", " The corpus consists of 53 documents., ", " The corpus consists of 53 documents, with an average of 156.1 sentences per document, and a total of 8,275 sentences and 167,739 words. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average, and the corpus comprises 8,275 sentences and 167,739 words in total. \n", " 53 documents. ", " The corpus consists of 53 documents, each with an average of 156.1 sentences per document.  In total, it contains 8,275 sentences and 167,739 words. ", " The corpus consists of 53 documents, with 8,275 sentences and 167,739 words in total. \n\n", " 53 documents, ", " The corpus consists of 53 documents. \n", " 53 documents. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. \n\n", " The corpus comprises 53 documents, with an average of 156.1 sentences per document. ", " 53 documents, ", " 53 documents. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. \n", " The corpus consists of 53 documents, with 8,275 sentences and 167,739 words in total. \n", " The corpus consists of 53 documents, with an average number of 156.1 sentences per document. ", " The corpus contains 53 documents with 8,275 sentences and 167,739 words. \n", " The corpus consists of 53 documents. ", " The corpus consists of 53 documents, 8,275 sentences, and 167,739 words. \n\n", " The corpus consists of 53 documents. ", " 53 documents. \n", " 53 documents, ", " 53 documents, ", " 53 documents. \n", " The corpus consists of 53 documents. ", " 53, ", " The corpus consists of 53 documents. \n", " 53 documents. ", " The corpus consists of 53 documents with an average of 156.1 sentences per document. \n", " 53 documents. \n", " 53 documents, ", " 53 documents. \n", " The corpus consists of 53 documents with an average of 156.1 sentences per document, 8,275 sentences in total. ", " The corpus comprises 53 documents, with an average of 156.1 sentences per document. \n", " The corpus consists of 53 documents. ", " 53 documents, \n", " 167,739 words in total and 53 documents, ", " [Your concise final answer here], ", " The corpus consists of 53 documents. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. \n\n", " 53 documents. ", " The corpus contains 53 documents with 8,275 sentences and 167,739 words. \n", " 53 documents. ", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average, and comprises 8,275 sentences and 167,739 words in total. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average, totaling 8,275 sentences and 167,739 words in total. ", " 53 documents. \n", " 53 documents. \n", " The corpus consists of 53 documents. \n", " The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average, and a total of 167,739 words. \n", " 53 documents. \n", " 53 documents, "], "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": [" Text categorization and sentiment classification. ", " text categorization and sentiment classification. \n", ", ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " text categorization, sentiment classification, and classification tasks, ", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification, and text classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification, ", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " text categorization, sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification, ", " Text categorization and sentiment classification. \n", " text categorization and sentiment classification. ", " Text categorization, sentiment classification, classification. ", " text categorization and sentiment classification. \n", " Text categorization and sentiment classification. ", " Text categorization and sentiment classification. \n", " text categorization and sentiment classification, ", " text categorization and sentiment classification. \n", " Text categorization, sentiment classification, and text classification. \n", " Text categorization and sentiment classification, ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " text categorization, sentiment classification, and text classification. \n", " text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification, ", " Text categorization, sentiment classification. \n", " Text categorization, sentiment classification, and text classification. ", " Text categorization and sentiment classification, ", " Text categorization and sentiment classification. \n", " text categorization and sentiment classification, ", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. ", " Text categorization, sentiment classification. ", " text categorization and sentiment classification. \n", " text categorization and sentiment classification, ", " Text categorization and sentiment classification, ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " text categorization, sentiment classification, baseball-hockey classification task. \n", " Text categorization, sentiment classification. \n", " text categorization and sentiment classification. ", " Text categorization and sentiment classification, ", " Text categorization, sentiment classification. ", " text categorization and sentiment classification, \n", " text categorization and sentiment classification. \n", " text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " [Your concise final answer here], ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification, classification of baseball-hockey, and other text classification tasks. \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " text categorization, sentiment classification. \n", " text categorization, sentiment classification. ", " Text categorization and sentiment classification. \n", " text categorization, sentiment classification, \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification. \n", " Text categorization and sentiment classification. ", " Text categorization and sentiment classification. \n", " text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization, sentiment classification, and classification. ", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " Text categorization and sentiment classification. \n", " text categorization and sentiment classification. \n", " Text categorization, sentiment classification, and topic modeling. \n"], "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": [" Baseline features are 100 features extracted from the baseline CNN, ", " The baseline features are 100 neurons from the fully-connected layer of the baseline CNN. \n", " The baseline features extracted from the baseline CNN architecture. \n", " 100 baseline features. \n", " Baseline features are 100 features extracted from the fully-connected layer of the baseline CNN. \n", " The baseline features. ", " 100 baseline features. \n", " 100 baseline features, ", " The baseline features are 100 baseline features. ", " 100 baseline features. \n", " 100 baseline features, ", " Baseline features are the features extracted from the baseline CNN. \n", " 100 features, ", " The baseline features are extracted from the fully-connected layer of the baseline CNN. ", " Baseline features are 100-dimensional vectors extracted from a CNN. ", " The baseline features are 100 neurons extracted from the fully-connected layer of the baseline CNN, ", " 100 baseline features. \n", " 100 baseline features. \n", " 100 baseline features. \n", " 100 baseline features, ", " Baseline features are the features extracted from the baseline CNN architecture and they are 100 neurons. \n", " 100 baseline features in our experiment. ", " 100 baseline features, ", " The baseline features are 100 features extracted from a CNN trained on the sarcasm dataset, which are inherent semantics extracted from the sarcastic corpus by employing deep domain understanding. \n", " The baseline features are 100 features extracted from the baseline CNN's fully-connected layer. \n", " 100 baseline features, ", " 100 baseline features. \n", " 100 baseline features, ", " The baseline features are the features extracted from the baseline CNN, which are 100 neurons. ", " The baseline features, which have been found to be the most effective in our experiments, are the features extracted using the baseline CNN architecture. \n", " 100 baseline features. \n", " 100 baseline features. ", " The baseline features. \n", " Baseline features are the features extracted from the baseline CNN, ", " 4) the features extracted from the fully-connected layer of the baseline CNN architecture., ", " 100 baseline features. \n", " 100 baseline features. \n", " 100 features, ", " 100 baseline features in our experiment. \n", " 100 features, ", " The baseline features are the features extracted from the baseline CNN. ", " Baseline features are the features extracted from a CNN directly trained on the sarcasm dataset, ", " 100 baseline features, ", " Answer should be concise and use only one phrase or sentence. \n\n", " Baseline features are extracted from a deep CNN that directly aims to classify a sentence as sarcastic vs non-sarcastic. \n", " The baseline features, which are extracted from the CNN architecture used in the baseline method, are the 100 features learned from the fully-connected layer of the network, which corresponds to the output of the network before the final softmax layer. \n", " The network's baseline features are the features extracted from the baseline CNN architecture.  ", " Baseline features extracted from the CNN baseline model. \n", " The baseline features are the 100 features extracted from the baseline CNN. \n", " Baseline features are the features extracted from the fully-connected layer of the baseline CNN architecture, ", " The baseline features are the features extracted from the CNN when it is used directly to classify the sarcastic and non-sarcastic tweets, with 100 baseline features. ", " Baseline features are the features extracted from the baseline CNN used in the experiment. \n", " 100 baseline features. ", " Baseline features are the features extracted from the baseline CNN architecture, which is a CNN model that directly aims to classify a sentence as sarcastic vs non-sarcastic. ", " 100 baseline features in the experiment. \n", " 100 baseline features. \n", " Baseline features are features extracted from the fully-connected layer of the CNN used for sarcasm detection. \n", " 100 baseline features extracted from the pre-trained CNN. \n", " Baseline features are the features extracted from the fully-connected layer of the baseline CNN model. \n", " 100 baseline features, ", " The baseline features are those features extracted from the baseline CNN, which directly classifies a sentence as sarcastic vs non-sarcastic. These features are the inherent semantics from the sarcastic corpus, extracted by employing deep domain understanding. \n", " The fully-connected layer of the baseline CNN has 100 neurons, so 100 baseline features are extracted from this network. \n", " The network's baseline features are the features extracted from the baseline CNN architecture. ", " 100 baseline features, ", " Baseline features are the features extracted from the baseline CNN. ", " 100 baseline features, ", " Baseline features are extracted from the baseline CNN architecture. ", " Baseline features, ", " 100 features. \n", " The baseline features in the experiment are the features extracted by the baseline CNN model. ", " Baseline features are the features extracted from a deep CNN that directly aims to classify a sentence as sarcastic vs non-sarcastic. \n", " 100, ", " The baseline features are 100 features, ", " 100 baseline features are extracted from the baseline CNN, where the fully-connected layer has 100 neurons. \n", " Baseline features, ", " 100 baseline features. \n", " 100 baseline features. \n", " 100 baseline features. \n", " 100, ", " Baseline features extracted by the CNN from the sarcastic corpus using deep domain understanding. ", " 100 baseline features extracted from the CNN, ", " Baseline features extracted from baseline CNN. ", " Baseline features are the features extracted from the CNN of the baseline method, which directly aims to classify a sentence as sarcastic vs non-sarcastic, and consist of 100 features. ", " The fully-connected layer of the baseline CNN has 100 neurons, so 100 baseline features are extracted from this network. \n", " 100 baseline features. \n", " \n", " 100 baseline features extracted from the fully-connected layer. ", " The baseline features are those extracted from the baseline CNN model using the fully-connected layer. \n", " 100 baseline features. ", " 100 features. \n", " The network's baseline features are the features extracted from the fully-connected layer of the baseline CNN architecture.  They outperform other feature combinations.\n\n", " The baseline features are the features extracted from the fully-connected layer of the baseline CNN, which is the network that directly aims to classify a sentence as sarcastic vs non-sarcastic. ", " The baseline features are the features extracted from the baseline CNN. \n", " Baseline features, ", " The baseline features are the features extracted from the baseline CNN, ", " Baseline features are 100 features extracted from the fully-connected layer of the baseline CNN. \n", " 100 baseline features in our experiment, ", " Baseline features are extracted from the fully-connected layer of the baseline CNN and are 100 features in the experiment. \n", " 100 baseline features, ", " 100 baseline features. \n"], "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": [" 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " \n", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103. \n", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " \n", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, ", " 0.6103, "], "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": [" unanswerable, ", " Various previous methods, ", " Their model is compared to 5 previous methods, ", " BERT-QC, CNN, Li and Roth model, Question Expansion with Hypernyms, and Topic words, ", " BERT-QC, CNN, BERT-QC, question expansion with hypernyms, label definitions, topic words, and wordlist features, ", " Learned methods, ", " BERT-QC is compared to multiple previous models. \n", " Learned models including CNN, LSTM, and previously reported results of term frequency models, ", " Previous methods including CNN, LSTMs, and learned models, ", " TREC question classification methods, ", " Their model is compared to previous methods in the domains of open, science, and medical question classification. \n", " previous question classification methods, ", " TREC question classification methods, ", " BERT-QC is compared to existing methods that include term frequency models, rule-based pattern matching, CNN, LSTM, TREC question classification methods and question classification methods developed on the biomedical domain. \n", " Their model is compared to several previous methods, ", " BERT-QC is compared to previous models such as term frequency models, learned methods, CNN, LSTM, and other question classification systems. \n", " TREC question classification methods, \n", " Previous methods, \n", " BERT-QC is compared to multiple previous methods. ", " Previous methods including term frequency models, CNN and LSTM variants, and learned models, ", " The previous methods, ", " Previous methods, such as learned methods like CNN and LSTM variants and simple term-frequency models, as well as rule-based methods like customized rule-based pattern matching and rule-based and machine learning approaches, are compared to the proposed model. \n", " Various previous methods, ", " Previous methods including rule-based pattern matching, learned methods such as CNN and LSTM, and simple term frequency models. \n", " BERT-QC is compared to previous methods such as term frequency models, CNN, LSTM variants, rule-based pattern matching, learned methods, and hand-crafted or semi-automated syntactic or semantic extraction rules. ", " Previous methods using rule-based or semi-automated syntactic or semantic extraction rules to infer question classes, \n", " BERT-QC is compared to previous methods including a variety of CNN and LSTM variants, rule-based approaches, and other machine learning models such as CPT2 and focus-driven semantic features (FDSF). \n", " BERT-QC, CNN, Li and Roth model, and others. \n", " BERT-QC is compared to a number of previous methods, including CNN, question expansion with hypernyms, and rule-based methods. \n", " Previous methods, ", " Previous models using learned methods, \n", " BERT-QC surpasses previous state-of-the-art performance, ", " TREC question classification methods, ", " BERT-QC model is compared to various previous methods including term frequency models, CNN models, LSTM models, and rule-based pattern matching models. ", " BERT-QC, CNN, and previous methods, ", " TREC question classification methods, \n", " TREC question classification methods, ", " BERT-QC model compares to multiple models including CNN, LSTM, rule-based pattern matching, BERT, CPT2 model, focus-driven semantic features (FDSF), TREC question classification methods, and GARD corpus of consumer health questions. ", " \n", " BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy, surpassing state-of-the-art performance on the GARD corpus of consumer health questions and MLBioMedLAT corpus of biomedical questions. ", " BERT-QC, previous models, and TREC, Recent work, and previous question classification models, and Rule-based methods, and TREC methods, \n", " TREC and BERT-QC model's state-of-the-art performance, ", " BERT-QC is compared to various previous methods including CNN, LSTM, rule-based pattern matching, and TREC question classification methods. ", " BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy, surpassing state-of-the-art performance on the GARD corpus of consumer health questions and MLBioMedLAT corpus of biomedical questions., ", " previous methods, ", " Previous methods using term frequency models, CNN, LSTM, rule-based methods, and machine learning approaches. \n", " BERT-QC model is compared to previous methods using CNN and BERT, as well as rule-based methods, and the results show that BERT-QC achieves state-of-the-art performance across multiple datasets. \n", " Their model is compared to previous methods such as learned methods and rule-based methods, as well as models that use hand-crafted or semi-automated syntactic or semantic extraction rules. ", " BERT-QC is compared to several previous methods including term frequency models, learned methods, CNN and LSTM variants, rule-based pattern matching and machine learning approaches. \n", " Previous methods. \n", " State-of-the-art performance models, ", " BERT-QC, previously reported performance, rule-based methods, CNN variants, learned methods, state-of-the-art performance, TREC question classification methods, Li and Roth model, Madabushi et al. model, Van-tu et al. model, Roberts et al. model. \n", " BERT-QC model is compared to the CNN model. \n", " TREC, GARD, and MLBioMedLAT, ", " TREC question classification methods, ", " TREC question classification methods, Roberts et al.'s method, and Madabushi et al.'s method. \n", " BERT-QC model is compared to several previous methods, including a recent emphasis on learned methods and CNN variants. ", " BERT-QC, CNN, and Madabushi et al.'s model, ", " BERT-QC surpasses state-of-the-art performance on benchmark question classification datasets, including TREC, GARD, and MLBioMedLAT. Additionally, their model also demonstrates gains of up to 12% compared to other models on the question classification task across the open, science, and medical domains. \n", " Their model is compared to previous methods on TREC question classification, GARD corpus of consumer health questions, and MLBioMedLAT corpus of biomedical questions, ", " BERT-QC achieves state-of-the-art performance compared to previous models, ", " Recent work in question classification methods, including term-frequency models, CNN and LSTM variants, and hand-crafted or semi-automated syntactic or semantic extraction rules. \n\n", " TREC question classification methods, ", " State-of-the-art question classification methods, ", " Previous models, ", " Previous methods such as CNN, LSTM variants, and simple term frequency models. \n", " Various previous methods, ", " Multiple previous question classification methods, \n", " Their model is compared to a variety of previous methods, including learned models, term-frequency models, and rule-based methods. \n", " BERT-QC is compared to other models including CNN, BERT-QC, Baseline, and recent reported performance on TREC-6 and TREC-50. Additionally, the authors compare their model to several other models on the GARD corpus and the MLBioMedLAT dataset, as well as with rule-based methods that achieve strong results on the GARD question classification dataset. Specifically, the authors compare their model with Roberts et al. model CPT2, which uses stemming and lists of semantically related words and cue phrases to achieve 80.4% accuracy on the GARD dataset.\n\n", " Other question classification models, ", " BERT-QC is compared to several previous models. ", " Various previous methods, ", " Their model is compared to multiple previous methods, including term frequency models, CNN, LSTM, rule-based methods, and previous BERT-QC models. \n", " BERT-QC is compared to a variety of previous methods including CNN and LSTM variants, but it is noted that strong performance is also possible using a single learned model. \n", " Previous TREC question classification methods, ", " The previous methods, ", " TREC and other learned models. \n", " Their model is compared to previous methods such as CNN, SVM, rule-based methods, and learned methods, ", " BERT-QC is compared to previous methods in multiple domains including open, science, and medical domains, ", " Previous methods such as Madabushi et al., Xia et al., and Van-tu et al. are compared to their model. \n\n", " The previous methods are BERT-QC with single-label learning and previous rule-based methods. \n", " Previous methods, \n", " Previous methods such as term frequency models, CNN variants, LSTM variants, rule-based methods and learned models. \n", " Previously reported performance on this dataset. \n", " Previous question classification methods developed on one dataset generally do not exhibit strong transfer performance to other datasets. \n", " Previous learned models for TREC question classification, \n", " Previous models, \n", " Their model is compared to a recent emphasis on learned methods, including a large set of CNN and LSTM variants. \n", " BERT-QC, TREC, CNN, and rule-based methods. \n", " Previous methods developed on TREC questions, \n", " Multiple previous methods, ", " TREC question classification methods, \n", " BERT-QC, Previous methods, Madabushi et al., Xia et al., Van-tu et al., Roberts et al., Kim, Lei et al., Rao et al., Kim et al. are all used as a comparison for the question classification model., \n", " TREC question classification methods. \n", " State-of-the-art performance, ", " BERT-QC surpasses state-of-the-art performance on the GARD corpus of consumer health questions and MLBioMedLAT corpus of biomedical questions. In the open domain, BERT-QC matches state-of-the-art performance on TREC. \n", " Other question classification models. ", " BERT-QC achieves state-of-the-art performance on fine-grained classification (TREC-50) for a learned model at 92.0% accuracy, and near state-of-the-art performance on coarse classification (TREC-6) at 96.2% accuracy, surpassing state-of-the-art performance on the GARD corpus of consumer health questions and MLBioMedLAT corpus of biomedical questions. \n", " Previous methods, "], "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": [" The number of clusters (k), the dimension of word vectors (vocabulary size), the number of iterations in k-means. \n\n", ", ", " The number of clusters. \n", " The number of clusters (K), the number of dimensions (D), and the type of word vector model were varied in the experiments. The number of clusters was set between 50 and 2000, the number of dimensions was not explicitly stated but the skip-gram model was used which has a low dimensionality. The type of word vector model included GloVe and skipgram, ", " \n", " number of clusters, ", " Number of clusters and the word vector model type, ", " k and the number of clusters, ", " the number of clusters, ", " k, ", " Number of clusters (k), ", " Number of clusters, ", " the number of clusters and the type of word vectors, ", " The number of clusters, ", " Number of clusters, ", " k, the number of clusters, ", " k (number of clusters), ", " \n", " Different values for k, for each k, different seed initialization for 10 times, 250, 500, 1000, 2000. \n", " Number of clusters, ", "  The number of clusters, ", " number of clusters, ", " Number of clusters, ", " The number of clusters (K), \n", " number of clusters, \n", " The number of clusters, ", " The number of clusters (k) were varied in the experiments on the four tasks, as well as the dimensionality of the word vectors. The dimensionality of the word vectors was set to 100 or 300 and k was varied between 250 and 2000, and for each k, 10 different initializations were used to avoid overfitting. ", " number of clusters, ", " k values of the k-means algorithm, ", " The number of clusters (k) was varied. ", " The number of clusters, \n", " k, ", " The number of clusters was varied, ", " Different values for the number of clusters. ", " The number of clusters, ", " Different values of k were varied, ", " The number of clusters (k), the type of word vectors, and the dimension of the word embeddings. Additionally, the out-of-domain word vectors trained on Wikipedia were also compared. ", " The number of clusters was varied in the experiments on the four tasks, ", " The number of clusters, ", " The number of clusters (K) was varied., ", " Number of clusters, ", " The number of clusters, \n", " The number of clusters, ", " The number of clusters (k), ", " k (number of clusters), ", " \n", " k, ", " number of clusters, ", " The number of clusters was varied. ", " k, ", " The number of clusters was varied. \n", " the number of clusters, ", " The number of clusters (k) and the dimension of the word embeddings, and the type of word embeddings (e.g., skipgram, cbow, GloVe)., ", " The number of clusters was varied, ", " The number of clusters (k) was varied, ", " Different number of clusters (250 to 1000 classes). ", " The number of clusters (k) was varied, ", " Different number of clusters, ", " The number of clusters, \n", " The number of clusters, ", " The number of clusters, ", " The number of clusters. \n", " The number of clusters (k) was varied in the experiments. ", " The number of clusters. \n", " The number of clusters, ", " k-means clusters, number of clusters, \n", " The number of clusters, ", " The number of clusters, ", " The number of clusters (k) was varied. ", " k, ", " The number of clusters (k) was varied in the experiments. The other hyperparameters were kept the same for all experiments., ", " The number of clusters (K) and the type of word embeddings, ", " \n", " The number of clusters, ", " Different number of clusters, ", " The number of clusters, ", " cluster inertia, number of clusters, ", " k-means clusters, ", " Different values for the number of clusters, ", " the number of clusters, ", " k-means cluster numbers and dimensions, ", " Different values of k, where k is the number of clusters, were varied. ", " The number of clusters. ", " The number of clusters (K), ", " The number of clusters was varied in the experiments on the four tasks. ", " The number of clusters (K) and the dimensionality (d) of the word vectors were varied. Additionally, the dimensionality of the word vectors and the type of embeddings (skipgram, cbow, or GloVe) were also varied in the experiments. However, the dimensionality of the word vectors was set to d=100 for the experiments with the skipgram and cbow models. ", " k-means clusters initialization times and the number of clusters, ", " Number of clusters. \n", "  number of clusters, ", " The number of clusters, \n", " Number of clusters, ", " The number of clusters (k) and the type of word vectors., ", " [], ", " The hyperparameters that were varied in the experiments include the number of clusters, the type of word vectors (skipgram, cbow, or GloVe), and the number of dimensions of the word embeddings, ", " Number of clusters, ", " The number of clusters was varied, ", " The number of clusters, ", " The number of clusters, ", " The number of clusters and the type of word vectors, ", " The number of clusters. "], "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": [" The newly produced contextual embeddings have larger training sets of a few hundred million tokens, ", " Much larger, ", " Much larger, ", " Larger by tens of millions of tokens. \n", " Hundreds of millions of tokens, ", " The training sets of these versions of ELMo are significantly larger, ", " Much larger, ", " Significantly larger, ", " The training sets of these versions of ELMo are significantly larger, ", " Larger, ", " Several hundred million tokens, ", " Larger, ", " much larger, ", " Larger \n", " Several hundred million tokens, ", " Much larger. \n", " Much larger, ", " Much larger, ", " Much larger, ", " The training sets of these versions of ELMo are 10 to 14 times larger. \n", " A few hundred million tokens larger, ", " Much larger. \n", " Much larger, ", " Much larger, ", " Significantly larger, ", " Significantly larger, ", " Much larger, ", " Larger by tens of millions of tokens. \n", " Much larger, ", " Much larger, \n", " Hundreds of millions of tokens, ", " A few hundred million tokens larger, ", " Larger, ", " 100-300 times larger, ", " much larger, ", " Substantially larger, ", " The new ELMo embeddings were trained on larger training sets, specifically, ranging from 20 million to 280 million tokens. \n\n", " Much larger, ", " A few hundred million tokens larger. \n", " Much larger, ", " Significantly larger, \n", " Hundreds of millions of tokens larger, ", " Larger, ", " Several hundred million tokens larger. \n", " Several hundred million tokens larger. \n", " The EMBEDDIA ELMo models were trained on larger sets with around 270-280 million tokens, ", " Much larger, ", " Larger, \n", " \n", " Several hundred million tokens larger. \n", " Several hundred million tokens larger, ", " Much larger, ", " A few hundred million tokens larger. \n", " The newly produced ELMo embeddings have training sets that are several hundred million tokens larger. \n", " Much larger, \n", " A few hundred million tokens larger. \n", " Larger, ", " The EMBEDDIA versions are significantly larger, and a few hundred million tokens are a sufficiently large corpus. \n", " Significantly larger, ", " Much larger, ", " Much larger, ", " Several hundred million tokens larger, ", " much larger, ", " Several hundred million tokens, ", " much larger, ", " Much larger, ", " Larger. \n", " much larger \n", " The new ELMo models were trained on larger sets of about 270 million tokens, ", " A few hundred million tokens, ", " They are 14-15 times larger. \n", " Significantly larger. \n", " Several hundred million tokens larger, ", " The ELMo versions in this study were trained on larger sets with sizes ranging from 270 million tokens to one billion tokens, while the previous ELMoForManyLangs model was trained on a 20-million-word corpus. \n", " Several hundred million tokens larger. \n", " Much larger, ", " A few hundred million tokens larger, ", " Several hundred million tokens larger. \n", " A few hundred million tokens larger. \n", " Significantly larger. \n", " Significantly larger, ", " Much larger, ", "  Several hundred million tokens, ", " Much larger. \n", " Much larger. \n", " Several hundred million tokens larger, ", " Much larger, ", " \n", " Significantly larger, ", " Significantly larger \n", " Several hundred million tokens larger. \n", " A few hundred million tokens larger. \n", " Much larger, ", " A few hundred million tokens larger. \n", " Much larger, ", " Much larger, \n", " The larger ones are about 5-6 times larger, ", " Significantly larger. \n", " Much larger, ", " A few hundred million tokens larger. \n"], "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": [" Yes, ", " yes, ", " Yes, ", " Yes. ", " Yes, ", " Yes, ", " Yes, \n", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes. \n", " Yes. ", " yes, ", " yes, ", " Yes, ", " yes ", " Yes, ", " Yes, their model is a multimodal NER model that learns NER from both text and images. \n", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " Yes. ", " yes, ", " Yes, ", " yes, ", " Yes, their model learns NER from both text and images. \n", " yes, ", " yes, ", " yes, ", " yes ", " Yes, ", " yes, ", " yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " yes, ", " Yes, ", " yes, ", " Yes, their model does. ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " Yes, ", " Yes. \n", " Yes, ", " Yes, their NER model learns NER from both text and images. \n", " yes, ", " yes, ", " Yes, ", " yes, ", " Yes, ", " yes ", " Yes, ", " Yes, ", " Yes, ", " Yes, ", " yes ", " Yes. ", " Yes, \n", " Yes, ", " yes, ", " Yes, ", " Yes, ", " Yes, ", " Yes, their NER model learns NER from both text and images. \n", " yes, ", " Yes, ", " yes, ", " yes, ", " yes, ", " yes, ", " yes, "], "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": [" yes, ", " Yes, ", " no, ", " no, ", " unanswerable, ", " yes, ", " no, ", " no, ", " yes, ", " Yes, ", " Yes, ", " no, ", " yes, ", " no, ", " yes, ", " no, ", " no, ", " No, unanswerable, ", " yes, ", " yes, ", " unanswerable, ", " yes, ", " unanswerable, ", " yes, ", " yes, ", " yes, ", " No, ", " No, unanswerable, ", " yes, ", " no, ", " no, ", " yes, ", " no, ", " no, ", " No, ", " no, ", " No, ", " Yes, ", " Yes, ", " no, ", " no, ", " yes, ", " No, unanswerable, ", " no, ", " Yes, ", " No, ", " unanswerable, ", " No, ", " Yes, ", " No, unanswerable, ", " no, ", " unanswerable, ", " unanswerable, ", " Yes, ", " no, ", " no, ", " no, ", " No, ", " No, ", " yes, ", " unanswerable, ", " yes, ", " \n", " yes, ", " no, ", " Yes, ", " yes, ", " No, ", " no, ", " No, ", " no, ", " Yes, ", " no, ", " Yes, ", " Yes, ", " no, ", " yes, ", " Yes, ", " no, ", " no, ", " unanswerable, ", " no, ", " no, ", " yes, ", " no, ", " no, ", " no, ", " no, ", " yes, ", " unanswerable, ", " No, ", " No, ", " no, ", " no, ", " yes, ", " no, ", " no, ", " Yes, ", " Yes, ", " no, "], "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": [" 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " OurNepali dataset contains 6946 sentences, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " ", " 6946, ", " 6946, ", ", ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", ", ", " 6946 sentences and also 6946 sentences in the raw dataset. \n\n", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " OurNepali dataset contains 6946 sentences, ", " 6946, ", " 6946, ", " 6946 and 16225 (unique words), ", " 6946, ", " 6946, ", " 6946, ", " OurNepali dataset contains 6946 sentences. ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946 and 16225 unique words, ", " 6946, ", " 6946, ", " 6946, ", " 6946 and 16225 unique words extracted from POS-tagged Nepali National Corpus and 16,225 sentences, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", ", ", " 6946, ", " 6946, ", " 6946, ", ", ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946 and 16225, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, ", " 6946, "], "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": [" The Penn Treebank, and the Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " The WSJ portion of the Penn Treebank. ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " The Penn Treebank, ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " The Penn Treebank. ", " Penn Treebank. ", " Penn Treebank. \n", " Penn Treebank and WSJ corpus, ", " The Penn Treebank, ", " The Penn Treebank. ", " The WSJ corpus. \n", " Penn Treebank and Wall Street Journal (WSJ) portion. ", " The Penn Treebank. \n", " Penn Treebank, ", " WSJ corpus. ", " WSJ, ", " WSJ. ", " The Penn Treebank and the Wall Street Journal (WSJ) corpus. \n", " The WSJ portion of the Penn Treebank. ", " Penn Treebank and WSJ corpus, ", " The Penn Treebank and WSJ corpus. \n", " The Penn Treebank, ", " Penn Treebank, ", " The Penn Treebank. ", " The WSJ portion of the Penn Treebank. ", " Penn Treebank and Wall Street Journal (WSJ) corpus. \n", " The Penn Treebank, ", " The Penn Treebank and the Wall Street Journal (WSJ) portion of the Penn Treebank. ", " Penn Treebank and WSJ corpus. ", " The Penn Treebank. \n", " The Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " WSJ, ", " WSJ, ", " Penn Treebank, ", " Penn Treebank. \n", " The Penn Treebank. \n", " The Penn Treebank and WSJ portion of the Penn Treebank. \n", " The Penn Treebank, specifically the Wall Street Journal (WSJ) portion. ", " The Penn Treebank. \n", " Penn Treebank and the WSJ portion. ", " The Penn Treebank and the WSJ portion of the Penn Treebank. \n", " The WSJ portion of the Penn Treebank, ", " The Penn Treebank. \n", " Penn Treebank. \n", " The Wall Street Journal (WSJ) portion of the Penn Treebank. ", " Penn Treebank and WSJ corpus. \n", " The Penn Treebank. ", " Penn Treebank and the Wall Street Journal (WSJ) portion of the Penn Treebank. ", " WSJ. ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. ", " Penn Treebank. \n", " WSJ corpus. \n", " Penn Treebank, ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " The Penn Treebank. ", " Penn Treebank, ", " WSJ, ", " The Penn Treebank. ", " The Penn Treebank. \n", " WSJ corpus. ", " The Penn Treebank, ", " Penn Treebank. \n", " The WSJ portion of the Penn Treebank. ", " The Penn Treebank and the Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " The WSJ portion of the Penn Treebank. \n", " The WSJ portion of the Penn Treebank. ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. ", " The Penn Treebank and Wall Street Journal (WSJ) portion of it. ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. ", " The Penn Treebank WSJ corpus. \n", " Penn Treebank and Wall Street Journal, ", " The WSJ portion of the Penn Treebank. \n", " Penn Treebank and one billion word language modeling benchmark dataset. ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. ", " The Penn Treebank, specifically the Wall Street Journal (WSJ) portion. ", " WSJ corpus. ", " The Penn Treebank and WSJ corpus. \n", " The Penn Treebank, ", " The Penn Treebank, \n", " The Penn Treebank. ", " Penn Treebank, ", " The Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " WSJ portion of the Penn Treebank. \n", " Penn Treebank and WSJ corpus. \n", " The Penn Treebank. ", " WSJ, ", " Penn Treebank. ", " The Penn Treebank, ", " WSJ, ", " Penn Treebank. \n", " The Penn Treebank. \n", " The Penn Treebank and WSJ portion of it. \n", " The Penn Treebank and Wall Street Journal (WSJ) corpus. \n", " Penn Treebank, ", " Penn Treebank and one billion word language modeling benchmark dataset. ", " The Wall Street Journal (WSJ) portion of the Penn Treebank, and a one billion word language modeling benchmark dataset. ", " Penn Treebank and Wall Street Journal (WSJ) portion of the Penn Treebank. \n", " Penn Treebank. \n", " Penn Treebank and WSJ corpus. "], "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": [" Through a survey among engineers. \n", " Many engineers face a large overhead in mastering framework details. \n", " They conducted a survey among engineers and analyzed the NLP jobs submitted to a commercial centralized GPU cluster. \n", " \n", " They mention that other general-purpose deep learning frameworks offer huge flexibility in DNN model design and support various NLP tasks. \n", " They mention that although several general-purpose deep learning frameworks and NLP deep learning toolkits exist, they require a large overhead of mastering these framework details.\n", " They mention that many general-purpose deep learning frameworks have gained popularity in NLP community. However, building models under these frameworks requires a large overhead of mastering these framework details. \n", " They mention in the Related Work section that many engineers favor a higher level abstraction to hide the framework details. \n", " Many engineers require higher level abstraction to hide the framework details. \n", " They cite examples of popular deep learning frameworks such as TensorFlow, PyTorch and Keras, as well as several popular deep learning toolkits in NLP. \n", " They evidence it through the results of their survey and an analysis of NLP jobs. \n", " By citing the limitations of popular deep learning frameworks and toolkits. \n", " They conducted a survey among engineers. \n", " They cite the many general-purpose deep learning frameworks and popular deep learning toolkits, such as TensorFlow, PyTorch, and OpenNMT, that have gained popularity in the NLP community, but have high learning curve. \n", " They conducted a survey among engineers and analyzed the NLP jobs submitted to a commercial centralized GPU cluster, which showed that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques. \n", " They cite general-purpose deep learning frameworks such as TensorFlow, PyTorch, and Keras. \n", " By pointing out the limitation of popular general-purpose deep learning frameworks and deep learning toolkits. \n", " They mention several popular general-purpose deep learning frameworks that offer huge flexibility in DNN model design and support various NLP tasks, but require a large overhead of mastering these framework details.\n", " They mention that general-purpose deep learning frameworks offer huge flexibility in DNN model design and support various NLP tasks, but building models under these frameworks requires a large overhead of mastering these framework details. \n", " By citing existing general-purpose deep learning frameworks like TensorFlow and PyTorch, which have gained popularity in NLP community. \n", " They cite several general-purpose deep learning frameworks and popular deep learning toolkits in NLP as an example. \n", " By citing existing general-purpose deep learning frameworks and deep learning toolkits, and then mentioning that they are not flexible enough to support new network architectures or new components. \n", " They reference several existing general-purpose deep learning frameworks and popular deep learning toolkits for NLP. \n", " Many NLP engineers prefer a higher level abstraction to hide the framework details. \n", " They refer to general-purpose deep learning frameworks such as TensorFlow, PyTorch, and Keras. \n", " They mention it, but don't provide evidence from surveys, experiments or user studies. \n", " Through a survey among engineers and analysis of a commercial centralized GPU cluster. \n", " They state that although many frameworks offer huge flexibility in DNN model design and support various NLP tasks, building models under these frameworks requires a large overhead of mastering these framework details. \n", " By referencing existing general-purpose deep learning frameworks such as TensorFlow, PyTorch and Keras. \n", " They conducted a survey among engineers, but it was not specified in the article how they collected and analyzed the data. \n", " By mentioning the flexibility of general-purpose deep learning frameworks. ", " They evidence this claim by stating that \"higher level abstraction to hide the framework details is favored by many engineers\".\n", " They mention that general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, offer huge flexibility in DNN model design and support various NLP tasks, but building models under these frameworks requires a large overhead of mastering these framework details. \n", " They evidence the claim through a survey they conducted among engineers. \n", " They reference general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras. \n", " They mention that several general-purpose deep learning frameworks, such as TensorFlow, PyTorch, and Keras, offer huge flexibility in DNN model design and support various NLP tasks. However, building models under these frameworks requires a large overhead of mastering these framework details. \n", " Through a commercial centralized GPU cluster. \n", " They mention that general-purpose deep learning frameworks such as TensorFlow and PyTorch have \"huge flexibility\" which can be overwhelming. \n", " They mention several general-purpose deep learning frameworks, including TensorFlow, PyTorch and Keras, which have huge flexibility in DNN model design but require mastering the framework details. ", " They mention that general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, offer huge flexibility in DNN model design and support various NLP tasks, which however requires mastering framework details.  \n", " Through a survey among engineers and analyzing NLP jobs submitted to a commercial centralized GPU cluster. \n", " They surveyed many engineers. \n", " \n", " They conducted a survey among engineers and analyzed the NLP jobs submitted to a commercial centralized GPU cluster. \n", " By citing the existence of many deep learning frameworks, general-purpose deep learning frameworks and popular deep learning toolkits. \n", " They cite the existence of many popular general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras. \n", " They surveyed engineers and analyzed job data. \n", " They conducted a survey among engineers.\n", " They cite a few popular deep learning frameworks including TensorFlow, PyTorch, and Keras, and also several popular deep learning toolkits in NLP, including OpenNMT and AllenNLP, which suggest that most popular deep learning frameworks and toolkits provide huge flexibility in DNN model design but with high mastering overhead. \n\n", " They do not directly evidence it. \n", " They refer to several general-purpose deep learning frameworks such as TensorFlow, PyTorch and Keras. \n", " They mention that engineers require higher level abstraction to hide framework details. \n", " They conducted a survey among engineers and analyzed NLP jobs submitted to a commercial centralized GPU cluster. \n", " They conducted a survey among engineers. \n", " They conducted a survey among engineers, ", " They mention that engineers are favored to use a higher-level abstraction to hide the framework details. \n", " By mentioning existing frameworks such as TensorFlow, PyTorch and Keras and existing deep learning toolkits including OpenNMT, AllenNLP, which have been widely used but require a large overhead to master their framework details. \n", " They cite the case of commercial centralized GPU cluster jobs, ", " The authors cited several examples of popular general-purpose deep learning frameworks and NLP toolkits. \n", " They point to the fact that a commercial centralized GPU cluster only sees 87.5% NLP related jobs belong to a few common tasks. \n", " They mention that other general-purpose deep learning frameworks offer huge flexibility in DNN model design and support various NLP tasks. \n", " They state that many general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, offer huge flexibility in DNN model design and support various NLP tasks, but building models under these frameworks requires a large overhead of mastering these framework details.\n", " They mention that general-purpose deep learning frameworks such as TensorFlow, PyTorch and Keras offer huge flexibility in DNN model design and support various NLP tasks, but building models under these frameworks requires mastering these framework details. ", " They conducted a survey among engineers. \n", " By citing several general-purpose deep learning frameworks, and by describing their own toolkit NeuronBlocks. \n", " They cite several examples of deep learning frameworks and toolkits in the NLP community. \n", " By referring to the related work on existing frameworks such as TensorFlow, PyTorch and Keras. \n", " They cite general-purpose deep learning frameworks. \n", " They conducted a survey among engineers, \n", " They mention that general-purpose deep learning frameworks like TensorFlow, PyTorch and Keras offer huge flexibility but also require a large overhead of mastering these framework details. \n", " Many engineers face a large overhead of mastering framework details when building models under general-purpose deep learning frameworks.\n", " By citing the popularity of high-level abstractions such as NeuronBlocks.\n", " They conducted a survey among engineers. \n", " They referenced several general-purpose deep learning frameworks and toolkits, \n", " They show that many existing general-purpose deep learning frameworks and popular deep learning toolkits are not flexible enough to support new network architectures or new components.\n", " By mentioning several popular deep learning frameworks, such as TensorFlow, PyTorch and Keras, and by citing works like OpenNMT and AllenNLP. \n", " They referenced general-purpose deep learning frameworks such as TensorFlow, PyTorch and Keras.\n", " They cite that general-purpose deep learning frameworks offer huge flexibility in DNN model design and support various NLP tasks, but require a large overhead of mastering these framework details. \n", " They mention that general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, offer huge flexibility in DNN model design and support various NLP tasks but require a large overhead of mastering these framework details. ", " By citing the existence of various popular deep learning frameworks and toolkits, including TensorFlow, PyTorch, Keras, OpenNMT, AllenNLP, etc., as an evidence of the claim. \n", " They conducted a survey among engineers, ", " They cite existing general-purpose deep learning frameworks like TensorFlow, PyTorch, and Keras, as well as popular deep learning toolkits like OpenNMT and AllenNLP, which have limitations in terms of flexibility and generality. \n", " They refer to several general-purpose deep learning frameworks and popular deep learning toolkits in NLP, and note that they reduce development cost but are limited to certain tasks. \n", " They have conducted a survey among engineers. \n", " Many engineers have a large overhead of mastering framework details when building models under general-purpose deep learning frameworks. \n", " Many engineers have to master framework details. \n", " By stating that related work has many deep learning frameworks and toolkits but they are limited in certain tasks, thus less flexible. \n", " By mentioning that several general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, offer huge flexibility in DNN model design but require a large overhead of mastering these framework details. \n", " They surveyed engineers and observed that a huge flexibility in DNN model design is favored by many engineers, but mastering framework details is unfavorable. \n", " They referenced the popular deep learning frameworks such as TensorFlow, PyTorch, and Keras, and several popular deep learning toolkits in NLP, including OpenNMT and AllenNLP, which are widely employed but limited to certain tasks. \n", " By mentioning the use of general-purpose deep learning frameworks such as TensorFlow, PyTorch and Keras which offer huge flexibility in DNN model design but require a large overhead of mastering these framework details. \n\n", " They conducted a survey among engineers and analyzed NLP jobs submitted to a commercial centralized GPU cluster.\n", " They mention that there are several general-purpose deep learning frameworks, such as TensorFlow, PyTorch and Keras, which have gained popularity in NLP community. \n", " They mention that general-purpose deep learning frameworks such as TensorFlow and PyTorch offer huge flexibility in DNN model design but also require a large overhead of mastering these framework details. ", " By mentioning that general-purpose deep learning frameworks offer huge flexibility in DNN model design but require a large overhead of mastering their framework details. \n", " By noting that there are several general-purpose deep learning frameworks, yet engineers have to master the framework details. \n", " They cite several general-purpose deep learning frameworks such as TensorFlow and PyTorch, which offer flexibility but have a high overhead in mastering framework details. \n", " By pointing out the limited focus of general-purpose deep learning frameworks and popular deep learning toolkits in NLP, making it difficult for engineers to address a wide range of tasks and network architectures. \n", " They conducted a survey among engineers. \n", " They surveyed engineers and analyzed the NLP jobs submitted to a commercial centralized GPU cluster.\n"], "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": [" Eusboost, MWMOTE and MLP. \n", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE and MLP. ", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE, MLP, and several state-of-the-art methods in the field of emotion recognition and speech processing. \n", " Eusboost, MWMOTE, and MLP, ", " s2sL compared to MLP, Eusboost, MWMOTE, \n", " Eusboost, MWMOTE, and MLP, ", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, MLP,  and state-of-the-art methods. \n\n", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE, MLP, and state-of-the-art methods, ", " s2sL compared to MLP and Eusboost, MWMOTE techniques. \n", " MLP, Eusboost, MWMOTE, state-of-the-art methods, WEKA toolkit, openSMILE toolkit, and other methods, ", " MLP, Eusboost, and MWMOTE, ", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, and MLP, ", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, ", " s2sL, MLP, Eusboost, MWMOTE, and other state-of-the-art methods, ", " s2s-MLP and Eusboost, MWMOTE, \n", " Eusboost, MWMOTE, WEKA toolkit, openSMILE toolkit, MLP, state-of-the-art methods, MLP,  standard methods, standard methods, WEKA toolkit, openSMILE toolkit. \n", " MWMOTE, Eusboost, MLP. \n", " Eusboost, MWMOTE, and state-of-the-art methods, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE and MLP, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, MLP and state-of-the-art methods, ", " Eusboost, MWMOTE, MLP, and WEKA. \n", " Eusboost, MWMOTE and MLP. \n", " Eusboost and MWMOTE, ", " Eusboost and MWMOTE, ", " MWMOTE, Eusboost, and MLP. \n", " Eusboost, MWMOTE, and MLP, ", " Eusboost and MWMOTE. \n", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, and state-of-the-art methods. \n", " Eusboost, MWMOTE, MLP, Eusboost and MWMOTE. \n", " MWMOTE, Eusboost, MLP, and WEKA toolkit. \n", " s2sL outperforms MLP and Eusboost and MWMOTE techniques in some cases, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, MLP, and state-of-the-art methods, ", " MWMOTE, Eusboost, and MLP, ", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE and MLP, ", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE, MLP, state-of-the-art methods, and WEKA. \n", " Eusboost and MWMOTE, ", " MWMOTE, Eusboost, MLP,, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE and MLP, ", " Eusboost, MWMOTE, MLP. \n", " Eusboost, MWMOTE, MLP, and other conventional methods, ", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE, MLP, and state-of-the-art methods, ", " Eusboost, MWMOTE, MLP, and softmax function, ", " MWMOTE, Eusboost, and MLP, ", " Eusboost, MWMOTE and MLP, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, and traditional MLP.  However, the proposed s2sL framework performs better than them in several cases., ", " Eusboost, MWMOTE, and MLP (Modified), ", " Eusboost, MWMOTE and MLP, ", " Eusboost and MWMOTE, ", " s2sL, MLP, Eusboost, MWMOTE, \n", " Eusboost, MWMOTE, and MLP, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE and traditional MLP, ", " Eusboost and MWMOTE, ", " s2s-MLP, MLP, Eusboost, MWMOTE, and other state-of-the-art methods, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, MLP, and state-of-the-art methods, ", " MWMOTE, Eusboost, and MLP, ", " Eusboost, MWMOTE, MLP and state-of-the-art methods, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE and state-of-the-art methods. \n", " Eusboost, MWMOTE, MLP. \n", " Eusboost, MWMOTE, and MLP (modified to handle the data representation), ", " s2sL is compared to MLP and Eusboost/MWMOTE. \n", " Eusboost and MWMOTE, ", " MLP and Eusboost, MWMOTE, \n", " Eusboost, MWMOTE, MLP, WEKA, OpenSMILE, and MLP., ", " Eusboost and MWMOTE, ", " MLP, Eusboost, and MWMOTE, ", " MWMOTE and Eusboost, ", " MLP, Eusboost, MWMOTE, \n", " s2s-MLP (their model) vs. Eusboost, MWMOTE, and MLP, ", " Eusboost and MWMOTE, ", " Eusboost, MWMOTE, MLP, WEKA toolkit, and openSMILE toolkit. \n", " Eusboost, MWMOTE, MLP, and others. \n", " Eusboost, MWMOTE, and MLP, ", " Eusboost, MWMOTE, MLP, \n", " Eusboost, MWMOTE, WEKA, openSMILE, CLDNNs, FFNN, Sparse autoencoder. \n", " MLP, Eusboost, and MWMOTE, ", " s2sL outperforms MLP, and also performs better than Eusboost and MWMOTE techniques on imbalanced data. Also, it outperforms MLP for Speech/Music and Neutral/Sad classification tasks, especially at low resource conditions. \n", " Eusboost, MWMOTE, MLP and WEKA toolkit. However, Eusboost and MWMOTE are only compared for data imbalance problem in emotion classification,  WEKA toolkit is used for feature selection. "], "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": [" SimpleQuestions and WebQSP, ", " WebQSP and SimpleQuestions, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP, \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n\n", " WebQuestions and SimpleQuestions, ", " WebQuestions, ", " WebQSP and SimpleQuestions. \n\n", " SimpleQuestions and WebQSP, \n", " SimpleQuestions and WebQSP, ", " WebQuestions, SimpleQuestions, \n", " SimpleQuestions and WebQSP, ", " WebQuestions and SimpleQuestions. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " WebQuestions, ", " SimpleQuestions and WebQSP, \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n\n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. ", " WebQSP and SimpleQuestions. \n\n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " WebQSP and SimpleQuestions, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n\n", " SimpleQuestions and WebQSP, ", " WebQSP and SimpleQuestions, ", " SimpleQuestions and WebQSP. \n", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP, ", " WebQSP and SimpleQuestions. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, \n", " SimpleQuestions and WebQSP, \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " WebQSP and SimpleQuestions. \n", " SimpleQuestions and WebQSP. \n", " WebQuestions and SimpleQuestions. \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " WebQuestions BIBREF0 and SimpleQuestions BIBREF2. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " WebQuestions BIBREF0 and SimpleQuestions BIBREF2, ", " WebQSP and SimpleQuestions, ", " WebQSP, SimpleQuestions, ", " SimpleQuestions and WebQSP, ", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, \n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP, ", " WebQSP and SimpleQuestions. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP, ", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n", " SimpleQuestions and WebQSP. \n\n", " SimpleQuestions and WebQSP, ", " SimpleQuestions and WebQSP. \n\n", " SimpleQuestions and WebQSP, ", " WebQuestions and SimpleQuestions, ", " SimpleQuestions and WebQSP. \n"], "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
